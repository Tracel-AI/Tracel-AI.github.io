import{a as e,g as le,f as n,i as t,t as ce}from"./entry-client-b35941c6.js";import{B as pe,R as a,a as he}from"./index-fe52d109.js";import{S as de}from"./index-30423bfa.js";import{c as ge}from"./blogs-31dabd14.js";import{L as me}from"./page-f4108470.js";const ue=ce("<div><h2>Introduction</h2><p>Burn has had the capability to run deep learning models on GPU since its inception, thanks to its use of Tch (bindings to LibTorch). This approach has proven to be highly effective, allowing for rapid iteration on the architecture, user APIs, and the automatic differentiation system. The need to write kernels and apply low-level performance optimizations was alleviated, allowing the team to focus on making significant progress faster. As we reached a level of satisfaction with our advancements, we slowly transitioned our focus to lower-level implementations. This allows us to leverage Burn's type system and enhance hardware compatibility. Ultimately, to be able to maximize performance and apply specific optimizations such as operation fusion, we need control over GPU kernels; thus we decided to write our own GPU backend.</p><p>After careful consideration of the technology to employ for developing our first GPU backend, we opted to use WGPU<!#><!/>. While we already had a CUDA backend with LibTorch, we recognized the value of starting with a cross-platform backend rather than solely focusing on CUDA. Initially, we contemplated using the Vulkan API, but we acknowledged that it is not universally supported across all devices. WGPU, on the other hand, offers several advantages. It stands out as the most popular and well-supported Rust graphics library, with numerous packages built upon it. WGPU emerged as the optimal solution for Cross-Platform GPU programming, automatically targeting Vulkan, OpenGL, Metal, Direct X11/12, and WebGPU.</p><h2>Cross-platform Applications</h2><h3>Video games</h3><p>Burn is not the sole crate that harnesses WGPU for cross-platform graphics. It finds widespread adoption in various domains, including game engines like Bevy<!#><!/> and UI frameworks like Egui<!#><!/>. The potential of integrating deep learning models with Burn into game engines excites us, as it eliminates the need for Python and enables the utilization of the same graphics API as the game itself. The prospect of incorporating on-device learning into game mechanics opens up countless exciting possibilities. This is something not achievable when deploying trained models with only an inference runtime. We eagerly await the development of future Pokemon and Tamagotchi games ðŸ˜….</p><h3>Privacy</h3><p>With the comprehensive support from the entire Burn ecosystem, we have the capability to deploy on any device, which in turn opens up possibilities for models to be trained locally using the user's GPU. This becomes particularly advantageous when handling sensitive data, as it ensures that information remains secured without being transmitted to an untrusted server. This localized approach proves highly beneficial for tasks involving biometric recognition, allowing training on personal sensitive data while avoiding sharing it with large corporations. Even when using only inference, this approach proves valuable, especially in the context of language models where you might not want your prompts to be tracked. Naturally, to implement this, the model weights would need to be downloaded to the device, which may make handling very large models impractical at this stage. Nonetheless, the potential applications and privacy benefits are significant and promising.</p><h3>Web and mobile</h3><p>The use of GPUs within browsers has historically been challenging, given the numerous limitations associated with WebGL for computations<!#><!/>. However, substantial efforts have been made to address these issues with the introduction of the next generation of GPU APIs. Notably, Google has developed WebGPU , and although browser support is relatively recent<!#><!/>, it shows great promise. We believe that this technology holds vast potential for professional web applications, enabling them to significantly reduce latency and enhance privacy, particularly in image processing and video editing applications. Additionally, mobile applications can benefit from the ability to seamlessly support offline mode by running inference locally. For instance, translation applications can leverage on-device processing to translate foreign languages while traveling abroad without requiring internet access.</p><h2>Hardware</h2><p>NVIDIA has dominated the GPGPU field since the introduction of CUDA in 2007<!#><!/>. It played a crucial role in the development of AlexNet, the first large-scale deep learning model that demonstrated superior results over classical machine learning techniques<!#><!/>. Consequently, many tools have been built around NVIDIA's hardware, potentially leading to an industry-wide dependency that may not be advantageous for consumers. However, with WGPU supporting all major graphics APIs, any hardware manufacturer can now run Burn models. This opens up the possibility of training models locally on GPUs from AMD, Intel, and Mac OS, greatly enhancing the development experience for those without an NVIDIA GPU. Furthermore, WGPU proves to be a valuable asset for deploying models on embedded devices with GPU capabilities, further expanding its versatility.</p><h2>Performance</h2><p>In this initial iteration of the backend, our primary focus lay on ensuring correctness, refining the GPU computing pipeline, and enhancing GPU memory management. While some efforts have been devoted to performance optimization, like implementing a tiling 2D algorithm for the matrix multiplication kernel, further enhancements, such as warptiling, can unlock even greater speed improvements. Presently, all kernels are written using WGSL, but in the future, we may explore using SPIR-V pass-through with Vulkan vendor graphics extensions to capitalize on hardware-specific features like Tensor Cores on NVIDIA GPUs. To facilitate the optimization process, we have established a robust benchmark system. This system allows us to compare various kernels executed on different hardware configurations with distinct parameters. Eventually, this benchmark system will serve as the foundation for implementing automatic kernel selection, commonly known as autotuning. By identifying the most performant kernels for the specific hardware and graphics API in use, we aim to streamline and enhance Burn's performance in a dynamic and adaptive manner.</p><h2>Conclusion</h2><p>WGPU serves as our initial cross-platform GPU backend implementation, providing essential capabilities for deploying Burn models on various hardware setups. Its support will be instrumental in implementing backend features like operation fusion, autotuning, and even quantization. We are genuinely interested in witnessing the potential of deep learning in diverse areas, including video games, web, mobile, and embedded applications. Supporting the community's use of Burn as they develop new applications is highly motivating for us. We look forward to discovering what will be built with Burn and WGPU.</p><h2>References</h2><!#><!/></div>",52),xe=()=>{const i=new he,b=i.addReference("WGPU: Cross-platform, safe, pure-rust graphics api","https://wgpu.rs/"),w=i.addReference("Bevy: A refreshingly simple data-driven game engine built in Rust","https://bevyengine.org/"),v=i.addReference("Egui: an easy-to-use immediate mode GUI in Rust that runs on both web and native","https://github.com/emilk/egui"),y=i.addReference("WebGPU computations performance in comparison to WebGL","https://pixelscommander.com/javascript/webgpu-computations-performance-in-comparison-to-webgl/"),x=i.addReference("Chrome ships WebGPU","https://developer.chrome.com/blog/webgpu-release/"),_=i.addReference("ImageNet Classification with Deep Convolutional Neural Networks","https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"),$=i.addReference("Cuda 1.0 Release","https://www.scientific-computing.com/press-releases/cuda-10"),P=i.addReference("How Nvidia dominated AI â€” and plans to keep it that way as generative AI explodes","https://venturebeat.com/ai/how-nvidia-dominated-ai-and-plans-to-keep-it-that-way-as-generative-ai-explodes/");return e(me,{get children(){return[e(de,{numStars:15,bot:30}),e(pe,{props:ge,get children(){return(()=>{const l=le(ue),S=l.firstChild,k=S.nextSibling,c=k.nextSibling,G=c.firstChild,U=G.nextSibling,[p,I]=n(U.nextSibling);p.nextSibling;const W=c.nextSibling,A=W.nextSibling,o=A.nextSibling,C=o.firstChild,T=C.nextSibling,[h,B]=n(T.nextSibling),R=h.nextSibling,N=R.nextSibling,[d,D]=n(N.nextSibling);d.nextSibling;const z=o.nextSibling,V=z.nextSibling,L=V.nextSibling,s=L.nextSibling,E=s.firstChild,q=E.nextSibling,[g,H]=n(q.nextSibling),M=g.nextSibling,F=M.nextSibling,[m,j]=n(F.nextSibling);m.nextSibling;const O=s.nextSibling,r=O.nextSibling,X=r.firstChild,J=X.nextSibling,[u,K]=n(J.nextSibling),Q=u.nextSibling,Y=Q.nextSibling,[f,Z]=n(Y.nextSibling);f.nextSibling;const ee=r.nextSibling,ie=ee.nextSibling,ne=ie.nextSibling,te=ne.nextSibling,ae=te.nextSibling,oe=ae.nextSibling,[se,re]=n(oe.nextSibling);return t(c,e(a,{get references(){return[b.ref()]}}),p,I),t(o,e(a,{get references(){return[w.ref()]}}),h,B),t(o,e(a,{get references(){return[v.ref()]}}),d,D),t(s,e(a,{get references(){return[y.ref()]}}),g,H),t(s,e(a,{get references(){return[x.ref()]}}),m,j),t(r,e(a,{get references(){return[$.ref(),P.ref()]}}),u,K),t(r,e(a,{get references(){return[_.ref()]}}),f,Z),t(l,()=>i.generate(),se,re),l})()}})]}})};export{xe as default};
