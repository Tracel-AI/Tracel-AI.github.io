<!DOCTYPE html><html lang="en" ><head ><script data-hk="0-0-0-0-0-0-0-0-0-0-0" src="/analytics.js"></script><script data-hk="0-0-0-0-0-0-0-0-0-0-1" async src="https://www.googletagmanager.com/gtag/js?id=G-S6CQMW5DNY"></script><meta data-sm="0-0-0-0-0-0-0-0-0-0-2-0" charset="utf-8"/><meta data-sm="0-0-0-0-0-0-0-0-0-0-3-0" name="viewport" content="width=device-width, initial-scale=1"/><link data-sm="0-0-0-0-0-0-0-0-0-0-4-0" rel="manifest" href="/manifest.webmanifest"/><meta data-sm="0-0-0-0-0-0-0-0-0-0-5-0" name="description" content="Burn Unstoppable Rusty Neurons"/><link data-sm="0-0-0-0-0-0-0-0-0-0-6-0" rel="icon" href="/favicon.ico" type="image/png" sizes="16x16"/><link data-sm="0-0-0-0-0-0-0-0-0-0-7-0" rel="apple-touch-icon" href="/pwa-192x192.png" sizes="192x192"/><meta data-sm="0-0-0-0-0-0-0-0-0-0-8-0" name="theme-color" content="#202A37"/><title data-sm="0-0-0-0-0-0-0-0-0-0-9-0">Burn - Deep Learning Framework</title><meta data-sm="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-11-0" property="og:type" content="article"/><meta data-sm="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-12-0" property="og:title" content="Burn's New Cross-Platform GPU Backend"/><meta data-sm="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-13-0" property="og:description" content="Introducing Burn's new Cross-Platform GPU Backend built using WGPU. Burn now supports running deep learning models on a variety of hardware configurations, leveraging graphics APIs such as Vulkan, DirectX 11/12, Metal, OpenGL, and WebGPU. We discuss the possible applications in various domains and glimpse into the promising future of the framework."/><meta data-sm="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-14-0" property="og:author" content="Nathaniel Simard, Louis Fortier-Dubois"/><meta data-sm="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-15-0" property="og:image" content="/blog3.jpeg"/><meta data-sm="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-16-0" property="article:published_time" content="2023-07-25T18:00:00.000Z"/><link rel="modulepreload" href="/assets/cross-platform-gpu-backend-2021b3c2.js"><link rel="modulepreload" href="/assets/entry-client-4d19b180.js"><link rel="stylesheet" href="/assets/entry-client-512d4306.css"><link rel="modulepreload" href="/assets/index-e39a1bee.js"><link rel="modulepreload" href="/assets/index-1ef6eb89.js"><link rel="modulepreload" href="/assets/blogs-0558d79c.js"><link rel="modulepreload" href="/assets/page-80f6abfb.js"></head><body class="overflow-x-hidden " ><!e0-0-0-0-0-0-0-0-0-1-0-0-0><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-0" class="dark"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-0" class="bg-gray-800 min-h-[100vh] w-full flex flex-col"><nav class="fixed w-full px-10 py-5 z-50 flex items-center text-gray-50 font-semibold transition-colors false"><a href="/" class="text-3xl hover:scale-110 transition-all">Burn</a><ul class="ml-auto flex space-x-12 text-xl"><li class="hover:scale-110 transition-all"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-1-0" link="true" href="/demo" class="text-white inactive" >Demo</a></li><li class="hover:scale-110 transition-all"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-2-0" link="true" href="/blog" class="text-white active" aria-current="page">Blog</a></li><li class="hover:scale-110 transition-all"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-3-0" target="_blank" rel="noreferrer" href="https://burn.dev/book/" class="text-white">Book</a></li><li class="hidden sm:block"><iframe src="https://ghbtns.com/github-btn.html?user=tracel-ai&amp;repo=burn&amp;type=star&amp;count=true&amp;size=large" width="170" height="30" title="GitHub"></iframe></li></ul></nav><!--#--><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-0" id="star-0" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-1" id="star-1" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-2" id="star-2" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-3" id="star-3" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-4" id="star-4" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-5" id="star-5" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-6" id="star-6" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-7" id="star-7" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-8" id="star-8" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-9" id="star-9" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-10" id="star-10" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-11" id="star-11" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-12" id="star-12" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-13" id="star-13" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-4-0-14" id="star-14" class="absolute bg-[#EBC65D] rounded-full invisible sm:visible"></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-10" class="flex justify-center w-full pt-20 bg-gradient-to-b from-[#202124] to-gray-800"><div class="w-full max-w-5xl mb-10 mx-3"><!--#--><!--/--><!--#--><!--/--><!--#--><!--/--><!--#--><!--/--><!--#--><!--/--><!--#--><!--/--><div class="mb-3"><p class="text-white font-bold text-xl px-2"><a href="/" class="hover:text-[#edc567]">burn</a><!--#--><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-17-0"><span> · </span><a href="/blog" class="hover:text-[#edc567]">blog</a></span><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-17-1"><span> · </span><a href="/blog/cross-platform-gpu-backend" class="hover:text-[#edc567]">cross-platform-gpu-backend</a></span><!--/--></p></div><article class="blog rounded-lg bg-white/5 pt-4"><div><h1 class="px-3 sm:px-8">Burn's New Cross-Platform GPU Backend</h1><div class="px-3 pb-4 sm:px-8"><img class="h-48 w-full rounded-lg mr-3 object-cover object-top" src="/blog3.jpeg" alt="Space digital art generated by stable diffusion."></div><div class="flex px-3 sm:px-8"><div class="flex"><div class="h-5 w-5 i-mdi-clipboard-text-clock"></div><span class="px-2">Tue Jul 25 2023</span></div><!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-18-0" class="pl-2 flex" href="https://x.com/nath_simard" target="_blank"><div class="h-5 w-5 i-mdi-account-edit"></div><span class="px-2">Nathaniel Simard</span></a><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-18-1" class="pl-2 flex" href="https://x.com/louisfd94" target="_blank"><div class="h-5 w-5 i-mdi-account-edit"></div><span class="px-2">Louis Fortier-Dubois</span></a><!--/--></div></div><div class="pb-4 px-3 sm:px-8"><div class="border-t-2 border-gray-900 my-6"></div><!--#--><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-0"><h2>Introduction</h2><p>Burn has had the capability to run deep learning models on GPU since its inception, thanks to its use of Tch (bindings to LibTorch). This approach has proven to be highly effective, allowing for rapid iteration on the architecture, user APIs, and the automatic differentiation system. The need to write kernels and apply low-level performance optimizations was alleviated, allowing the team to focus on making significant progress faster. As we reached a level of satisfaction with our advancements, we slowly transitioned our focus to lower-level implementations. This allows us to leverage Burn's type system and enhance hardware compatibility. Ultimately, to be able to maximize performance and apply specific optimizations such as operation fusion, we need control over GPU kernels; thus we decided to write our own GPU backend.</p><p>After careful consideration of the technology to employ for developing our first GPU backend, we opted to use WGPU<!--#--><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-1-1" class="reference px-1">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-1-0" class="hover:text-[#69b8e1]" href="#reference-1">1</a><!--/-->]</span><!--/-->. While we already had a CUDA backend with LibTorch, we recognized the value of starting with a cross-platform backend rather than solely focusing on CUDA. Initially, we contemplated using the Vulkan API, but we acknowledged that it is not universally supported across all devices. WGPU, on the other hand, offers several advantages. It stands out as the most popular and well-supported Rust graphics library, with numerous packages built upon it. WGPU emerged as the optimal solution for Cross-Platform GPU programming, automatically targeting Vulkan, OpenGL, Metal, Direct X11/12, and WebGPU.</p><h2>Cross-platform Applications</h2><h3>Video games</h3><p>Burn is not the sole crate that harnesses WGPU for cross-platform graphics. It finds widespread adoption in various domains, including game engines like Bevy<!--#--><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-2-1" class="reference px-1">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-2-0" class="hover:text-[#69b8e1]" href="#reference-2">2</a><!--/-->]</span><!--/--> and UI frameworks like Egui<!--#--><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-3-1" class="reference px-1">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-3-0" class="hover:text-[#69b8e1]" href="#reference-3">3</a><!--/-->]</span><!--/-->. The potential of integrating deep learning models with Burn into game engines excites us, as it eliminates the need for Python and enables the utilization of the same graphics API as the game itself. The prospect of incorporating on-device learning into game mechanics opens up countless exciting possibilities. This is something not achievable when deploying trained models with only an inference runtime. We eagerly await the development of future Pokemon and Tamagotchi games 😅.</p><h3>Privacy</h3><p>With the comprehensive support from the entire Burn ecosystem, we have the capability to deploy on any device, which in turn opens up possibilities for models to be trained locally using the user's GPU. This becomes particularly advantageous when handling sensitive data, as it ensures that information remains secured without being transmitted to an untrusted server. This localized approach proves highly beneficial for tasks involving biometric recognition, allowing training on personal sensitive data while avoiding sharing it with large corporations. Even when using only inference, this approach proves valuable, especially in the context of language models where you might not want your prompts to be tracked. Naturally, to implement this, the model weights would need to be downloaded to the device, which may make handling very large models impractical at this stage. Nonetheless, the potential applications and privacy benefits are significant and promising.</p><h3>Web and mobile</h3><p>The use of GPUs within browsers has historically been challenging, given the numerous limitations associated with WebGL for computations<!--#--><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-4-1" class="reference px-1">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-4-0" class="hover:text-[#69b8e1]" href="#reference-4">4</a><!--/-->]</span><!--/-->. However, substantial efforts have been made to address these issues with the introduction of the next generation of GPU APIs. Notably, Google has developed WebGPU , and although browser support is relatively recent<!--#--><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-5-1" class="reference px-1">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-5-0" class="hover:text-[#69b8e1]" href="#reference-5">5</a><!--/-->]</span><!--/-->, it shows great promise. We believe that this technology holds vast potential for professional web applications, enabling them to significantly reduce latency and enhance privacy, particularly in image processing and video editing applications. Additionally, mobile applications can benefit from the ability to seamlessly support offline mode by running inference locally. For instance, translation applications can leverage on-device processing to translate foreign languages while traveling abroad without requiring internet access.</p><h2>Hardware</h2><p>NVIDIA has dominated the GPGPU field since the introduction of CUDA in 2007<!--#--><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-6-2" class="reference px-1">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-6-0" class="hover:text-[#69b8e1]" href="#reference-6">6,</a><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-6-1" class="hover:text-[#69b8e1]" href="#reference-7">7</a><!--/-->]</span><!--/-->. It played a crucial role in the development of AlexNet, the first large-scale deep learning model that demonstrated superior results over classical machine learning techniques<!--#--><span data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-7-1" class="reference px-1">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-7-0" class="hover:text-[#69b8e1]" href="#reference-8">8</a><!--/-->]</span><!--/-->. Consequently, many tools have been built around NVIDIA's hardware, potentially leading to an industry-wide dependency that may not be advantageous for consumers. However, with WGPU supporting all major graphics APIs, any hardware manufacturer can now run Burn models. This opens up the possibility of training models locally on GPUs from AMD, Intel, and Mac OS, greatly enhancing the development experience for those without an NVIDIA GPU. Furthermore, WGPU proves to be a valuable asset for deploying models on embedded devices with GPU capabilities, further expanding its versatility.</p><h2>Performance</h2><p>In this initial iteration of the backend, our primary focus lay on ensuring correctness, refining the GPU computing pipeline, and enhancing GPU memory management. While some efforts have been devoted to performance optimization, like implementing a tiling 2D algorithm for the matrix multiplication kernel, further enhancements, such as warptiling, can unlock even greater speed improvements. Presently, all kernels are written using WGSL, but in the future, we may explore using SPIR-V pass-through with Vulkan vendor graphics extensions to capitalize on hardware-specific features like Tensor Cores on NVIDIA GPUs. To facilitate the optimization process, we have established a robust benchmark system. This system allows us to compare various kernels executed on different hardware configurations with distinct parameters. Eventually, this benchmark system will serve as the foundation for implementing automatic kernel selection, commonly known as autotuning. By identifying the most performant kernels for the specific hardware and graphics API in use, we aim to streamline and enhance Burn's performance in a dynamic and adaptive manner.</p><h2>Conclusion</h2><p>WGPU serves as our initial cross-platform GPU backend implementation, providing essential capabilities for deploying Burn models on various hardware setups. Its support will be instrumental in implementing backend features like operation fusion, autotuning, and even quantization. We are genuinely interested in witnessing the potential of deep learning in diverse areas, including video games, web, mobile, and embedded applications. Supporting the community's use of Burn as they develop new applications is highly motivating for us. We look forward to discovering what will be built with Burn and WGPU.</p><h2>References</h2><!--#--><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-8"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-0-0" id="reference-1"><span class="pr-2">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-0-1-0" link="true" target="_blank" href="https://wgpu.rs/" class="hover:font-bold text-[#69b8e1] inactive" >1</a><!--/-->]</span><!--#-->WGPU: Cross-platform, safe, pure-rust graphics api<!--/--></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-1-0" id="reference-2"><span class="pr-2">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-1-1-0" link="true" target="_blank" href="https://bevyengine.org/" class="hover:font-bold text-[#69b8e1] inactive" >2</a><!--/-->]</span><!--#-->Bevy: A refreshingly simple data-driven game engine built in Rust<!--/--></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-2-0" id="reference-3"><span class="pr-2">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-2-1-0" link="true" target="_blank" href="https://github.com/emilk/egui" class="hover:font-bold text-[#69b8e1] inactive" >3</a><!--/-->]</span><!--#-->Egui: an easy-to-use immediate mode GUI in Rust that runs on both web and native<!--/--></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-3-0" id="reference-4"><span class="pr-2">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-3-1-0" link="true" target="_blank" href="https://pixelscommander.com/javascript/webgpu-computations-performance-in-comparison-to-webgl/" class="hover:font-bold text-[#69b8e1] inactive" >4</a><!--/-->]</span><!--#-->WebGPU computations performance in comparison to WebGL<!--/--></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-4-0" id="reference-5"><span class="pr-2">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-4-1-0" link="true" target="_blank" href="https://developer.chrome.com/blog/webgpu-release/" class="hover:font-bold text-[#69b8e1] inactive" >5</a><!--/-->]</span><!--#-->Chrome ships WebGPU<!--/--></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-5-0" id="reference-6"><span class="pr-2">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-5-1-0" link="true" target="_blank" href="https://www.scientific-computing.com/press-releases/cuda-10" class="hover:font-bold text-[#69b8e1] inactive" >6</a><!--/-->]</span><!--#-->Cuda 1.0 Release<!--/--></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-6-0" id="reference-7"><span class="pr-2">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-6-1-0" link="true" target="_blank" href="https://venturebeat.com/ai/how-nvidia-dominated-ai-and-plans-to-keep-it-that-way-as-generative-ai-explodes/" class="hover:font-bold text-[#69b8e1] inactive" >7</a><!--/-->]</span><!--#-->How Nvidia dominated AI — and plans to keep it that way as generative AI explodes<!--/--></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-7-0" id="reference-8"><span class="pr-2">[<!--#--><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-5-9-7-1-0" link="true" target="_blank" href="https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" class="hover:font-bold text-[#69b8e1] inactive" >8</a><!--/-->]</span><!--#-->ImageNet Classification with Deep Convolutional Neural Networks<!--/--></div></div><!--/--></div><!--/--></div></article></div></div><!--/--><div class="w-full flex justify-center pt-10 pb-10 border-t-2 border-gray-900 mt-10"><div class="grid md:grid-cols-3 gap-x-16 gap-y-6 md:gap-x-60 bg-gray-800 text-gray-500"><ul data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-0" class="space-y-2"><h3 class="uppercase mb-2 font-bold">examples</h3><!--#--><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-0"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-1-0" target="_blank" rel="noreferrer" href="https://github.com/tracel-ai/burn/tree/main/examples/mnist"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-1-1" class="flex items-center"><!--#--><!--/--><!--#-->MNIST<!--/--></div></a></li><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-2"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-3-0" target="_blank" rel="noreferrer" href="https://github.com/tracel-ai/burn/tree/main/examples/text-classification"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-3-1" class="flex items-center"><!--#--><!--/--><!--#-->Text Classification<!--/--></div></a></li><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-4"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-5-0" target="_blank" rel="noreferrer" href="https://github.com/tracel-ai/burn/tree/main/examples/onnx-inference"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-1-5-1" class="flex items-center"><!--#--><!--/--><!--#-->ONNX Inference<!--/--></div></a></li><!--/--></ul><ul data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-2" class="space-y-2"><h3 class="uppercase mb-2 font-bold">community</h3><!--#--><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-3-0"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-3-1-0" target="_blank" rel="noreferrer" href="https://github.com/tracel-ai/burn"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-3-1-1" class="flex items-center"><!--#--><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-3-1-2" class="text-xl i-mdi-github mr-2"></div><!--/--><!--#-->Github<!--/--></div></a></li><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-3-2"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-3-3-0" target="_blank" rel="noreferrer" href="https://discord.gg/uPEBbYYDB6"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-3-3-1" class="flex items-center"><!--#--><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-3-3-2" class="text-xl i-mdi-discord mr-2"></div><!--/--><!--#-->Discord<!--/--></div></a></li><!--/--></ul><ul data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-4" class="space-y-2"><h3 class="uppercase mb-2 font-bold">about</h3><!--#--><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-0"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-1-0" target="_blank" rel="noreferrer" href="https://burn.dev/docs/burn"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-1-1" class="flex items-center"><!--#--><!--/--><!--#-->Documentation<!--/--></div></a></li><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-2"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-3-0" target="_blank" rel="noreferrer" href="/book"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-3-1" class="flex items-center"><!--#--><!--/--><!--#-->Book<!--/--></div></a></li><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-4"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-5-0" target="_blank" rel="noreferrer" href="https://crates.io/crates/burn"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-5-1" class="flex items-center"><!--#--><!--/--><!--#-->Crates.io<!--/--></div></a></li><li data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-6"><a data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-7-0" target="_blank" rel="noreferrer" href="https://github.com/tracel-ai/burn#license"><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-1-1-0-0-0-6-5-7-1" class="flex items-center"><!--#--><!--/--><!--#-->License<!--/--></div></a></li><!--/--></ul></div></div></div><div data-hk="0-0-0-0-0-0-0-0-0-1-0-0-0-0-0-0-2-0" style="position:fixed;z-index:9999;top:16px;bottom:16px;left:16px;right:16px;pointer-events:none" class=""><style>.sldt-active{z-index:9999;}.sldt-active>*{pointer-events:auto;}</style><!--#--><!--/--></div></div><!/e0-0-0-0-0-0-0-0-0-1-0-0-0><script>(e=>{let t=e=>e&&e.hasAttribute&&(e.hasAttribute("data-hk")?e:t(e.host&&e.host instanceof Node?e.host:e.parentNode));["click", "input"].forEach((o=>document.addEventListener(o,(o=>{let s=o.composedPath&&o.composedPath()[0]||o.target,a=t(s);a&&!e.completed.has(a)&&e.events.push([a,o])}))))})(window._$HY||(_$HY={events:[],completed:new WeakSet,r:{},fe(){},init(e,t){_$HY.r[e]=[new Promise((e=>t=e)),t]},set(e,t,o){(o=_$HY.r[e])&&o[1](t),_$HY.r[e]=[t]},unset(e){delete _$HY.r[e]},load:e=>_$HY.r[e]}));</script><!--xs--><script type="module" async src="/assets/entry-client-4d19b180.js"></script></body></html>