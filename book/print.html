<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Burn Book</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="motivation.html"><strong aria-hidden="true">2.</strong> Why Burn?</a></li><li class="chapter-item expanded "><a href="getting-started.html"><strong aria-hidden="true">3.</strong> Getting started</a></li><li class="chapter-item expanded "><a href="basic-workflow/index.html"><strong aria-hidden="true">4.</strong> Basic Workflow: From Training to Inference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="basic-workflow/model.html"><strong aria-hidden="true">4.1.</strong> Model</a></li><li class="chapter-item expanded "><a href="basic-workflow/data.html"><strong aria-hidden="true">4.2.</strong> Data</a></li><li class="chapter-item expanded "><a href="basic-workflow/training.html"><strong aria-hidden="true">4.3.</strong> Training</a></li><li class="chapter-item expanded "><a href="basic-workflow/backend.html"><strong aria-hidden="true">4.4.</strong> Backend</a></li><li class="chapter-item expanded "><a href="basic-workflow/inference.html"><strong aria-hidden="true">4.5.</strong> Inference</a></li><li class="chapter-item expanded "><a href="basic-workflow/conclusion.html"><strong aria-hidden="true">4.6.</strong> Conclusion</a></li></ol></li><li class="chapter-item expanded "><a href="building-blocks/index.html"><strong aria-hidden="true">5.</strong> Building Blocks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="building-blocks/backend.html"><strong aria-hidden="true">5.1.</strong> Backend</a></li><li class="chapter-item expanded "><a href="building-blocks/tensor.html"><strong aria-hidden="true">5.2.</strong> Tensor</a></li><li class="chapter-item expanded "><a href="building-blocks/autodiff.html"><strong aria-hidden="true">5.3.</strong> Autodiff</a></li><li class="chapter-item expanded "><a href="building-blocks/module.html"><strong aria-hidden="true">5.4.</strong> Module</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.5.</strong> Config</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.6.</strong> Learner</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.7.</strong> Metric</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.8.</strong> Record</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.9.</strong> Dataset</div></li></ol></li><li class="chapter-item expanded "><a href="import/onnx-model.html"><strong aria-hidden="true">6.</strong> Import ONNX Model</a></li><li class="chapter-item expanded "><a href="advanced/index.html"><strong aria-hidden="true">7.</strong> Advanced</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="advanced/backend-extension/index.html"><strong aria-hidden="true">7.1.</strong> Backend Extension</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="advanced/backend-extension/custom-wgpu-kernel.html"><strong aria-hidden="true">7.1.1.</strong> Custom WGPU Kernel</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.2.</strong> Custom Training Loops</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.3.</strong> Custom Optimizer</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.4.</strong> Custom Metric</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.5.</strong> Custom Kernels</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">7.5.1.</strong> WGPU</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.6.</strong> WebAssembly</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.7.</strong> No-Std</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">8.</strong> Terms and Concepts</div></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Burn Book</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>This book will help you get started with the Burn deep learning framework.</p>
<p>Although it is still in progress, it is planned to be divided into multiple sections, catering to
complete beginners and advanced users. For the time being, we offer an introductory guide in which
we perform training and inference on a simple model, while highlighting certain peculiarities of
Burn that set it apart from other frameworks.</p>
<p>Throughout the book, we assume a basic understanding of Rust and deep learning concepts.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="why-burn"><a class="header" href="#why-burn">Why Burn?</a></h1>
<p>Why bother with the effort of creating an entirely new deep learning framework from scratch when
PyTorch, TensorFlow, and other frameworks already exist? Spoiler alert: Burn isn't merely a
replication of PyTorch or TensorFlow in Rust. It represents a novel approach, placing significant
emphasis on making the right compromises in the right areas to facilitate exceptional flexibility,
high performance, and a seamless developer experience. Burn isn’t a framework specialized for only
one type of application, it is designed to serve as a versatile framework suitable for a wide range
of research and production uses. The foundation of Burn's design revolves around three key user
profiles:</p>
<p><strong>Machine Learning Researchers</strong> require tools to construct and execute experiments efficiently.
It’s essential for them to iterate quickly on their ideas and design testable experiments which can
help them discover new findings. The framework should facilitate the swift implementation of
cutting-edge research while ensuring fast execution for testing.</p>
<p><strong>Machine Learning Engineers</strong> are another important demographic to keep in mind. Their focus leans
less on swift implementation and more on establishing robustness, seamless deployment, and
cost-effective operations. They seek dependable, economical models capable of achieving objectives
without excessive expense. The whole machine learning workflow —from training to inference— must be
as efficient as possible with minimal unpredictable behavior.</p>
<p><strong>Low level Software Engineers</strong> working with hardware vendors want their processing units to run
models as fast as possible to gain competitive advantage. This endeavor involves harnessing
hardware-specific features such as Tensor Core for Nvidia. Since they are mostly working at a system
level, they want to have absolute control over how the computation will be executed.</p>
<p>The goal of Burn is to satisfy all of those personas!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<h2 id="installing-rust"><a class="header" href="#installing-rust">Installing Rust</a></h2>
<p>Burn is a deep learning framework in the Rust programming language. Therefore, it goes without
saying that one must have basic notions of Rust. Reading the first chapters of the
<a href="https://doc.rust-lang.org/book/">Rust book</a> is a great way to begin.</p>
<p>In particular, the books'
<a href="https://doc.rust-lang.org/book/ch01-01-installation.html">installation page</a> explains in details
the most convenient way for you to install Rust on your computer, which is the very first thing to
do in order to run Burn.</p>
<h2 id="creating-a-burn-application"><a class="header" href="#creating-a-burn-application">Creating a Burn application</a></h2>
<p>Once Rust is correctly installed, create a new Rust application by using Rust's package manager
Cargo, which was installed with Rust. In the directory of your choice, run</p>
<pre><code class="language-console">cargo new my_burn_app
</code></pre>
<p>This will create the <code>my_burn_app</code> project directory. Head inside and open the <code>Cargo.toml</code> file. It
should contain something like:</p>
<pre><code class="language-toml">[package]
name = &quot;my_burn_app&quot;
version = &quot;0.1.0&quot;
edition = &quot;2021&quot;

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
</code></pre>
<p>Under dependencies, add</p>
<pre><code class="language-toml">burn = &quot;0.9.0&quot;
burn-wgpu = &quot;0.9.0&quot;
</code></pre>
<p>Then, to compile the dependencies, execute</p>
<pre><code class="language-console">cargo build
</code></pre>
<p>This will install Burn, along with the WGPU backend for Burn, which allows to execute low-level
operations on every platform, using the GPU.</p>
<h2 id="writing-a-code-snippet"><a class="header" href="#writing-a-code-snippet">Writing a code snippet</a></h2>
<p>Now open <code>src/main.rs</code> and replace its content with</p>
<pre><pre class="playground"><code class="language-rust">use burn::tensor::Tensor;
use burn_wgpu::{AutoGraphicsApi, WgpuBackend};

fn main() {
    // Type alias to be cleaner
    type Backend = WgpuBackend&lt;AutoGraphicsApi, f32, i32&gt;;

    // Creation of two tensors, the first with explicit values and the second one with ones, with the same shape as the first
    let tensor_1 = Tensor::&lt;Backend, 2&gt;::from_data([[2., 3.], [4., 5.]]);
    let tensor_2 = Tensor::&lt;Backend, 2&gt;::ones_like(&amp;tensor_1);

    // Print the element-wise addition (done with the WGPU backend) of the two tensors.
    println!(&quot;{}&quot;, tensor_1 + tensor_2);
}</code></pre></pre>
<p>By running <code>cargo run</code>, you should now see the result of the addition:</p>
<pre><code class="language-console">Tensor {
  data: [[3.0, 4.0], [5.0, 6.0]],
  shape:  [2, 2],
  device:  BestAvailable,
  backend:  &quot;wgpu&quot;,
  kind:  &quot;Float&quot;,
  dtype:  &quot;f32&quot;,
}
</code></pre>
<p>While the previous example is somewhat trivial, the upcoming
<a href="./basic-workflow/README.html">basic workflow section</a> will walk you through a much more relevant
example for deep learning applications.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="guide"><a class="header" href="#guide">Guide</a></h1>
<p>This guide will walk you through the process of creating a custom model built with Burn. We will
train a simple convolutional neural network model on the MNIST dataset and prepare it for inference.</p>
<p>For clarity, we sometimes omit imports in our code snippets. For more details, please refer to the
corresponding code in the <code>examples/guide</code> directory.</p>
<h2 id="key-learnings"><a class="header" href="#key-learnings">Key Learnings</a></h2>
<ul>
<li>Creating a project</li>
<li>Creating neural network models</li>
<li>Importing and preparing datasets</li>
<li>Training models on data</li>
<li>Choosing a backend</li>
<li>Using a model for inference</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model"><a class="header" href="#model">Model</a></h1>
<p>The first step is to create a project and add the different Burn dependencies. In a <code>Cargo.toml</code>
file, add the <code>burn</code>, <code>burn-wgpu</code>, <code>burn-dataset</code>, <code>burn-autodiff</code> and <code>burn-train</code>. Note that the
<code>serde</code> dependency is necessary for serialization and is mandatory for the time being.</p>
<pre><code class="language-toml">[package]
edition = &quot;2021&quot;
name = &quot;My first Burn model&quot;

[dependencies]
burn = &quot;0.9&quot;
burn-wgpu = &quot;0.9&quot;
burn-dataset = &quot;0.9&quot;
burn-autodiff = &quot;0.9&quot;
burn-train = &quot;0.9&quot;

# Serialization
serde = &quot;1&quot;
</code></pre>
<p>Our goal will be to create a basic convolutional neural network used for image classification. We
will keep the model simple by using two convolution layers followed by two linear layers, some
pooling and ReLU activations. We will also use dropout to improve training performance.</p>
<p>Let us start by creating a model in a file <code>model.rs</code>.</p>
<pre><code class="language-rust   ignore">// Import required for the model.rs file
use burn::{
    config::Config,
    module::Module,
    nn::{
        conv::{Conv2d, Conv2dConfig},
        pool::{AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},
        Dropout, DropoutConfig, Linear, LinearConfig, ReLU,
    },
    tensor::{backend::Backend, Tensor},
};

#[derive(Module, Debug)]
pub struct Model&lt;B: Backend&gt; {
    conv1: Conv2d&lt;B&gt;,
    conv2: Conv2d&lt;B&gt;,
    pool: AdaptiveAvgPool2d,
    dropout: Dropout,
    linear1: Linear&lt;B&gt;,
    linear2: Linear&lt;B&gt;,
    activation: ReLU,
}</code></pre>
<p>There are two major things going on in this code sample.</p>
<ol>
<li>
<p>You can create a deep learning module with the <code>#[derive(Module)]</code> attribute on top of a struct.
This will generate the necessary code so that the struct implements the <code>Module</code> trait. This
trait will make your module both trainable and (de)serializable while adding related
functionalities. Like other attributes often used in Rust, such as <code>Clone</code>, <code>PartialEq</code> or
<code>Debug</code>, each field within the struct must also implement the <code>Module</code> trait.</p>
</li>
<li>
<p>Note that the struct is generic over the <code>Backend</code> trait. The backend trait abstracts the
underlying low level implementations of tensor operations, allowing your new model to run on any
backend. Contrary to other frameworks, the backend abstraction isn't determined by a compilation
flag or a device type. This is important because you can extend the functionalities of a specific
backend (which will be covered in the more advanced sections of this book), and it allows for an
innovative autodiff system. You can also change backend during runtime, for instance to compute
training metrics on a cpu backend while using a gpu one only to train the model. In our example,
the backend in use will be determined later on.</p>
</li>
</ol>
<p>Next, we need to instantiate the model for training.</p>
<pre><code class="language-rust   ignore">#[derive(Config, Debug)]
pub struct ModelConfig {
    num_classes: usize,
    hidden_size: usize,
    #[config(default = &quot;0.5&quot;)]
    dropout: f64,
}

impl ModelConfig {
    /// Returns the initialized model.
    pub fn init&lt;B: Backend&gt;(&amp;self) -&gt; Model&lt;B&gt; {
        Model {
            conv1: Conv2dConfig::new([1, 8], [3, 3]).init(),
            conv2: Conv2dConfig::new([8, 16], [3, 3]).init(),
            pool: AdaptiveAvgPool2dConfig::new([8, 8]).init(),
            activation: ReLU::new(),
            linear1: LinearConfig::new(16 * 8 * 8, self.hidden_size).init(),
            linear2: LinearConfig::new(self.hidden_size, self.num_classes).init(),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }
}</code></pre>
<p>When creating a custom neural network module, it is often a good idea to create a config alongside
the model struct. This allows you to define default values for your network, thanks to the <code>Config</code>
attribute. The benefit of this attribute is that it makes the configuration serializable, enabling
you to painlessly save your model hyperparameters, enhancing your experimentation process. Note that
a constructor will automatically be generated for your configuration, which will take as input
values for the parameter which do not have default values:
<code>let config = ModelConfig::new(num_classes, hidden_size);</code>. The default values can be overridden
easily with builder-like methods: (e.g <code>config.with_dropout(0.2);</code>)</p>
<p>The first implementation block is related to the initialization method. As we can see, all fields
are set using the configuration of the corresponding neural network underlying module. In this
specific case, we have chosen to expand the tensor channels from 1 to 8 with the first layer, then
from 8 to 16 with the second layer, using a kernel size of 3 on all dimensions. We also use the
adaptive average pooling module to reduce the dimensionality of the images to an 8 by 8 matrix,
which we will flatten in the forward pass to have a 1024 (16 _ 8 _ 8) resulting tensor.</p>
<p>Now let's see how the forward pass is defined.</p>
<pre><code class="language-rust   ignore">impl&lt;B: Backend&gt; Model&lt;B&gt; {
    /// # Shapes
    ///   - Images [batch_size, height, width]
    ///   - Output [batch_size, num_classes]
    pub fn forward(&amp;self, images: Tensor&lt;B, 3&gt;) -&gt; Tensor&lt;B, 2&gt; {
        let [batch_size, height, width] = images.dims();

        // Create a channel at the second dimension.
        let x = images.reshape([batch_size, 1, height, width]);


        let x = self.conv1.forward(x); // [batch_size, 8, _, _]
        let x = self.dropout.forward(x);
        let x = self.conv2.forward(x); // [batch_size, 16, _, _]
        let x = self.dropout.forward(x);
        let x = self.activation.forward(x);

        let x = self.pool.forward(x); // [batch_size, 16, 8, 8]
        let x = x.reshape([batch_size, 16 * 8 * 8]);
        let x = self.linear1.forward(x);
        let x = self.dropout.forward(x);
        let x = self.activation.forward(x);

        self.linear2.forward(x) // [batch_size, num_classes]
    }
}</code></pre>
<p>For former PyTorch users, this might feel very intuitive, as each module is directly incorporated
into the code using an eager API. Note that no abstraction is imposed for the forward method. You
are free to define multiple forward functions with the names of your liking. Most of the neural
network modules already built with Burn use the <code>forward</code> nomenclature, simply because it is
standard in the field.</p>
<p>Similar to neural network modules, the <code>Tensor</code> struct given as a parameter also takes the Backend
trait as a generic argument, alongside its rank. Even if it is not used in this specific example, it
is possible to add the kind of the tensor as a third generic argument.</p>
<pre><code class="language-rust   ignore">Tensor&lt;B, 3&gt; // Float tensor (default)
Tensor&lt;B, 3, Float&gt; // Float tensor (explicit)
Tensor&lt;B, 3, Int&gt; // Int tensor
Tensor&lt;B, 3, Bool&gt; // Bool tensor</code></pre>
<p>Note that the specific element type, such as <code>f16</code>, <code>f32</code> and the likes, will be defined later with
the backend.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data"><a class="header" href="#data">Data</a></h1>
<p>Typically, one trains a model on some dataset. Burn provides a library of very useful dataset
sources and transformations. In particular, there are Hugging Face dataset utilities that allow to
download and store data from Hugging Face into an SQLite database for extremely efficient data
streaming and storage. For this guide, we will use the MNIST dataset provided by Hugging Face.</p>
<p>To iterate over a dataset efficiently, we will define a struct which will implement the <code>Batcher</code>
trait. The goal of a batcher is to map individual dataset items into a batched tensor that can be
used as input to our previously defined model.</p>
<pre><code class="language-rust   ignore">use burn::{
    data::{dataloader::batcher::Batcher, dataset::source::huggingface::MNISTItem},
    tensor::{backend::Backend, Data, ElementConversion, Int, Tensor},
};

pub struct MNISTBatcher&lt;B: Backend&gt; {
    device: B::Device,
}

impl&lt;B: Backend&gt; MNISTBatcher&lt;B&gt; {
    pub fn new(device: B::Device) -&gt; Self {
        Self { device }
    }
}
</code></pre>
<p>This codeblock defines a batcher struct with the device in which the tensor should be sent before
being passed to the model. Note that the device is an associative type of the <code>Backend</code> trait since
not all backends expose the same devices. As an example, the Libtorch-based backend exposes
<code>Cuda(gpu_index)</code>, <code>Cpu</code>, <code>Vulkan</code> and <code>Metal</code> devices, while the ndarray backend only exposes the
<code>Cpu</code> device.</p>
<p>Next, we need to actually implement the batching logic.</p>
<pre><code class="language-rust   ignore">#[derive(Clone, Debug)]
pub struct MNISTBatch&lt;B: Backend&gt; {
    pub images: Tensor&lt;B, 3&gt;,
    pub targets: Tensor&lt;B, 1, Int&gt;,
}

impl&lt;B: Backend&gt; Batcher&lt;MNISTItem, MNISTBatch&lt;B&gt;&gt; for MNISTBatcher&lt;B&gt; {
    fn batch(&amp;self, items: Vec&lt;MNISTItem&gt;) -&gt; MNISTBatch&lt;B&gt; {
        let images = items
            .iter()
            .map(|item| Data::&lt;f32, 2&gt;::from(item.image))
            .map(|data| Tensor::&lt;B, 2&gt;::from_data(data.convert()))
            .map(|tensor| tensor.reshape([1, 28, 28]))
            // Normalize: make between [0,1] and make the mean=0 and std=1
            // values mean=0.1307,std=0.3081 are from the PyTorch MNIST example
            // https://github.com/pytorch/examples/blob/54f4572509891883a947411fd7239237dd2a39c3/mnist/main.py#L122
            .map(|tensor| ((tensor / 255) - 0.1307) / 0.3081)
            .collect();

        let targets = items
            .iter()
            .map(|item| Tensor::&lt;B, 1, Int&gt;::from_data(Data::from([(item.label as i64).elem()])))
            .collect();

        let images = Tensor::cat(images, 0).to_device(&amp;self.device);
        let targets = Tensor::cat(targets, 0).to_device(&amp;self.device);

        MNISTBatch { images, targets }
    }
}</code></pre>
<p>In the previous example, we implement the <code>Batcher</code> trait with a list of <code>MNISTItem</code> as input and a
single <code>MNISTBatch</code> as output. The batch contains the images in the form of a 3D tensor, along with
a targets tensor that contains the indexes of the correct digit class. The first step is to parse
the image array into a <code>Data</code> struct. Burn provides the <code>Data</code> struct to encapsulate tensor storage
information without being specific for a backend. When creating a tensor from data, we often need to
convert the data precision to the current backend in use. This can be done with the <code>.convert()</code>
method. While importing the <code>burn::tensor::ElementConversion</code> trait, you can call <code>.elem()</code> on a
specific number to convert it to the current backend element type in use.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="training"><a class="header" href="#training">Training</a></h1>
<p>We are now ready to write the necessary code to train our model on the MNIST dataset. Instead of a
simple tensor, the model should output an item that can be understood by the learner, a struct whose
responsibility is to apply an optimizer to the model. The output struct is used for all metrics
calculated during the training. Therefore it should include all the necessary information to
calculate any metric that you want for a task.</p>
<p>Burn provides two basic output types: <code>ClassificationOutput</code> and <code>RegressionOutput</code>. They implement
the necessary trait to be used with metrics. It is possible to create your own item, but it is
beyond the scope of this guide.</p>
<p>Since the MNIST task is a classification problem, we will use the <code>ClassificationOutput</code> type.</p>
<pre><code class="language-rust   ignore">impl&lt;B: Backend&gt; Model&lt;B&gt; {
    pub fn forward_classification(
        &amp;self,
        images: Tensor&lt;B, 3&gt;,
        targets: Tensor&lt;B, 1, Int&gt;,
    ) -&gt; ClassificationOutput&lt;B&gt; {
        let output = self.forward(images);
        let loss = CrossEntropyLoss::new(None).forward(output.clone(), targets.clone());

        ClassificationOutput::new(loss, output, targets)
    }
}</code></pre>
<p>As evident from the preceding code block, we employ the cross-entropy loss module for loss
calculation, without the inclusion of any padding token. We then return the classification output
containing the loss, the output tensor with all logits and the targets.</p>
<p>Please take note that tensor operations receive owned tensors as input. For reusing a tensor
multiple times, you need to use the clone function. There's no need to worry; this process won't
involve actual copying of the tensor data. Instead, it will simply indicate that the tensor is
employed in multiple instances, implying that certain operations won't be performed in place. In
summary, our API has been designed with owned tensors to optimize performance.</p>
<p>Moving forward, we will proceed with the implementation of both the training and validation steps
for our model.</p>
<pre><code class="language-rust   ignore">impl&lt;B: ADBackend&gt; TrainStep&lt;MNISTBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt; for Model&lt;B&gt; {
    fn step(&amp;self, batch: MNISTBatch&lt;B&gt;) -&gt; TrainOutput&lt;ClassificationOutput&lt;B&gt;&gt; {
        let item = self.forward_classification(batch.images, batch.targets);

        TrainOutput::new(self, item.loss.backward(), item)
    }
}

impl&lt;B: Backend&gt; ValidStep&lt;MNISTBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt; for Model&lt;B&gt; {
    fn step(&amp;self, batch: MNISTBatch&lt;B&gt;) -&gt; ClassificationOutput&lt;B&gt; {
        self.forward_classification(batch.images, batch.targets)
    }
}</code></pre>
<p>Here we define the input and output types as generic arguments in the <code>TrainStep</code> and <code>ValidStep</code>.
We will call them <code>MNISTBatch</code> and <code>ClassificationOutput</code>. In the training step, the computation of
gradients is straightforward, necessitating a simple invocation of <code>backward()</code> on the loss. Note
that contrary to PyTorch, gradients are not store alongside each tensor parameter, but are rather
returned by the backward pass, as such: <code>let gradients = loss.backward();</code>. The gradient of a
parameter can be obtained with the grad function: <code>let grad = tensor.grad(&amp;gradients);</code>. Although it
is not necessary when using the learner struct and the optimizers, it can prove to be quite useful
when debugging or writing custom training loops. One of the differences between the training and the
validation steps is that the former requires the backend to implement <code>ADBackend</code> and not just
<code>Backend</code>. Otherwise, the <code>backward</code> function is not available, as the backend does not support
autodiff. We will see later how to create a backend with autodiff support.</p>
<p>Let us move on to establishing the practical training configuration.</p>
<pre><code class="language-rust   ignore">#[derive(Config)]
pub struct TrainingConfig {
    pub model: ModelConfig,
    pub optimizer: AdamConfig,
    #[config(default = 10)]
    pub num_epochs: usize,
    #[config(default = 64)]
    pub batch_size: usize,
    #[config(default = 4)]
    pub num_workers: usize,
    #[config(default = 42)]
    pub seed: u64,
    #[config(default = 1.0e-4)]
    pub learning_rate: f64,
}

pub fn train&lt;B: ADBackend&gt;(artifact_dir: &amp;str, config: TrainingConfig, device: B::Device) {
    std::fs::create_dir_all(artifact_dir).ok();
    config
        .save(format!(&quot;{artifact_dir}/config.json&quot;))
        .expect(&quot;Save without error&quot;);

    B::seed(config.seed);

    let batcher_train = MNISTBatcher::&lt;B&gt;::new(device.clone());
    let batcher_valid = MNISTBatcher::&lt;B::InnerBackend&gt;::new(device.clone());

    let dataloader_train = DataLoaderBuilder::new(batcher_train)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MNISTDataset::train());

    let dataloader_test = DataLoaderBuilder::new(batcher_valid)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MNISTDataset::test());

    let learner = LearnerBuilder::new(artifact_dir)
        .metric_train_plot(AccuracyMetric::new())
        .metric_valid_plot(AccuracyMetric::new())
        .metric_train_plot(LossMetric::new())
        .metric_valid_plot(LossMetric::new())
        .with_file_checkpointer(1, CompactRecorder::new())
        .devices(vec![device])
        .num_epochs(config.num_epochs)
        .build(
            config.model.init::&lt;B&gt;(),
            config.optimizer.init(),
            config.learning_rate,
        );

    let model_trained = learner.fit(dataloader_train, dataloader_test);

    model_trained
        .save_file(format!(&quot;{artifact_dir}/model&quot;), &amp;CompactRecorder::new())
        .expect(&quot;Failed to save trained model&quot;);
}</code></pre>
<p>It is a good practice to use the <code>Config</code> derive to create the experiment configuration. In the
<code>train</code> function, the first thing we are doing is making sure the <code>artifact_dir</code> exists, using the
standard rust library for file manipulation. All checkpoints, logging and metrics will be stored
under the this directory. We then initialize our dataloaders using our previously created batcher.
Since no automatic differentiation is needed during the validation phase, the backend used for the
corresponding batcher is <code>B::InnerBackend</code> (see <a href="basic-workflow/./backend.html">Backend</a>). The autodiff capabilities
are available through a type system, making it nearly impossible to forget to deactivate gradient
calculation.</p>
<p>Next, we create our learner with the accuracy and loss metric on both training and validation steps
along with the device and the epoch. We also configure the checkpointer using the <code>CompactRecorder</code>
to indicate how weights should be stored. This struct implements the <code>Recorder</code> trait, which makes
it capable of saving records for persistency.</p>
<p>We then build the learner with the model, the optimizer and the learning rate. Notably, the third
argument of the build function should actually be a learning rate <em>scheduler</em>. When provided with a
float as in our example, it is automatically transformed into a <em>constant</em> learning rate scheduler.
The learning rate is not part of the optimizer config as it is often done in other frameworks, but
rather passed as a parameter when executing the optimizer step. This avoids having to mutate the
state of the optimizer and is therefore more functional. It makes no difference when using the
learner struct, but it will be an essential nuance to grasp if you implement your own training loop.</p>
<p>Once the learner is created, we can simply call <code>fit</code> and provide the training and validation
dataloaders. For the sake of simplicity in this example, we employ the test set as the validation
set; however, we do not recommend this practice for actual usage.</p>
<p>Finally, the trained model is returned by the <code>fit</code> method, and the only remaining task is saving
the trained weights using the <code>CompactRecorder</code>. This recorder employs the <code>MessagePack</code> format with
<code>gzip</code> compression, <code>f16</code> for floats and <code>i32</code> for integers. Other recorders are available, offering
support for various formats, such as <code>BinCode</code> and <code>JSON</code>, with or without compression. Any backend,
regardless of precision, can load recorded data of any kind.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend"><a class="header" href="#backend">Backend</a></h1>
<p>We have effectively written most of the necessary code for training our model. However, we have not explicitly designated the backend to be utilized at any point.
Indeed, only the <code>main</code> function remains. </p>
<pre><code class="language-rust   ignore">use burn::optim::AdamConfig;
use guide::model::ModelConfig;

fn main() {
    type MyBackend = burn_wgpu::WgpuBackend&lt;burn_wgpu::AutoGraphicsApi, f32, i32&gt;;
    type MyAutodiffBackend = burn_autodiff::ADBackendDecorator&lt;MyBackend&gt;;

    let device = burn_wgpu::WgpuDevice::default();
    guide::training::train::&lt;MyAutodiffBackend&gt;(
        &quot;/tmp/guide&quot;,
        guide::training::TrainingConfig::new(ModelConfig::new(10, 512), AdamConfig::new()),
        device,
    );
}</code></pre>
<p>In this example, we use the <code>WgpuBackend</code> which is compatible with any operating system and will use the GPU. For other options, see the Burn README. 
This backend type takes the graphics api, the float type and the int type as generic argument that will be used during the training. By leaving the graphics API as <code>burn_wgpu::AutoGraphicsApi</code>, it should automatically use an API available on your machine. 
The autodiff backend is simply the same backend, wrapped within the <code>ADBackendDecorator</code> struct which imparts differentiability to any backend.</p>
<p>We call the <code>train</code> function defined earlier with a directory for artifacts, the configuration of the model (the number of digit classes is 10 and the hidden dimension is 512), the optimizer configuration which in our case will be the default Adam configuration, and the device which can be obtained from the backend.</p>
<p>When running the example, we can see the training progression through a basic CLI dashboard:</p>
<img title="a title" alt="Alt text" src="basic-workflow/./training-output.png">
<div style="break-before: page; page-break-before: always;"></div><h1 id="inference"><a class="header" href="#inference">Inference</a></h1>
<p>Now that we have trained our model, the next natural step is to use it for inference.</p>
<p>For loading a model primed for inference, it is of course more efficient to directly load the
weights into the model, bypassing the need to initially set arbitrary weights or worse, weights
computed from a Xavier normal initialization only to then promptly replace them with the stored
weights. With that in mind, let's create a new initialization function receiving the record as
input.</p>
<pre><code class="language-rust   ignore">impl ModelConfig {
    /// Returns the initialized model using the recorded weights.
    pub fn init_with&lt;B: Backend&gt;(&amp;self, record: ModelRecord&lt;B&gt;) -&gt; Model&lt;B&gt; {
        Model {
            conv1: Conv2dConfig::new([1, 8], [3, 3]).init_with(record.conv1),
            conv2: Conv2dConfig::new([8, 16], [3, 3]).init_with(record.conv2),
            pool: AdaptiveAvgPool2dConfig::new([8, 8]).init(),
            activation: ReLU::new(),
            linear1: LinearConfig::new(16 * 8 * 8, self.hidden_size).init_with(record.linear1),
            linear2: LinearConfig::new(self.hidden_size, self.num_classes)
                .init_with(record.linear2),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }
}</code></pre>
<p>It is important to note that the <code>ModelRecord</code> was automatically generated thanks to the <code>Module</code>
trait. It allows us to load the module state without having to deal with fetching the correct type
manually. Everything is validated when loading the model with the record.</p>
<p>Now let's create a simple <code>infer</code> method in which we will load our trained model.</p>
<pre><code class="language-rust   ignore">pub fn infer&lt;B: Backend&gt;(artifact_dir: &amp;str, device: B::Device, item: MNISTItem) {
    let config =
        TrainingConfig::load(format!(&quot;{artifact_dir}/config.json&quot;)).expect(&quot;A config exists&quot;);
    let record = CompactRecorder::new()
        .load(format!(&quot;{artifact_dir}/model&quot;).into())
        .expect(&quot;Failed to load trained model&quot;);

    let model = config.model.init_with::&lt;B&gt;(record).to_device(&amp;device);

    let label = item.label;
    let batcher = MNISTBatcher::new(device);
    let batch = batcher.batch(vec![item]);
    let output = model.forward(batch.images);
    let predicted = output.argmax(1).flatten::&lt;1&gt;(0, 1).into_scalar();

    println!(&quot;Predicted {} Expected {}&quot;, predicted, label);
}</code></pre>
<p>The first step is to load the configuration of the training to fetch the correct model
configuration. Then we can fetch the record using the same recorder as we used during training.
Finally we can init the model with the configuration and the record before sending it to the wanted
device for inference. For simplicity we can use the same batcher used during the training to pass
from a MNISTItem to a tensor.</p>
<p>By running the infer function, you should see the predictions of your model!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h1>
<p>In this short guide, we've introduced you to the fundamental building blocks for getting started
with Burn. While there's still plenty to explore, our goal has been to provide you with the
essential knowledge to kickstart your productivity within the framework.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building-blocks"><a class="header" href="#building-blocks">Building Blocks</a></h1>
<p>In this section, we'll guide you through the core elements that make up Burn. We'll walk you through
the key components that serve as the building blocks of the framework and your future projects.</p>
<p>As you explore Burn, you might notice that we occasionally draw comparisons to PyTorch. We believe
it can provide a smoother learning curve and help you grasp the nuances more effectively.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend-1"><a class="header" href="#backend-1">Backend</a></h1>
<p>Nearly everything in Burn is based on the <code>Backend</code> trait, which enables you to run tensor
operations using different implementations without having to modify your code. While a backend may
not necessarily have autodiff capabilities, the <code>ADBackend</code> trait specifies when autodiff is needed.
This trait not only abstracts operations but also tensor, device, and element types, providing each
backend the flexibility they need. It's worth noting that the trait assumes eager mode since burn
fully supports dynamic graphs. However, we may create another API to assist with integrating
graph-based backends, without requiring any changes to the user's code.</p>
<p>Users are not expected to directly use the backend trait methods, as it is primarily designed with
backend developers in mind rather than Burn users. Therefore, most Burn userland APIs are generic
across backends. This approach helps users discover the API more organically with proper
autocomplete and documentation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensor"><a class="header" href="#tensor">Tensor</a></h1>
<p>As previously explained in the <a href="building-blocks/../basic-workflow/model.html">model section</a>, the Tensor struct has 3
generic arguments: the backend, the dimension number (rank), and the kind.</p>
<pre><code class="language-rust   ignore">Tensor&lt;B, D&gt;           // Float tensor (default)
Tensor&lt;B, D, Float&gt;    // Explicit float tensor
Tensor&lt;B, D, Int&gt;      // Int tensor
Tensor&lt;B, D, Bool&gt;     // Bool tensor</code></pre>
<p>Note that the specific element types used for <code>Float</code>, <code>Int</code>, and <code>Bool</code> tensors are defined by
backend implementations.</p>
<h2 id="operations"><a class="header" href="#operations">Operations</a></h2>
<p>Almost all Burn operations take ownership of the input tensors. Therefore, reusing a tensor multiple
times will necessitate cloning it. Don't worry, the tensor's buffer isn't copied, but a reference to
it is increased. This makes it possible to determine exactly how many times a tensor is used, which
is very convenient for reusing tensor buffers and improving performance. For that reason, we don't
provide explicit inplace operations. If a tensor is used only one time, inplace operations will
always be used when available.</p>
<p>Normally with PyTorch, explicit inplace operations aren't supported during the backward pass, making
them useful only for data preprocessing or inference-only model implementations. With Burn, you can
focus more on <em>what</em> the model should do, rather than on <em>how</em> to do it. We take the responsibility
of making your code run as fast as possible during training as well as inference. The same
principles apply to broadcasting; all operations support broadcasting unless specified otherwise.</p>
<p>Here, we provide a list of all supported operations along with their PyTorch equivalents. Note that
for the sake of simplicity, we ignore type signatures. For more details, refer to the
<a href="https://docs.rs/burn/latest/burn/tensor/struct.Tensor.html">full documentation</a>.</p>
<h3 id="basic-operations"><a class="header" href="#basic-operations">Basic Operations</a></h3>
<p>Those operations are available for all tensor kinds: <code>Int</code>, <code>Float</code>, and <code>Bool</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Tensor::empty(shape)</code></td><td><code>torch.empty(shape)</code></td></tr>
<tr><td><code>Tensor::empty_device(shape, device)</code></td><td><code>torch.empty(shape, device=device)</code></td></tr>
<tr><td><code>tensor.dims()</code></td><td><code>tensor.size()</code></td></tr>
<tr><td><code>tensor.shape()</code></td><td><code>tensor.shape</code></td></tr>
<tr><td><code>tensor.reshape(shape)</code></td><td><code>tensor.view(shape)</code></td></tr>
<tr><td><code>tensor.flatten(start_dim, end_dim)</code></td><td><code>tensor.flatten(start_dim, end_dim)</code></td></tr>
<tr><td><code>tensor.squeeze(dim)</code></td><td><code>tensor.squeeze(dim)</code></td></tr>
<tr><td><code>tensor.unsqueeze()</code></td><td><code>tensor.unsqueeze(0)</code></td></tr>
<tr><td><code>tensor.slice(ranges)</code></td><td><code>tensor[(*ranges,)]</code></td></tr>
<tr><td><code>tensor.slice_assign(ranges, values)</code></td><td><code>tensor[(*ranges,)] = values</code></td></tr>
<tr><td><code>tensor.device()</code></td><td><code>tensor.device</code></td></tr>
<tr><td><code>tensor.to_device(device)</code></td><td><code>tensor.to(device)</code></td></tr>
<tr><td><code>tensor.repeat(2, 4)</code></td><td><code>tensor.repeat([1, 1, 4])</code></td></tr>
<tr><td><code>tensor.equal(other)</code></td><td><code>x == y</code></td></tr>
<tr><td><code>Tensor::cat(tensors, dim)</code></td><td><code>torch.cat(tensors, dim)</code></td></tr>
<tr><td><code>tensor.into_data()</code></td><td>N/A</td></tr>
<tr><td><code>tensor.to_data()</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::from_data(data)</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::from_data_device(data, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.into_primitive()</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::from_primitive(primitive)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<h3 id="numeric-operations"><a class="header" href="#numeric-operations">Numeric Operations</a></h3>
<p>Those operations are available for numeric tensor kinds: <code>Float</code> and <code>Int</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.into_scalar()</code></td><td><code>tensor.item()</code> (for single-element tensors)</td></tr>
<tr><td><code>tensor + other</code> or <code>tensor.add(other)</code></td><td><code>tensor + other</code></td></tr>
<tr><td><code>tensor + scalar</code> or <code>tensor.add_scalar(scalar)</code></td><td><code>tensor + scalar</code></td></tr>
<tr><td><code>tensor - other</code> or <code>tensor.sub(other)</code></td><td><code>tensor - other</code></td></tr>
<tr><td><code>tensor - scalar</code> or <code>tensor.sub_scalar(scalar)</code></td><td><code>tensor - scalar</code></td></tr>
<tr><td><code>tensor / other</code> or <code>tensor.div(other)</code></td><td><code>tensor / other</code></td></tr>
<tr><td><code>tensor / scalar</code> or <code>tensor.div_scalar(scalar)</code></td><td><code>tensor / scalar</code></td></tr>
<tr><td><code>tensor * other</code> or <code>tensor.mul(other)</code></td><td><code>tensor * other</code></td></tr>
<tr><td><code>tensor * scalar</code> or <code>tensor.mul_scalar(scalar)</code></td><td><code>tensor * scalar</code></td></tr>
<tr><td><code>-tensor</code> or <code>tensor.neg()</code></td><td><code>-tensor</code></td></tr>
<tr><td><code>Tensor::zeros(shape)</code></td><td><code>torch.zeros(shape)</code></td></tr>
<tr><td><code>Tensor::zeros_device(shape, device)</code></td><td><code>torch.zeros(shape, device=device)</code></td></tr>
<tr><td><code>Tensor::ones(shape)</code></td><td><code>torch.ones(shape)</code></td></tr>
<tr><td><code>Tensor::ones_device(shape, device)</code></td><td><code>torch.ones(shape, device=device)</code></td></tr>
<tr><td><code>Tensor::full(shape, fill_value)</code></td><td><code>torch.full(shape, fill_value)</code></td></tr>
<tr><td><code>Tensor::full_device(shape, fill_value, device)</code></td><td><code>torch.full(shape, fill_value, device=device)</code></td></tr>
<tr><td><code>tensor.mean()</code></td><td><code>tensor.mean()</code></td></tr>
<tr><td><code>tensor.sum()</code></td><td><code>tensor.sum()</code></td></tr>
<tr><td><code>tensor.mean_dim(dim)</code></td><td><code>tensor.mean(dim)</code></td></tr>
<tr><td><code>tensor.sum_dim(dim)</code></td><td><code>tensor.sum(dim)</code></td></tr>
<tr><td><code>tensor.equal_elem(other)</code></td><td><code>tensor.eq(other)</code></td></tr>
<tr><td><code>tensor.greater(other)</code></td><td><code>tensor.gt(other)</code></td></tr>
<tr><td><code>tensor.greater_elem(scalar)</code></td><td><code>tensor.gt(scalar)</code></td></tr>
<tr><td><code>tensor.greater_equal(other)</code></td><td><code>tensor.ge(other)</code></td></tr>
<tr><td><code>tensor.greater_equal_elem(scalar)</code></td><td><code>tensor.ge(scalar)</code></td></tr>
<tr><td><code>tensor.lower(other)</code></td><td><code>tensor.lt(other)</code></td></tr>
<tr><td><code>tensor.lower_elem(scalar)</code></td><td><code>tensor.lt(scalar)</code></td></tr>
<tr><td><code>tensor.lower_equal(other)</code></td><td><code>tensor.le(other)</code></td></tr>
<tr><td><code>tensor.lower_equal_elem(scalar)</code></td><td><code>tensor.le(scalar)</code></td></tr>
<tr><td><code>tensor.mask_where(mask, value_tensor)</code></td><td><code>torch.where(mask, value_tensor, tensor)</code></td></tr>
<tr><td><code>tensor.mask_fill(mask, value)</code></td><td><code>tensor.masked_fill(mask, value)</code></td></tr>
<tr><td><code>tensor.gather(dim, indices)</code></td><td><code>torch.gather(tensor, dim, indices)</code></td></tr>
<tr><td><code>tensor.scatter(dim, indices, values)</code></td><td><code>tensor.scatter_add(dim, indices, values)</code></td></tr>
<tr><td><code>tensor.select(dim, indices)</code></td><td><code>tensor.index_select(dim, indices)</code></td></tr>
<tr><td><code>tensor.select_assign(dim, indices, values)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.argmax(dim)</code></td><td><code>tensor.argmax(dim)</code></td></tr>
<tr><td><code>tensor.max()</code></td><td><code>tensor.max()</code></td></tr>
<tr><td><code>tensor.max_dim(dim)</code></td><td><code>tensor.max(dim)</code></td></tr>
<tr><td><code>tensor.max_dim_with_indices(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.argmin(dim)</code></td><td><code>tensor.argmin(dim)</code></td></tr>
<tr><td><code>tensor.min()</code></td><td><code>tensor.min()</code></td></tr>
<tr><td><code>tensor.min_dim(dim)</code></td><td><code>tensor.min(dim)</code></td></tr>
<tr><td><code>tensor.min_dim_with_indices(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.clamp(min, max)</code></td><td><code>torch.clamp(tensor, min=min, max=max)</code></td></tr>
<tr><td><code>tensor.clamp_min(min)</code></td><td><code>torch.clamp(tensor, min=min)</code></td></tr>
<tr><td><code>tensor.clamp_max(max)</code></td><td><code>torch.clamp(tensor, max=max)</code></td></tr>
<tr><td><code>tensor.abs()</code></td><td><code>torch.abs(tensor)</code></td></tr>
</tbody></table>
</div>
<h3 id="float-operations"><a class="header" href="#float-operations">Float Operations</a></h3>
<p>Those operations are only available for <code>Float</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.exp()</code></td><td><code>tensor.exp()</code></td></tr>
<tr><td><code>tensor.log()</code></td><td><code>tensor.log()</code></td></tr>
<tr><td><code>tensor.log1p()</code></td><td><code>tensor.log1p()</code></td></tr>
<tr><td><code>tensor.erf()</code></td><td><code>tensor.erf()</code></td></tr>
<tr><td><code>tensor.powf(value)</code></td><td><code>tensor.pow(value)</code></td></tr>
<tr><td><code>tensor.sqrt()</code></td><td><code>tensor.sqrt()</code></td></tr>
<tr><td><code>tensor.cos()</code></td><td><code>tensor.cos()</code></td></tr>
<tr><td><code>tensor.sin()</code></td><td><code>tensor.sin()</code></td></tr>
<tr><td><code>tensor.tanh()</code></td><td><code>tensor.tanh()</code></td></tr>
<tr><td><code>tensor.from_floats(floats)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.int()</code></td><td>Similar to <code>tensor.to(torch.long)</code></td></tr>
<tr><td><code>tensor.zeros_like()</code></td><td><code>torch.zeros_like(tensor)</code></td></tr>
<tr><td><code>tensor.ones_like()</code></td><td><code>torch.ones_like(tensor)</code></td></tr>
<tr><td><code>tensor.random_like(distribution)</code></td><td><code>torch.rand_like()</code> only uniform</td></tr>
<tr><td><code>tensor.one_hot(index, num_classes)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.transpose()</code></td><td><code>tensor.T</code></td></tr>
<tr><td><code>tensor.swap_dims(dim1, dim2)</code></td><td><code>tensor.transpose(dim1, dim2)</code></td></tr>
<tr><td><code>tensor.matmul(other)</code></td><td><code>tensor.matmul(other)</code></td></tr>
<tr><td><code>tensor.var(dim)</code></td><td><code>tensor.var(dim)</code></td></tr>
<tr><td><code>tensor.var_bias(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.var_mean(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.var_mean_bias(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.random(shape, distribution)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.random_device(shape, distribution, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.to_full_precision()</code></td><td><code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.from_full_precision(tensor)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<h1 id="int-operations"><a class="header" href="#int-operations">Int Operations</a></h1>
<p>Those operations are only available for <code>Int</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.from_ints(ints)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.float()</code></td><td>Similar to <code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.arange(5..10)</code></td><td><code>tensor.arange(start=5, end=10)</code></td></tr>
<tr><td><code>tensor.arange_device(5..10, device)</code></td><td><code>tensor.arange(start=5, end=10, device=device)</code></td></tr>
<tr><td><code>tensor.arange_step(5..10, 2)</code></td><td><code>tensor.arange(start=5, end=10, step=2)</code></td></tr>
<tr><td><code>tensor.arange_step_device(5..10, 2, device)</code></td><td><code>tensor.arange(start=5, end=10, step=2, device=device)</code></td></tr>
</tbody></table>
</div>
<h1 id="bool-operations"><a class="header" href="#bool-operations">Bool Operations</a></h1>
<p>Those operations are only available for <code>Bool</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.float()</code></td><td>Similar to <code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.int()</code></td><td>Similar to <code>tensor.to(torch.long)</code></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="autodiff"><a class="header" href="#autodiff">Autodiff</a></h1>
<p>Burn's tensor also supports autodifferentiation, which is an essential part of any deep learning
framework. We introduced the <code>Backend</code> trait in the <a href="building-blocks/./backend.html">previous section</a>, but Burn also
has another trait for autodiff: <code>ADBackend</code>.</p>
<p>However, not all tensors support auto-differentiation; you need a backend that implements both the
<code>Backend</code> and <code>ADBackend</code> traits. Fortunately, you can add autodifferentiation capabilities to any
backend using a backend decorator: <code>type MyAutodiffBackend = ADBackendDecorator&lt;MyBackend&gt;</code>. This
decorator implements both the <code>ADBackend</code> and <code>Backend</code> traits by maintaining a dynamic
computational graph and utilizing the inner backend to execute tensor operations.</p>
<p>The <code>ADBackend</code> trait adds new operations on float tensors that can't be called otherwise. It also
provides a new associated type, <code>B::Gradients</code>, where each calculated gradient resides.</p>
<pre><code class="language-rust  ignore">fn calculate_gradients&lt;B: ADBackend&gt;(tensor: Tensor&lt;B, 2&gt;) -&gt; B::Gradients {
    let mut gradients = tensor.clone().backward();

    let tensor_grad = tensor.grad(&amp;gradients);        // get
    let tensor_grad = tensor.grad_remove(&amp;mut gradients); // pop

    gradients
}</code></pre>
<p>Note that some functions will always be available even if the backend doesn't implement the
<code>ADBackend</code> trait. In such cases, those functions will do nothing.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.detach()</code></td><td><code>tensor.detach()</code></td></tr>
<tr><td><code>tensor.require_grad()</code></td><td><code>tensor.requires_grad()</code></td></tr>
<tr><td><code>tensor.is_require_grad()</code></td><td><code>tensor.requires_grad</code></td></tr>
<tr><td><code>tensor.set_require_grad(require_grad)</code></td><td><code>tensor.requires_grad(False)</code></td></tr>
</tbody></table>
</div>
<p>However, you're unlikely to make any mistakes since you can't call <code>backward</code> on a tensor that is on
a backend that doesn't implement <code>ADBackend</code>. Additionally, you can't retrieve the gradient of a
tensor without an autodiff backend.</p>
<h2 id="difference-with-pytorch"><a class="header" href="#difference-with-pytorch">Difference with PyTorch</a></h2>
<p>The way Burn handles gradients is different from PyTorch. First, when calling <code>backward</code>, each
parameter doesn't have its <code>grad</code> field updated. Instead, the backward pass returns all the
calculated gradients in a container. This approach offers numerous benefits, such as the ability to
easily send gradients to other threads.</p>
<p>You can also retrieve the gradient for a specific parameter using the <code>grad</code> method on a tensor.
Since this method takes the gradients as input, it's hard to forget to call <code>backward</code> beforehand.
Note that sometimes, using <code>grad_remove</code> can improve performance by allowing inplace operations.</p>
<p>In PyTorch, when you don't need gradients for inference or validation, you typically need to scope
your code using a block.</p>
<pre><code class="language-python"># Inference mode
torch.inference():
   # your code
   ...

# Or no grad
torch.no_grad():
   # your code
   ...
</code></pre>
<p>With Burn, you don't need to wrap the backend with the <code>ADBackendDecorator</code> for inference, and you
can call <code>inner()</code> to obtain the inner tensor, which is useful for validation.ß</p>
<pre><code class="language-rust  ignore">/// Use `B: ADBackend`
fn example_validation&lt;B: ADBackend&gt;(tensor: Tensor&lt;B, 2&gt;) {
    let inner_tensor: Tensor&lt;B::InnerBackend, 2&gt; = tensor.inner();
    let _ = inner_tensor + 5;
}

/// Use `B: Backend`
fn example_inference&lt;B: Backend&gt;(tensor: Tensor&lt;B, 2&gt;) {
    let _ = tensor + 5;
    ...
}</code></pre>
<p><strong>Gradients with Optimizers</strong></p>
<p>We've seen how gradients can be used with tensors, but the process is a bit different when working
with optimizers from <code>burn-core</code>. To work with the <code>Module</code> trait, a translation step is required to
link tensor parameters with their gradients. This step is necessary to easily support gradient
accumulation and training on multiple devices, where each module can be forked and run on different
devices in parallel. We'll explore deeper into this topic in the <a href="building-blocks/./module.html">Module</a> section.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="module"><a class="header" href="#module">Module</a></h1>
<p>The <code>Module</code> derive allows you to create your own neural network modules, similar to PyTorch. The
derive function only generates the necessary methods to essentially act as a parameter container for
your type, it makes no assumptions about how the forward pass is declared.</p>
<pre><code class="language-rust  ignore">use burn::nn;
use burn::module::Module;
use burn::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&lt;B: Backend&gt; {
    linear_inner: Linear&lt;B&gt;,
    linear_outer: Linear&lt;B&gt;,
    dropout: Dropout,
    gelu: GELU,
}

impl&lt;B: Backend&gt; PositionWiseFeedForward&lt;B&gt; {
    /// Normal method added to a struct.
    pub fn forward&lt;const D: usize&gt;(&amp;self, input: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt; {
        let x = self.linear_inner.forward(input);
        let x = self.gelu.forward(x);
        let x = self.dropout.forward(x);

        self.linear_outer.forward(x)
    }
}</code></pre>
<p>Note that all fields declared in the struct must also implement the <code>Module</code> trait.</p>
<h2 id="tensor-1"><a class="header" href="#tensor-1">Tensor</a></h2>
<p>If you want to create your own module that contains tensors, and not just other modules defined with
the <code>Module</code> derive, you need to be careful to achieve the behavior you want.</p>
<ul>
<li>
<p><code>Param&lt;Tensor&lt;B, D&gt;&gt;</code>: If you want the tensor to be included as a parameter of your modules, you
need to wrap the tensor in a <code>Param</code> struct. This will create an ID that will be used to identify
this parameter. This is essential when performing module optimization and when saving states such
as optimizer and module checkpoints. Note that a module's record only contains parameters.</p>
</li>
<li>
<p><code>Param&lt;Tensor&lt;B, D&gt;&gt;.set_require_grad(false)</code>: If you want the tensor to be included as a
parameter of your modules, and therefore saved with the module's weights, but you don't want it to
be updated by the optimizer.</p>
</li>
<li>
<p><code>Tensor&lt;B, D&gt;</code>: If you want the tensor to act as a constant that can be recreated when
instantiating a module. This can be useful when generating sinusoidal embeddings, for example.</p>
</li>
</ul>
<h2 id="methods"><a class="header" href="#methods">Methods</a></h2>
<p>These methods are available for all modules.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>module.devices()</code></td><td>N/A</td></tr>
<tr><td><code>module.fork(device)</code></td><td>Similar to <code>module.to(device).detach()</code></td></tr>
<tr><td><code>module.to_device(device)</code></td><td><code>module.to(device)</code></td></tr>
<tr><td><code>module.no_grad()</code></td><td><code>module.require_grad_(False)</code></td></tr>
<tr><td><code>module.num_params()</code></td><td>N/A</td></tr>
<tr><td><code>module.visit(visitor)</code></td><td>N/A</td></tr>
<tr><td><code>module.map(mapper)</code></td><td>N/A</td></tr>
<tr><td><code>module.into_record()</code></td><td>Similar to <code>state_dict</code></td></tr>
<tr><td><code>module.load_record(record)</code></td><td>Similar to <code>load_state_dict(state_dict)</code></td></tr>
<tr><td><code>module.save_file(file_path, recorder)</code></td><td>N/A</td></tr>
<tr><td><code>module.load_file(file_path, recorder)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<p>Similar to the backend trait, there is also the <code>ADModule</code> trait to signify a module with autodiff
support.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>module.valid()</code></td><td><code>module.eval()</code></td></tr>
</tbody></table>
</div>
<h2 id="visitor--mapper"><a class="header" href="#visitor--mapper">Visitor &amp; Mapper</a></h2>
<p>As mentioned earlier, modules primarily function as parameter containers. Therefore, we naturally
offer several ways to perform functions on each parameter. This is distinct from PyTorch, where
extending module functionalities is not as straightforward.</p>
<p>The <code>map</code> and <code>visitor</code> methods are quite similar but serve different purposes. Mapping is used for
potentially mutable operations where each parameter of a module can be updated to a new value. In
Burn, optimizers are essentially just sophisticated module mappers. Visitors, on the other hand, are
used when you don't intend to modify the module but need to retrieve specific information from it,
such as the number of parameters or a list of devices in use.</p>
<p>You can implement your own mapper or visitor by implementing these simple traits:</p>
<pre><code class="language-rust  ignore">/// Module visitor trait.
pub trait ModuleVisitor&lt;B: Backend&gt; {
    /// Visit a tensor in the module.
    fn visit&lt;const D: usize&gt;(&amp;mut self, id: &amp;ParamId, tensor: &amp;Tensor&lt;B, D&gt;);
}

/// Module mapper trait.
pub trait ModuleMapper&lt;B: Backend&gt; {
    /// Map a tensor in the module.
    fn map&lt;const D: usize&gt;(&amp;mut self, id: &amp;ParamId, tensor: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt;;
}</code></pre>
<h2 id="built-in-modules"><a class="header" href="#built-in-modules">Built-in Modules</a></h2>
<p>Burn comes with built-in modules that you can use to build your own modules.</p>
<h3 id="convolutions"><a class="header" href="#convolutions">Convolutions</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Conv1d</code></td><td><code>nn.Conv1d</code></td></tr>
<tr><td><code>Conv2d</code></td><td><code>nn.Conv2d</code></td></tr>
<tr><td><code>ConvTranspose1d</code></td><td><code>nn.ConvTranspose1d</code></td></tr>
<tr><td><code>ConvTranspose2d</code></td><td><code>nn.ConvTranspose2d</code></td></tr>
</tbody></table>
</div>
<h3 id="general"><a class="header" href="#general">General</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>BatchNorm</code></td><td><code>nn.BatchNorm1d</code>, <code>nn.BatchNorm2d</code> etc.</td></tr>
<tr><td><code>Dropout</code></td><td><code>nn.Dropout</code> or <code>nn.Dropout2d</code> etc.</td></tr>
<tr><td><code>GELU</code></td><td><code>nn.GELU</code></td></tr>
<tr><td><code>LayerNorm</code></td><td><code>nn.LayerNorm</code></td></tr>
<tr><td><code>Linear</code></td><td><code>nn.Linear</code></td></tr>
<tr><td><code>Embedding</code></td><td><code>nn.Embedding</code></td></tr>
<tr><td><code>Relu</code></td><td><code>nn.ReLU</code></td></tr>
</tbody></table>
</div>
<h3 id="loss"><a class="header" href="#loss">Loss</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>CrossEntropyLoss</code></td><td><code>nn.CrossEntropyLoss</code></td></tr>
<tr><td><code>MSELoss</code></td><td><code>nn.MSELoss</code></td></tr>
</tbody></table>
</div>
<h3 id="pooling"><a class="header" href="#pooling">Pooling</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>AdaptiveAvgPool1d</code></td><td><code>nn.AdaptiveAvgPool1d</code></td></tr>
<tr><td><code>AdaptiveAvgPool2d</code></td><td><code>nn.AdaptiveAvgPool2d</code></td></tr>
<tr><td><code>AvgPool1d</code></td><td><code>nn.AvgPool1d</code></td></tr>
<tr><td><code>AvgPool2d</code></td><td><code>nn.AvgPool2d</code></td></tr>
<tr><td><code>MaxPool1d</code></td><td><code>nn.MaxPool1d</code></td></tr>
<tr><td><code>MaxPool2d</code></td><td><code>nn.MaxPool2d</code></td></tr>
</tbody></table>
</div>
<h3 id="rnns"><a class="header" href="#rnns">RNNs</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Gru</code></td><td><code>nn.GRU</code></td></tr>
<tr><td><code>Lstm</code></td><td><code>nn.LSTM</code></td></tr>
<tr><td><code>GateController</code></td><td><em>No direct equivalent</em></td></tr>
</tbody></table>
</div>
<h3 id="transformer"><a class="header" href="#transformer">Transformer</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>MultiHeadAttention</code></td><td><code>nn.MultiheadAttention</code></td></tr>
<tr><td><code>TransformerDecoder</code></td><td><code>nn.TransformerDecoder</code></td></tr>
<tr><td><code>TransformerEncoder</code></td><td><code>nn.TransformerEncoder</code></td></tr>
<tr><td><code>PositionalEncoding</code></td><td><em>No direct equivalent</em></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="import-onnx-model"><a class="header" href="#import-onnx-model">Import ONNX Model</a></h1>
<h2 id="why-importing-models-is-necessary"><a class="header" href="#why-importing-models-is-necessary">Why Importing Models is Necessary</a></h2>
<p>In the realm of deep learning, it's common to switch between different frameworks depending on your
project's specific needs. Maybe you've painstakingly fine-tuned a model in TensorFlow or PyTorch and
now you want to reap the benefits of Burn's unique features for deployment or further testing. This
is precisely the scenario where importing models into Burn can be a game-changer.</p>
<h2 id="traditional-methods-the-drawbacks"><a class="header" href="#traditional-methods-the-drawbacks">Traditional Methods: The Drawbacks</a></h2>
<p>If you've been working with other deep learning frameworks like PyTorch, it's likely that you've
exported model weights before. PyTorch, for instance, lets you save model weights using its
<code>torch.save()</code> function. Yet, to port this model to another framework, you face the arduous task of
manually recreating the architecture in the destination framework before loading in the weights. Not
only is this method tedious, but it's also error-prone and hinders smooth interoperability between
frameworks.</p>
<p>It's worth noting that for models using cutting-edge, framework-specific features, manual porting
might be the only option, as standards like ONNX might not yet support these new innovations.</p>
<h2 id="enter-onnx"><a class="header" href="#enter-onnx">Enter ONNX</a></h2>
<p><a href="https://onnx.ai/onnx/intro/index.html">ONNX (Open Neural Network Exchange)</a> is designed to solve
such complications. It's an open-standard format that exports both the architecture and the weights
of a deep learning model. This feature makes it exponentially easier to move models between
different frameworks, thereby significantly aiding interoperability. ONNX is supported by a number
of frameworks including but not limited to TensorFlow, PyTorch, Caffe2, and Microsoft Cognitive
Toolkit.</p>
<h3 id="advantages-of-onnx"><a class="header" href="#advantages-of-onnx">Advantages of ONNX</a></h3>
<p>ONNX stands out for encapsulating two key elements:</p>
<ol>
<li><strong>Model Information</strong>: It captures the architecture, detailing the layers, their connections, and
configurations.</li>
<li><strong>Weights</strong>: ONNX also contains the trained model's weights.</li>
</ol>
<p>This dual encapsulation not only simplifies the porting of models between frameworks but also allows
seamless deployment across different environments without compatibility concerns.</p>
<h2 id="burns-onnx-support-importing-made-easy"><a class="header" href="#burns-onnx-support-importing-made-easy">Burn's ONNX Support: Importing Made Easy</a></h2>
<p>Understanding the important role that ONNX plays in the contemporary deep learning landscape, Burn
simplifies the process of importing ONNX models via an intuitive API designed to mesh well with
Burn's ecosystem.</p>
<p>Burn's solution is to translate ONNX files into Rust source code as well as Burn-compatible weights.
This transformation is carried out through the burn-import crate's code generator during build time,
providing advantages for both executing and further training ONNX models.</p>
<h3 id="advantages-of-burns-onnx-approach"><a class="header" href="#advantages-of-burns-onnx-approach">Advantages of Burn's ONNX Approach</a></h3>
<ol>
<li>
<p><strong>Native Integration</strong>: The generated Rust code is fully integrated into Burn's architecture,
enabling your model to run on various backends without the need for a separate ONNX runtime.</p>
</li>
<li>
<p><strong>Trainability</strong>: The imported model is not just for inference; it can be further trained or
fine-tuned using Burn's native training loop.</p>
</li>
<li>
<p><strong>Portability</strong>: As the model is converted to Rust source code, it can be compiled into
WebAssembly for browser execution. Likewise, this approach is beneficial for no-std embedded
devices.</p>
</li>
<li>
<p><strong>Optimization</strong>: Rust's compiler can further optimize the generated code for target
architectures, thereby improving performance.</p>
</li>
</ol>
<h3 id="sample-code-for-importing-onnx-model"><a class="header" href="#sample-code-for-importing-onnx-model">Sample Code for Importing ONNX Model</a></h3>
<p>Below is a step-by-step guide to importing an ONNX model into a Burn-based project:</p>
<h4 id="step-1-update-buildrs"><a class="header" href="#step-1-update-buildrs">Step 1: Update <code>build.rs</code></a></h4>
<p>Include the <code>burn-import</code> crate and use the following Rust code in your <code>build.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust">use burn_import::onnx::ModelGen;

fn main() {
    // Generate Rust code from the ONNX model file
    ModelGen::new()
        .input(&quot;src/model/mnist.onnx&quot;)
        .out_dir(&quot;model/&quot;)
        .run_from_script();
}</code></pre></pre>
<h4 id="step-2-modify-modrs"><a class="header" href="#step-2-modify-modrs">Step 2: Modify <code>mod.rs</code></a></h4>
<p>Add this code to the <code>mod.rs</code> file located in <code>src/model</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub mod mnist {
    include!(concat!(env!(&quot;OUT_DIR&quot;), &quot;/model/mnist.rs&quot;));
}
<span class="boring">}</span></code></pre></pre>
<h4 id="step-3-utilize-imported-model"><a class="header" href="#step-3-utilize-imported-model">Step 3: Utilize Imported Model</a></h4>
<p>Here's how to use the imported model in your application:</p>
<pre><pre class="playground"><code class="language-rust">mod model;

use burn::tensor;
use burn_ndarray::NdArrayBackend;
use model::mnist::Model;

fn main() {
    // Initialize a new model instance
    let model: Model&lt;NdArrayBackend&lt;f32&gt;&gt; = Model::new();

    // Create a sample input tensor (zeros for demonstration)
    let input = tensor::Tensor::&lt;NdArrayBackend&lt;f32&gt;, 4&gt;::zeros([1, 1, 28, 28]);

    // Perform inference
    let output = model.forward(input);

    // Print the output
    println!(&quot;{:?}&quot;, output);
}</code></pre></pre>
<h3 id="working-examples"><a class="header" href="#working-examples">Working Examples</a></h3>
<p>For practical examples, please refer to:</p>
<ol>
<li><a href="https://github.com/burn-rs/burn/tree/main/examples/onnx-inference">MNIST Inference Example</a></li>
<li><a href="https://github.com/burn-rs/models/tree/main/squeezenet-burn">SqueezeNet Image Classification</a></li>
</ol>
<p>By combining ONNX's robustness with Burn's unique features, you'll have the flexibility and power to
streamline your deep learning workflows like never before.</p>
<hr />
<blockquote>
<p>🚨<strong>Note</strong>: <code>burn-import</code> crate is in active development and currently supports a
<a href="https://github.com/burn-rs/burn/blob/main/burn-import/SUPPORTED-ONNX-OPS.md">limited set of ONNX operators</a>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced"><a class="header" href="#advanced">Advanced</a></h1>
<p>In this section, we will go into advanced topics that extend beyond basic usage. Given Burn's
exceptional flexibility, a lot of advanced use cases become possible.</p>
<p>Before going through this section, we strongly recommend exploring the
<a href="advanced/../basic-workflow/README.html">basic workflow</a> section and the
<a href="advanced/../building-blocks/README.html">building blocks</a> section. Establishing a solid understanding of how
the framework operates is crucial to comprehending the advanced concepts presented here. While you
have the freedom to explore the advanced sections in any order you prefer, it's important to note
that this section is not intended to be linear, contrary to preceding sections. Instead, it serves
as a repository of use cases that you can refer to for guidance as needed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend-extension"><a class="header" href="#backend-extension">Backend Extension</a></h1>
<p>Burn aims to be the most flexible deep learning framework. While it's crucial to maintain
compatibility with a wide variety of backends, Burn provides the ability to extend the functionality
of a backend implementation to suit your modeling requirements. This versatility is advantageous in
numerous ways, such as supporting custom operations like flash attention or manually fusing
operations for enhanced performance.</p>
<p>In this section, we will go into the process of extending a backend, providing multiple examples.
But before we proceed, let's establish the fundamental principles that will empower you to craft
your own backend extensions.</p>
<p>As you can observe, most types in Burn are generic over the Backend trait. This might give the
impression that Burn operates at a high level over the backend layer. However, making the trait
explicit instead of being chosen via a compilation flag was a thoughtful design decision. This
explicitness does not imply that all backends must be identical; rather, it offers a great deal of
flexibility when composing backends. The autodifferentiation backend trait (see
<a href="advanced/backend-extension/../building-blocks/autodiff.html">autodiff section</a>) is an example of how the backend trait has been
extended to enable gradient computation with backpropagation. Furthermore, this design allows you to
create your own backend extension. To achieve this, you need to design your own backend trait
specifying which functions should be supported.</p>
<pre><code class="language-rust  ignore">pub trait Backend: burn::tensor::backend::Backend {
    fn my_new_function(tensor: B::TensorPrimitive&lt;2&gt;) -&gt; B::TensorPrimitive&lt;2&gt; {
        // You can define a basic implementation reusing the Burn Backend API.
        // This can be useful since all backends will now automatically support
        // your model. But performance can be improved for this new
        // operation by implementing this block in specific backends.
    }
}</code></pre>
<p>You can then implement your new custom backend trait for any backend that you want to support:</p>
<pre><code class="language-rust  ignore">impl&lt;E: TchElement&gt; Backend for burn_tch::TchBackend&lt;E&gt; {
   fn my_new_function(tensor: TchTensor&lt;E, 2&gt;) -&gt; TchTensor&lt;E, 2&gt; {
      // My Tch implementation
   }
}

impl&lt;E: NdArrayElement&gt; Backend for burn_ndarray::NdArrayBackend&lt;E&gt; {
    // No specific implementation, but the backend can still be used.
}</code></pre>
<p>You can support the backward pass using the same pattern.</p>
<pre><code class="language-rust  ignore">impl&lt;B: Backend&gt; Backend for burn_autodiff::ADBackendDecorator&lt;B&gt; {
    // No specific implementation; autodiff will work with the default
    // implementation. Useful if you still want to train your model, but
    // observe performance gains mostly during inference.
}

impl&lt;B: Backend&gt; Backend for burn_autodiff::ADBackendDecorator&lt;B&gt; {
   fn my_new_function(tensor: ADTensor&lt;E, 2&gt;) -&gt; ADTensor&lt;E, 2&gt; {
      // My own backward implementation, generic over my custom Backend trait.
      //
      // You can add a new method `my_new_function_backward` to your custom backend
      // trait if you want to invoke a custom kernel during the backward pass.
   }
}

impl&lt;E: TchElement&gt; Backend for burn_autodiff::ADBackendDecorator&lt;burn_tch::TchBackend&lt;E&gt;&gt; {
   fn my_new_function(tensor: ADTensor&lt;E, 2&gt;) -&gt; ADTensor&lt;E, 2&gt; {
      // My own backward implementation, generic over a backend implementation.
      //
      // This is another way to call a custom kernel for the backward pass that
      // doesn't require the addition of a new `backward` function in the custom backend.
      // This is useful if you don't want all backends to support training, reducing
      // the need for extra code when you know your model will only be trained on one
      // specific backend.
   }
}</code></pre>
<p>The specificity of each implementation will be covered by the examples provided in this section.
Currently, we only have one example, but more are yet to come!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-wgpu-kernel"><a class="header" href="#custom-wgpu-kernel">Custom WGPU Kernel</a></h1>
<p>In this section, you will learn how to create your own custom operation by writing your own kernel
with the WGPU backend. We will take the example of a common workflow in the deep learning field,
where we create a kernel to fuse multiple operations together. We will fuse a matmul kernel followed
by an addition and the ReLU activation function, which is commonly found in various models. All the
code can be found under the
<a href="https://github.com/burn-rs/burn/tree/main/examples/custom-wgpu-kernel">examples directory</a>.</p>
<h2 id="custom-backend-trait"><a class="header" href="#custom-backend-trait">Custom Backend Trait</a></h2>
<p>First, we need to determine the type signature of our newly created operation by defining our custom
backend traits. As we will use the associated type <code>TensorPrimitive</code> of the <code>Backend</code> trait, which
encapsulates the underlying tensor implementation of the backend, we will use a type alias to avoid
the ugly disambiguation with associated types.</p>
<pre><code class="language-rust  ignore">/// We use a type alias for better readability.
pub type FloatTensor&lt;B, const D: usize&gt; = &lt;B as burn::tensor::backend::Backend&gt;::TensorPrimitive&lt;D&gt;;

/// We create our own Backend trait that extends the Burn backend trait.
pub trait Backend: burn::tensor::backend::Backend {
    fn fused_matmul_add_relu&lt;const D: usize&gt;(
        lhs: FloatTensor&lt;Self, D&gt;,
        rhs: FloatTensor&lt;Self, D&gt;,
        bias: FloatTensor&lt;Self, D&gt;,
    ) -&gt; FloatTensor&lt;Self, D&gt;;
}

/// We create our own ADBackend trait that extends the Burn autodiff backend trait.
pub trait ADBackend: Backend + burn::tensor::backend::ADBackend {}</code></pre>
<p>In our project, we can use these traits instead of the <code>burn::tensor::backend::{Backend, ADBackend}</code>
traits provided by Burn. Burn's user APIs typically make use of the <code>Tensor</code> struct rather than
dealing directly with primitive tensor types. Therefore, we can encapsulate our newly defined
backend traits with functions that expose new operations while maintaining a consistent API.</p>
<pre><code class="language-rust  ignore">/// We define our custom implementation using the added function on our custom backend.
pub fn matmul_add_relu_custom&lt;B: Backend&gt;(
    lhs: Tensor&lt;B, 3&gt;,
    rhs: Tensor&lt;B, 3&gt;,
    bias: Tensor&lt;B, 3&gt;,
) -&gt; Tensor&lt;B, 3&gt; {
    let output = B::fused_matmul_add_relu(
        lhs.into_primitive(),
        rhs.into_primitive(),
        bias.into_primitive(),
    );

    Tensor::from_primitive(output)
}

/// We define a reference implementation using basic tensor operations.
pub fn matmul_add_relu_reference&lt;B: Backend&gt;(
    lhs: Tensor&lt;B, 3&gt;,
    rhs: Tensor&lt;B, 3&gt;,
    bias: Tensor&lt;B, 3&gt;,
) -&gt; Tensor&lt;B, 3&gt; {
    let x = lhs.matmul(rhs) + bias;

    activation::relu(x)
}
</code></pre>
<p>Note that we also provide a reference implementation for testing purposes, which allows us to easily
validate our new implementation. While not mandatory, having a reference implementation can be
valuable, especially in projects where creating a reference implementation solely using basic tensor
operations is feasible.</p>
<h2 id="forward-kernel"><a class="header" href="#forward-kernel">Forward Kernel</a></h2>
<p>Now, let's proceed to write the fused kernel using the WGSL shading language. To keep things simple,
we'll create a straightforward matmul kernel without employing any intricate techniques. Although we
won't delve into the details of the WGSL syntax, as it falls beyond the scope of this guide, we
still provide the implementation below for readers who are curious. The actual matmul, add and relu
computations are found at the end, after an extensive overhead whose use is to correctly map each
thread to the data it is responsible of, with support for batches.</p>
<pre><code class="language-wgsl  ignore">@group(0)
@binding(0)
var&lt;storage, read&gt; lhs: array&lt;{{ elem }}&gt;;

@group(0)
@binding(1)
var&lt;storage, read&gt; rhs: array&lt;{{ elem }}&gt;;

@group(0)
@binding(2)
var&lt;storage, read&gt; bias: array&lt;{{ elem }}&gt;;

@group(0)
@binding(3)
var&lt;storage, read_write&gt; output: array&lt;{{ elem }}&gt;;

@group(0)
@binding(4)
var&lt;storage, read&gt; info: array&lt;u32&gt;;

const BLOCK_SIZE = {{ workgroup_size_x }}u;

@compute
@workgroup_size({{ workgroup_size_x }}, {{ workgroup_size_y }}, 1)
fn main(
    @builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,
    @builtin(local_invocation_index) local_idx: u32,
    @builtin(workgroup_id) workgroup_id: vec3&lt;u32&gt;,
) {
    // Indices
    let row = workgroup_id.x * BLOCK_SIZE + (local_idx / BLOCK_SIZE);
    let col = workgroup_id.y * BLOCK_SIZE + (local_idx % BLOCK_SIZE);
    let batch = global_id.z;

    // Basic information
    let dim = info[0];
    let n_rows = info[6u * dim - 1u];
    let n_cols = info[6u * dim];
    let K = info[5u * dim - 1u];

    // Returns if outside the output dimension
    if row &gt;= n_rows || col &gt;= n_cols {
        return;
    }

    // Calculate the corresponding offsets with support for broadcasting.
    let offset_output = batch * n_rows * n_cols;
    var offset_lhs: u32 = 0u;
    var offset_rhs: u32 = 0u;

    let batch_dims = dim - 2u;
    for (var b: u32 = 1u; b &lt;= batch_dims; b++) {
        let stride_lhs = info[b];
        let stride_rhs = info[b + dim];
        let stride_output = info[b + 2u * dim];
        let shape_lhs = info[b + 3u * dim];
        let shape_rhs = info[b + 4u * dim];

        offset_lhs += offset_output / stride_output % shape_lhs * stride_lhs;
        offset_rhs += offset_output / stride_output % shape_rhs * stride_rhs;
    }

    // Basic matmul implementation
    var sum = 0.0;
    for (var k: u32 = 0u; k &lt; K; k++) {
        let lhs_index = row * K + k;
        let rhs_index = k * n_cols + col;

        sum += lhs[offset_lhs + lhs_index] * rhs[offset_rhs + rhs_index];
    }

    let output_index = row * n_cols + col;
    let index = offset_output + output_index;

    // Add and ReLU
    output[index] = max(sum + bias[index], 0.0);
}
</code></pre>
<p>Now, let's move on to the next step, which involves implementing the remaining code to launch the
kernel. The initial part entails loading the template and populating it with the appropriate
variables. The <code>register(name, value)</code> method simply replaces occurrences of <code>{{ name }}</code> in the
above WGSL code with some other string before it is compilated.</p>
<pre><code class="language-rust  ignore">// Source the kernel written in WGSL.
kernel_wgsl!(FusedMatmulAddReluRaw, &quot;./kernel.wgsl&quot;);

// Define our kernel type with workgroup information.
#[derive(new, Debug)]
struct FusedMatmulAddRelu&lt;E: FloatElement&gt; {
    workgroup_size_x: usize,
    workgroup_size_y: usize,
    _elem: PhantomData&lt;E&gt;,
}

// Implement the dynamic kernel trait for our kernel type.
impl&lt;E: FloatElement&gt; DynamicKernel for FusedMatmulAddRelu&lt;E&gt; {
    fn source_template(self) -&gt; SourceTemplate {
        // Extend our raw kernel with workgroup size information using the
        // `SourceTemplate` trait.
        FusedMatmulAddReluRaw::source_template()
            .register(&quot;workgroup_size_x&quot;, self.workgroup_size_x.to_string())
            .register(&quot;workgroup_size_y&quot;, self.workgroup_size_y.to_string())
            .register(&quot;elem&quot;, E::type_name())
    }

    fn id(&amp;self) -&gt; String {
        format!(&quot;{:?}&quot;, self)
    }
}</code></pre>
<p>Subsequently, we'll go into implementing our custom backend trait for the WGPU backend.</p>
<pre><code class="language-rust  ignore">/// Implement our custom backend trait for the existing backend `WgpuBackend`.
impl&lt;G: GraphicsApi, F: FloatElement, I: IntElement&gt; Backend for WgpuBackend&lt;G, F, I&gt; {
    fn fused_matmul_add_relu&lt;const D: usize&gt;(
        lhs: FloatTensor&lt;Self, D&gt;,
        rhs: FloatTensor&lt;Self, D&gt;,
        bias: FloatTensor&lt;Self, D&gt;,
    ) -&gt; WgpuTensor&lt;F, D&gt; {
        // Define workgroup size, hardcoded for simplicity.
        let workgroup_size_x = 16;
        let workgroup_size_y = 16;

        lhs.assert_is_on_same_device(&amp;rhs);
        lhs.assert_is_on_same_device(&amp;bias);

        // For simplicity, make sure each tensor is continuous.
        let lhs = into_contiguous(lhs);
        let rhs = into_contiguous(rhs);
        let bias = into_contiguous(bias);

        // Get the matmul relevant shapes.
        let num_rows = lhs.shape.dims[D - 2];
        let num_cols = rhs.shape.dims[D - 1];

        // Compute shape of output, while tracking number of batches.
        let mut num_batches = 1;
        let mut shape_out = [0; D];
        for i in 0..D - 2 {
            shape_out[i] = usize::max(lhs.shape.dims[i], rhs.shape.dims[i]);
            num_batches *= shape_out[i];
        }
        shape_out[D - 2] = num_rows;
        shape_out[D - 1] = num_cols;
        let shape_out = Shape::new(shape_out);

        // Create a buffer for the output tensor.
        let buffer = lhs
            .context
            .create_buffer(shape_out.num_elements() * core::mem::size_of::&lt;F&gt;());

        // Create the output tensor primitive.
        let output = WgpuTensor::new(lhs.context.clone(), shape_out, buffer);

        let blocks_needed_in_x = f32::ceil(num_rows as f32 / workgroup_size_x as f32) as u32;
        let blocks_needed_in_y = f32::ceil(num_cols as f32 / workgroup_size_y as f32) as u32;

        // Compile the kernel or use the cache based on the template id.
        let kernel = lhs.context.compile_dynamic(FusedMatmulAddRelu::&lt;F&gt;::new(
            workgroup_size_x,
            workgroup_size_y,
        ));

        // Build info buffer with tensor information needed by the kernel, such as shapes and strides.
        let info = build_info(&amp;[&amp;lhs, &amp;rhs, &amp;output]);
        let info_buffer = lhs
            .context
            .create_buffer_with_data(bytemuck::cast_slice(&amp;info));

        // Declare the wgsl workgroup with the number of blocks in x, y and z.
        let workgroup = WorkGroup::new(blocks_needed_in_x, blocks_needed_in_y, num_batches as u32);

        // Execute lazily the kernel with the launch information and the given buffers.
        lhs.context.execute(
            workgroup,
            kernel,
            &amp;[
                &amp;lhs.buffer,
                &amp;rhs.buffer,
                &amp;bias.buffer,
                &amp;output.buffer,
                &amp;info_buffer,
            ],
        );

        // Return the output tensor.
        output
    }
}</code></pre>
<p>In the preceding code block, we demonstrated how to launch the kernel that modifies the correct
buffer. It's important to note that Rust's mutability safety doesn't apply here; the context has the
capability to execute any mutable operation on any buffer. While this isn't a problem in the
previous scenario where we only modify the newly created output buffer, it is wise to keep this in
mind.</p>
<h2 id="backward"><a class="header" href="#backward">Backward</a></h2>
<p>Now that the custom backend trait is implemented for the WGPU backend, you can use it to invoke the
<code>matmul_add_relu_custom</code> function. However, calculating gradients is not yet possible at this stage.
If your use case does not extend beyond inference, there is no need to implement any of the
following code.</p>
<p>For the backward pass, we will leverage the backend implementation from <code>burn-autodiff</code>, which is
actually generic over the backend. Instead of crafting our own WGSL kernel for the backward pass, we
will use our fused kernel only for the forward pass, and compute the gradient using basic
operations.</p>
<pre><code class="language-rust  ignore">// Implement our custom backend trait for any backend that also implements our custom backend trait.
//
// Note that we could implement the backend trait only for the Wgpu backend instead of any backend that
// also implements our own API. This would allow us to call any function only implemented for Wgpu
// and potentially call a custom kernel crafted only for this task.
impl&lt;B: Backend&gt; Backend for ADBackendDecorator&lt;B&gt; {
    fn fused_matmul_add_relu&lt;const D: usize&gt;(
        lhs: FloatTensor&lt;Self, D&gt;,
        rhs: FloatTensor&lt;Self, D&gt;,
        bias: FloatTensor&lt;Self, D&gt;,
    ) -&gt; FloatTensor&lt;Self, D&gt; {
        // Create our zero-sized type that will implement the Backward trait.
        #[derive(Debug)]
        struct FusedMatmulAddReluBackward&lt;const D: usize&gt;;

        // Implement the backward trait for the given backend B, the node gradient being of rank D
        // with three other gradients to calculate (lhs, rhs, and bias).
        impl&lt;B: Backend, const D: usize&gt; Backward&lt;B, D, 3&gt; for FusedMatmulAddReluBackward&lt;D&gt; {
            // The state that must be built during the forward pass to compute the backward pass.
            //
            // Note that we could improve the performance further by only keeping the state of
            // tensors that are tracked, improving memory management, but for simplicity, we avoid
            // that part.
            type State = (
                FloatTensor&lt;B, D&gt;,
                FloatTensor&lt;B, D&gt;,
                FloatTensor&lt;B, D&gt;,
                Shape&lt;D&gt;,
            );

            fn backward(self, ops: Ops&lt;Self::State, 3&gt;, grads: &amp;mut Gradients) {
                // Get the nodes of each variable.
                let [node_lhs, node_rhs, node_bias] = ops.parents;
                // Fetch the gradient for the current node.
                let grad = grads.consume::&lt;B, D&gt;(&amp;ops.node);

                // Set the state.
                let (lhs, rhs, output, shape_bias) = ops.state;

                // Fetch shapes of the tensors to support broadcasting.
                let shape_lhs = B::shape(&amp;lhs);
                let shape_rhs = B::shape(&amp;rhs);

                // Compute the gradient of the output using the already existing `relu_backward`
                // function in the basic Burn backend trait.
                let grad_output = B::relu_backward(output, grad);

                // Compute the lhs gradient, which is the derivative of matmul with support for
                // broadcasting.
                let grad_lhs = broadcast_shape::&lt;B, D&gt;(
                    B::matmul(grad_output.clone(), B::transpose(rhs)),
                    &amp;shape_lhs,
                );
                // Compute the rhs gradient, which is the derivative of matmul with support for
                // broadcasting.
                let grad_rhs = broadcast_shape::&lt;B, D&gt;(
                    B::matmul(B::transpose(lhs), grad_output.clone()),
                    &amp;shape_rhs,
                );
                // The add derivative is only 1, so we just need to support broadcasting to
                // compute the bias gradient.
                let grad_bias = broadcast_shape::&lt;B, D&gt;(grad_output, &amp;shape_bias);

                // Register the gradient for each variable based on whether they are marked as
                // `tracked`.
                if let Some(node) = node_bias {
                    grads.register::&lt;B, D&gt;(node, grad_bias);
                }
                if let Some(node) = node_lhs {
                    grads.register::&lt;B, D&gt;(node, grad_lhs);
                }
                if let Some(node) = node_rhs {
                    grads.register::&lt;B, D&gt;(node, grad_rhs);
                }
            }
        }

        // Prepare a stateful operation with each variable node and corresponding graph.
        //
        // Each node can be fetched with `ops.parents` in the same order as defined here.
        match FusedMatmulAddReluBackward
            .prepare(
                [lhs.node, rhs.node, bias.node],
                [lhs.graph, rhs.graph, bias.graph],
            )
            .stateful()
        {
            OpsKind::Tracked(prep) =&gt; {
                // When at least one node is tracked, we should register our backward step.
                // We compute the output and the state before finishing the preparation.
                let bias_shape = B::shape(&amp;bias.primitive);
                let output = B::fused_matmul_add_relu(
                    lhs.primitive.clone(),
                    rhs.primitive.clone(),
                    bias.primitive,
                );

                let state = (lhs.primitive, rhs.primitive, output.clone(), bias_shape);
                prep.finish(state, output)
            }
            OpsKind::UnTracked(prep) =&gt; {
                // When no node is tracked, we can just compute the original operation without
                // keeping any state.
                let output = B::fused_matmul_add_relu(lhs.primitive, rhs.primitive, bias.primitive);
                prep.finish(output)
            }
        }
    }
}</code></pre>
<p>The previous code is self-documented to make it clearer, but here is what it does in summary.</p>
<p>We define <code>fused_matmul_add_relu</code> within <code>ADBackendDecorator&lt;B&gt;</code>, allowing any autodiff-decorated
backend to benefit from our implementation. In an autodiff-decorated backend, the forward pass must
still be implemented. This is achieved using a comprehensive match statement block where computation
is delegated to the inner backend, while keeping track of a state. The state comprises any
information relevant to the backward pass, such as input and output tensors, along with the bias
shape. When an operation isn't tracked (meaning there won't be a backward pass for this specific
operation in the graph), storing a state becomes unnecessary, and we simply perform the forward
computation.</p>
<p>The backward pass uses the gradient obtained from the preceding node in the computation graph. It
calculates the derivatives for <code>relu</code> (<code>relu_backward</code>), add (no operation is required here, as the
derivative is one), and <code>matmul</code> (another <code>matmul</code> with transposed inputs). This results in
gradients for both input tensors and the bias, which are registered for consumption by subsequent
operation nodes.</p>
<p>The only remaining part is to implement our autodiff-decorated backend trait for our WGPUBackend.</p>
<pre><code class="language-rust  ignore">impl&lt;G: GraphicsApi, F: FloatElement, I: IntElement&gt; ADBackend
    for ADBackendDecorator&lt;WgpuBackend&lt;G, F, I&gt;&gt;
{
}</code></pre>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>In this guide, we've implemented a fused kernel using the WGPU backend, enabling execution on any
GPU. By delving into the inner workings of both the WGPU backend and the autodiff backend, we've
gained a deeper understanding of these systems.</p>
<p>While extending a backend may be harder than working with straightforward tensors, the benefits can
be worth it. This approach enables the crafting of custom models with greater control over
execution, which can potentially greatly enhance the performance of your models.</p>
<p>It is worth noting that while the manual fusion of operations can be valuable, our future plans
include the development of a backend extension that will automate this process. As we conclude this
guide, we hope that you have gained insights into Burn's world of backend extensions, and that it
will help you to unleash the full potential of your projects.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
