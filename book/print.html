<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Burn Book ðŸ”¥</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="motivation.html"><strong aria-hidden="true">2.</strong> Why Burn?</a></li><li class="chapter-item expanded "><a href="getting-started.html"><strong aria-hidden="true">3.</strong> Getting started</a></li><li class="chapter-item expanded "><a href="basic-workflow/index.html"><strong aria-hidden="true">4.</strong> Basic Workflow: From Training to Inference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="basic-workflow/model.html"><strong aria-hidden="true">4.1.</strong> Model</a></li><li class="chapter-item expanded "><a href="basic-workflow/data.html"><strong aria-hidden="true">4.2.</strong> Data</a></li><li class="chapter-item expanded "><a href="basic-workflow/training.html"><strong aria-hidden="true">4.3.</strong> Training</a></li><li class="chapter-item expanded "><a href="basic-workflow/backend.html"><strong aria-hidden="true">4.4.</strong> Backend</a></li><li class="chapter-item expanded "><a href="basic-workflow/inference.html"><strong aria-hidden="true">4.5.</strong> Inference</a></li><li class="chapter-item expanded "><a href="basic-workflow/conclusion.html"><strong aria-hidden="true">4.6.</strong> Conclusion</a></li></ol></li><li class="chapter-item expanded "><a href="building-blocks/index.html"><strong aria-hidden="true">5.</strong> Building Blocks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="building-blocks/backend.html"><strong aria-hidden="true">5.1.</strong> Backend</a></li><li class="chapter-item expanded "><a href="building-blocks/tensor.html"><strong aria-hidden="true">5.2.</strong> Tensor</a></li><li class="chapter-item expanded "><a href="building-blocks/autodiff.html"><strong aria-hidden="true">5.3.</strong> Autodiff</a></li><li class="chapter-item expanded "><a href="building-blocks/module.html"><strong aria-hidden="true">5.4.</strong> Module</a></li><li class="chapter-item expanded "><a href="building-blocks/learner.html"><strong aria-hidden="true">5.5.</strong> Learner</a></li><li class="chapter-item expanded "><a href="building-blocks/metric.html"><strong aria-hidden="true">5.6.</strong> Metric</a></li><li class="chapter-item expanded "><a href="building-blocks/config.html"><strong aria-hidden="true">5.7.</strong> Config</a></li><li class="chapter-item expanded "><a href="building-blocks/record.html"><strong aria-hidden="true">5.8.</strong> Record</a></li><li class="chapter-item expanded "><a href="building-blocks/dataset.html"><strong aria-hidden="true">5.9.</strong> Dataset</a></li></ol></li><li class="chapter-item expanded "><a href="custom-training-loop.html"><strong aria-hidden="true">6.</strong> Custom Training Loop</a></li><li class="chapter-item expanded "><a href="saving-and-loading.html"><strong aria-hidden="true">7.</strong> Saving & Loading Models</a></li><li class="chapter-item expanded "><a href="import/index.html"><strong aria-hidden="true">8.</strong> Import Models</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="import/onnx-model.html"><strong aria-hidden="true">8.1.</strong> ONNX Model</a></li><li class="chapter-item expanded "><a href="import/pytorch-model.html"><strong aria-hidden="true">8.2.</strong> PyTorch Model</a></li></ol></li><li class="chapter-item expanded "><a href="advanced/index.html"><strong aria-hidden="true">9.</strong> Advanced</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="advanced/backend-extension/index.html"><strong aria-hidden="true">9.1.</strong> Backend Extension</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="advanced/backend-extension/custom-wgpu-kernel.html"><strong aria-hidden="true">9.1.1.</strong> Custom WGPU Kernel</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.2.</strong> Custom Optimizer</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.3.</strong> WebAssembly</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.4.</strong> No-Std</div></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Burn Book ðŸ”¥</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>Welcome to The Burn Book ðŸ‘‹</p>
<p>This book will help you get started with the Burn deep learning framework, whether you are an
advanced user or a beginner. We have crafted some sections for you:</p>
<ul>
<li>
<p><a href="./basic-workflow">Basic Workflow: From Training to Inference</a>: We'll start with the fundamentals,
guiding you through the entire workflow, from training your models to deploying them for
inference. This section lays the groundwork for your Burn expertise.</p>
</li>
<li>
<p><a href="./building-blocks">Building Blocks</a>: Dive deeper into Burn's core components, understanding how
they fit together. This knowledge forms the basis for more advanced usage and customization.</p>
</li>
<li>
<p><a href="./saving-and-loading.html">Saving &amp; Loading Models</a>: Learn how to easily save and load your trained
models.</p>
</li>
<li>
<p><a href="./custom-training-loop.html">Custom Training Loop</a>: Gain the power to customize your training
loops, fine-tuning your models to meet your specific requirements. This section empowers you to
harness Burn's flexibility to its fullest.</p>
</li>
<li>
<p><a href="./import">Importing Models</a>: Learn how to import ONNX and PyTorch models, expanding your
compatibility with other deep learning ecosystems.</p>
</li>
<li>
<p><a href="./advanced">Advanced</a>: Finally, venture into advanced topics, exploring Burn's capabilities at
their peak. This section caters to those who want to push the boundaries of what's possible with
Burn.</p>
</li>
</ul>
<p>Throughout the book, we assume a basic understanding of deep learning concepts, but we may refer to
additional material when it seems appropriate.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="why-burn"><a class="header" href="#why-burn">Why Burn?</a></h1>
<p>Why bother with the effort of creating an entirely new deep learning framework from scratch when
PyTorch, TensorFlow, and other frameworks already exist? Spoiler alert: Burn isn't merely a
replication of PyTorch or TensorFlow in Rust. It represents a novel approach, placing significant
emphasis on making the right compromises in the right areas to facilitate exceptional flexibility,
high performance, and a seamless developer experience. Burn isnâ€™t a framework specialized for only
one type of application, it is designed to serve as a versatile framework suitable for a wide range
of research and production uses. The foundation of Burn's design revolves around three key user
profiles:</p>
<p><strong>Machine Learning Researchers</strong> require tools to construct and execute experiments efficiently.
Itâ€™s essential for them to iterate quickly on their ideas and design testable experiments which can
help them discover new findings. The framework should facilitate the swift implementation of
cutting-edge research while ensuring fast execution for testing.</p>
<p><strong>Machine Learning Engineers</strong> are another important demographic to keep in mind. Their focus leans
less on swift implementation and more on establishing robustness, seamless deployment, and
cost-effective operations. They seek dependable, economical models capable of achieving objectives
without excessive expense. The whole machine learning workflow â€”from training to inferenceâ€” must be
as efficient as possible with minimal unpredictable behavior.</p>
<p><strong>Low level Software Engineers</strong> working with hardware vendors want their processing units to run
models as fast as possible to gain competitive advantage. This endeavor involves harnessing
hardware-specific features such as Tensor Core for Nvidia. Since they are mostly working at a system
level, they want to have absolute control over how the computation will be executed.</p>
<p>The goal of Burn is to satisfy all of those personas!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>Burn is a deep learning framework in the Rust programming language. Therefore, it goes without
saying that one must understand the basic notions of Rust. Reading the first chapters of the
<a href="https://doc.rust-lang.org/book/">Rust Book</a> is recommended, but don't worry if you're just starting
out. We'll try to provide as much context and reference to external resources when required. Just
look out for the <strong>ðŸ¦€ Rust Note</strong> indicators.</p>
<h2 id="installing-rust"><a class="header" href="#installing-rust">Installing Rust</a></h2>
<p>For installation instructions, please refer to the
<a href="https://doc.rust-lang.org/book/ch01-01-installation.html">installation page</a>. It explains in
details the most convenient way for you to install Rust on your computer, which is the very first
thing to do to start using Burn.</p>
<h2 id="creating-a-burn-application"><a class="header" href="#creating-a-burn-application">Creating a Burn application</a></h2>
<p>Once Rust is correctly installed, create a new Rust application by using Rust's build system and
package manager Cargo. It is automatically installed with Rust.</p>
<details>
<summary><strong>ðŸ¦€ Cargo Cheat Sheet</strong></summary>
<p><a href="https://doc.rust-lang.org/cargo/">Cargo</a> is a very useful tool to manage Rust projects because it
handles a lot of tasks. More precisely, it is used to compile your code, download the
libraries/packages your code depends on, and build said libraries.</p>
<p>Below is a quick cheat sheet of the main <code>cargo</code> commands you might use throughout this guide.</p>
<div class="table-wrapper"><table><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody>
<tr><td><code>cargo new</code> <em>path</em></td><td>Create a new Cargo package in the given directory.</td></tr>
<tr><td><code>cargo add</code> <em>crate</em></td><td>Add dependencies to the Cargo.toml manifest file.</td></tr>
<tr><td><code>cargo build</code></td><td>Compile the local package and all of its dependencies (in debug mode, use <code>-r</code> for release).</td></tr>
<tr><td><code>cargo check</code></td><td>Check the local package for compilation errors (much faster).</td></tr>
<tr><td><code>cargo run</code></td><td>Run the local package binary.</td></tr>
</tbody></table>
</div>
<p>For more information, check out
<a href="https://doc.rust-lang.org/book/ch01-03-hello-cargo.html">Hello, Cargo!</a> in the Rust Book.</p>
</details><br>
<p>In the directory of your choice, run the following:</p>
<pre><code class="language-console">cargo new my_burn_app
</code></pre>
<p>This will initialize the <code>my_burn_app</code> project directory with a <code>Cargo.toml</code> file a a <code>src</code>
directory with an auto-generated <code>main.rs</code> file inside. Head inside the directory to check:</p>
<pre><code class="language-console">cd my_burn_app
</code></pre>
<p>Then, add Burn as a dependency:</p>
<pre><code class="language-console">cargo add burn --features wgpu
</code></pre>
<p>Finally, compile the local package by executing the following:</p>
<pre><code class="language-console">cargo build
</code></pre>
<p>That's it, you're ready to start! You have a project configured with Burn and the WGPU backend,
which allows to execute low-level operations on any platform using the GPU.</p>
<h2 id="writing-a-code-snippet"><a class="header" href="#writing-a-code-snippet">Writing a code snippet</a></h2>
<p>The <code>src/main.rs</code> was automatically generated by Cargo, so let's replace its content with the
following:</p>
<pre><code class="language-rust  ignore">use burn::tensor::Tensor;
use burn::backend::Wgpu;

// Type alias for the backend to use.
type Backend = Wgpu;

fn main() {
    let device = Default::default();
    // Creation of two tensors, the first with explicit values and the second one with ones, with the same shape as the first
    let tensor_1 = Tensor::&lt;Backend, 2&gt;::from_data([[2., 3.], [4., 5.]], &amp;device);
    let tensor_2 = Tensor::&lt;Backend, 2&gt;::ones_like(&amp;tensor_1);

    // Print the element-wise addition (done with the WGPU backend) of the two tensors.
    println!(&quot;{}&quot;, tensor_1 + tensor_2);
}</code></pre>
<details>
<summary><strong>ðŸ¦€ Use Declarations</strong></summary>
<p>To bring any of the Burn module or item into scope, a <code>use</code> declaration is added.</p>
<p>In the example above, we wanted bring the <code>Tensor</code> struct and <code>Wgpu</code> backend into scope with the
following:</p>
<pre><code class="language-rust  ignore">use burn::tensor::Tensor;
use burn::backend::Wgpu;</code></pre>
<p>This is pretty self-explanatory in this case. But, the same declaration could be written as a
shortcut to simultaneously binding of multiple paths with a common prefix:</p>
<pre><code class="language-rust  ignore">use burn::{tensor::Tensor, backend::backend::Wgpu};</code></pre>
<p>In this example, the common prefix is pretty short and there are only two items to bind locally.
Therefore, the first usage with two <code>use</code> declarations might be preferred. But know that both
examples are valid. For more details on the <code>use</code> keyword, take a look at
<a href="https://doc.rust-lang.org/book/ch07-04-bringing-paths-into-scope-with-the-use-keyword.html">this section</a>
of the Rust Book or the
<a href="https://doc.rust-lang.org/reference/items/use-declarations.html">Rust reference</a>.</p>
</details><br>
<details>
<summary><strong>ðŸ¦€ Generic Data Types</strong></summary>
<p>If you're new to Rust, you're probably wondering why we had to use <code>Tensor::&lt;Backend, 2&gt;::...</code>.
That's because the <code>Tensor</code> struct is <a href="https://doc.rust-lang.org/book/ch10-01-syntax.html">generic</a>
over multiple concrete data types. More specifically, a <code>Tensor</code> can be used for 3 generic
parameters: a <code>Tensor</code> struct has 3 generic arguments: the backend, the number of dimensions (rank)
and the data type (defaults to <code>Float</code>). Here, we only specify the backend and number of dimensions
since a <code>Float</code> tensor is used by default. For more details on the <code>Tensor</code> struct, take a look at
<a href="./building-blocks/tensor.html">this section</a>.</p>
<p>Most of the time when generics are involved, the compiler can infer the generic parameters
automatically. In this case, the compiler needs a little help. This can usually be done in one of
two ways: providing a type annotation or binding the gereneric parameter via the <em>turbofish</em> <code>::&lt;&gt;</code>
syntax. In the example we used the so-called <em>turbofish</em> syntax, but we could have used type
annotations instead.</p>
<pre><code class="language-rust  ignore">let tensor_1: Tensor&lt;Backend, 2&gt; = Tensor::from_data([[2., 3.], [4., 5.]]);
let tensor_2 = Tensor::ones_like(&amp;tensor_1);</code></pre>
<p>You probably noticed that we provided a type annotation for the first tensor, yet it still worked.
That's because the compiler (correctly) inferred that <code>tensor_2</code> had the same generic parameters.
The same could have been done in the original example, but specifying the parameters for both is
more explicit.</p>
</details><br>
<p>By running <code>cargo run</code>, you should now see the result of the addition:</p>
<pre><code class="language-console">Tensor {
  data:
[[3.0, 4.0],
 [5.0, 6.0]],
  shape:  [2, 2],
  device:  BestAvailable,
  backend:  &quot;wgpu&quot;,
  kind:  &quot;Float&quot;,
  dtype:  &quot;f32&quot;,
}
</code></pre>
<p>While the previous example is somewhat trivial, the upcoming
basic workflow section will walk you through a much more relevant example for
deep learning applications.</p>
<h2 id="running-examples"><a class="header" href="#running-examples">Running examples</a></h2>
<p>Many additional Burn examples available in the
<a href="https://github.com/tracel-ai/burn/tree/main/examples">examples</a> directory. To run one, please refer
to the example's README.md for the specific command to execute.</p>
<p>Note that some examples use the
<a href="https://huggingface.co/docs/datasets/index"><code>datasets</code> library by HuggingFace</a> to download the
datasets required in the examples. This is a Python library, which means that you will need to
install Python before running these examples. This requirement will be clearly indicated in the
example's README when applicable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="guide"><a class="header" href="#guide">Guide</a></h1>
<p>This guide will walk you through the process of creating a custom model built with Burn. We will
train a simple convolutional neural network model on the MNIST dataset and prepare it for inference.</p>
<p>For clarity, we sometimes omit imports in our code snippets. For more details, please refer to the
corresponding code in the <code>examples/guide</code> <a href="https://github.com/tracel-ai/burn/tree/main/examples/guide">directory</a>. We
reproduce this example in a step-by-step fashion, from dataset creation to modeling and training in the following
sections.
The code for this demo can be executed from Burn's base directory using the command:</p>
<pre><code class="language-bash">cargo run --example guide
</code></pre>
<h2 id="key-learnings"><a class="header" href="#key-learnings">Key Learnings</a></h2>
<ul>
<li>Creating a project</li>
<li>Creating neural network models</li>
<li>Importing and preparing datasets</li>
<li>Training models on data</li>
<li>Choosing a backend</li>
<li>Using a model for inference</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model"><a class="header" href="#model">Model</a></h1>
<p>The first step is to create a project and add the different Burn dependencies. Start by creating a
new project with Cargo:</p>
<pre><code class="language-console">cargo new my-first-burn-model
</code></pre>
<p>As <a href="basic-workflow/../getting-started.html#creating-a-burn-application">mentioned previously</a>, this will initialize
your <code>my-first-burn-model</code> project directory with a <code>Cargo.toml</code> and a <code>src/main.rs</code> file.</p>
<p>In the <code>Cargo.toml</code> file, add the <code>burn</code> dependency with <code>train</code> and <code>wgpu</code> features.</p>
<pre><code class="language-toml">[package]
name = &quot;my-first-burn-model&quot;
version = &quot;0.1.0&quot;
edition = &quot;2021&quot;

[dependencies]
burn = { version = &quot;0.12.1&quot;, features = [&quot;train&quot;, &quot;wgpu&quot;] }
</code></pre>
<p>Our goal will be to create a basic convolutional neural network used for image classification. We
will keep the model simple by using two convolution layers followed by two linear layers, some
pooling and ReLU activations. We will also use dropout to improve training performance.</p>
<p>Let us start by defining our model struct in a new file <code>src/model.rs</code>.</p>
<pre><code class="language-rust   ignore">use burn::{
    config::Config,
    module::Module,
    nn::{
        conv::{Conv2d, Conv2dConfig},
        pool::{AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},
        Dropout, DropoutConfig, Linear, LinearConfig, ReLU,
    },
    tensor::{backend::Backend, Tensor},
};

#[derive(Module, Debug)]
pub struct Model&lt;B: Backend&gt; {
    conv1: Conv2d&lt;B&gt;,
    conv2: Conv2d&lt;B&gt;,
    pool: AdaptiveAvgPool2d,
    dropout: Dropout,
    linear1: Linear&lt;B&gt;,
    linear2: Linear&lt;B&gt;,
    activation: ReLU,
}</code></pre>
<p>There are two major things going on in this code sample.</p>
<ol>
<li>
<p>You can create a deep learning module with the <code>#[derive(Module)]</code> attribute on top of a struct.
This will generate the necessary code so that the struct implements the <code>Module</code> trait. This
trait will make your module both trainable and (de)serializable while adding related
functionalities. Like other attributes often used in Rust, such as <code>Clone</code>, <code>PartialEq</code> or
<code>Debug</code>, each field within the struct must also implement the <code>Module</code> trait.</p>
<details>
<summary><strong>ðŸ¦€ Trait</strong></summary>
<p>Traits are a powerful and flexible Rust language feature. They provide a way to define shared
behavior for a particular type, which can be shared with other types.</p>
<p>A type's behavior consists of the methods called on that type. Since all <code>Module</code>s should
implement the same functionality, it is defined as a trait. Implementing a trait on a particular
type usually requires the user to implement the defined behaviors of the trait for their types,
though that is not the case here as explained above with the <code>derive</code> attribute. Check out the
<a href="basic-workflow/model.html#derive-attribute">explainer below</a> to learn why.</p>
<p>For more details on traits, take a look at the
<a href="https://doc.rust-lang.org/book/ch10-02-traits.html">associated chapter</a> in the Rust Book.</p>
</details><br>
<details id="derive-attribute">
<summary><strong>ðŸ¦€ Derive Macro</strong></summary>
<p>The <code>derive</code> attribute allows traits to be implemented easily by generating code that will
implement a trait with its own default implementation on the type that was annotated with the
<code>derive</code> syntax.</p>
<p>This is accomplished through a feature of Rust called
<a href="https://doc.rust-lang.org/reference/procedural-macros.html">procedural macros</a>, which allow us
to run code at compile time that operates over Rust syntax, both consuming and producing Rust
syntax. Using the attribute <code>#[my_macro]</code>, you can effectively extend the provided code. You will
see that the derive macro is very frequently employed to recursively implement traits, where the
implementation consists of the composition of all fields.</p>
<p>In this example, we want to derive the <a href="basic-workflow/../building-blocks/module.html"><code>Module</code></a> and <code>Debug</code>
traits.</p>
<pre><code class="language-rust  ignore">#[derive(Module, Debug)]
pub struct MyCustomModule&lt;B: Backend&gt; {
    linear1: Linear&lt;B&gt;,
    linear2: Linear&lt;B&gt;,
    activation: ReLU,
}</code></pre>
<p>The basic <code>Debug</code> implementation is provided by the compiler to format a value using the <code>{:?}</code>
formatter. For ease of use, the <code>Module</code> trait implementation is automatically handled by Burn so
you don't have to do anything special. It essentially acts as parameter container.</p>
<p>For more details on derivable traits, take a look at the Rust
<a href="https://doc.rust-lang.org/book/appendix-03-derivable-traits.html">appendix</a>,
<a href="https://doc.rust-lang.org/reference/attributes/derive.html">reference</a> or
<a href="https://doc.rust-lang.org/rust-by-example/trait/derive.html">example</a>.</p>
</details><br>
</li>
<li>
<p>Note that the struct is generic over the <a href="basic-workflow/../building-blocks/backend.html"><code>Backend</code></a> trait. The
backend trait abstracts the underlying low level implementations of tensor operations, allowing
your new model to run on any backend. Contrary to other frameworks, the backend abstraction isn't
determined by a compilation flag or a device type. This is important because you can extend the
functionalities of a specific backend (see
<a href="basic-workflow/../advanced/backend-extension">backend extension section</a>), and it allows for an innovative
<a href="basic-workflow/../building-blocks/autodiff.html">autodiff system</a>. You can also change backend during runtime,
for instance to compute training metrics on a cpu backend while using a gpu one only to train the
model. In our example, the backend in use will be determined later on.</p>
<details>
<summary><strong>ðŸ¦€ Trait Bounds</strong></summary>
<p>Trait bounds provide a way for generic items to restrict which types are used as their
parameters. The trait bounds stipulate what functionality a type implements. Therefore, bounding
restricts the generic to types that conform to the bounds. It also allows generic instances to
access the methods of traits specified in the bounds.</p>
<p>For a simple but concrete example, check out the
<a href="https://doc.rust-lang.org/rust-by-example/generics/bounds.html">Rust By Example on bounds</a>.</p>
<p>In Burn, the <code>Backend</code> trait enables you to run tensor operations using different implementations
as it abstracts tensor, device and element types. The
<a href="basic-workflow/../getting-started.html#writing-a-code-snippet">getting started example</a> illustrates the advantage
of having a simple API that works for different backend implementations. While it used the WGPU
backend, you could easily swap it with any other supported backend.</p>
<pre><code class="language-rust  ignore">// Choose from any of the supported backends.
// type Backend = Candle&lt;f32, i64&gt;;
// type Backend = LibTorch&lt;f32&gt;;
// type Backend = NdArray&lt;f32&gt;;
type Backend = Wgpu;

// Creation of two tensors.
let tensor_1 = Tensor::&lt;Backend, 2&gt;::from_data([[2., 3.], [4., 5.]], &amp;device);
let tensor_2 = Tensor::&lt;Backend, 2&gt;::ones_like(&amp;tensor_1);

// Print the element-wise addition (done with the selected backend) of the two tensors.
println!(&quot;{}&quot;, tensor_1 + tensor_2);</code></pre>
<p>For more details on trait bounds, check out the Rust
<a href="https://doc.rust-lang.org/book/ch10-02-traits.html#trait-bound-syntax">trait bound section</a> or
<a href="https://doc.rust-lang.org/reference/items/traits.html#trait-bounds">reference</a>.</p>
</details><br>
</li>
</ol>
<p>Next, we need to instantiate the model for training.</p>
<pre><code class="language-rust   ignore">#[derive(Config, Debug)]
pub struct ModelConfig {
    num_classes: usize,
    hidden_size: usize,
    #[config(default = &quot;0.5&quot;)]
    dropout: f64,
}

impl ModelConfig {
    /// Returns the initialized model.
    pub fn init&lt;B: Backend&gt;(&amp;self, device: &amp;B::Device) -&gt; Model&lt;B&gt; {
        Model {
            conv1: Conv2dConfig::new([1, 8], [3, 3]).init(device),
            conv2: Conv2dConfig::new([8, 16], [3, 3]).init(device),
            pool: AdaptiveAvgPool2dConfig::new([8, 8]).init(),
            activation: ReLU::new(),
            linear1: LinearConfig::new(16 * 8 * 8, self.hidden_size).init(device),
            linear2: LinearConfig::new(self.hidden_size, self.num_classes).init(device),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }
}</code></pre>
<details>
<summary><strong>ðŸ¦€ References</strong></summary>
<p>In the previous example, the <code>init()</code> method signature uses <code>&amp;</code> to indicate that the parameter types
are references: <code>&amp;self</code>, a reference to the current receiver (<code>ModelConfig</code>), and
<code>device: &amp;B::Device</code>, a reference to the backend device.</p>
<pre><code class="language-rust  ignore">pub fn init&lt;B: Backend&gt;(&amp;self, device: &amp;B::Device) -&gt; Model&lt;B&gt; {
    Model {
        // ...
    }
}</code></pre>
<p>References in Rust allow us to point to a resource to access its data without owning it. The idea of
ownership is quite core to Rust and is worth
<a href="https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html">reading up on</a>.</p>
<p>In a language like C, memory management is explicit and up to the programmer, which means it is easy
to make mistakes. In a language like Java or Python, memory management is automatic with the help of
a garbage collector. This is very safe and straightforward, but also incurs a runtime cost.</p>
<p>In Rust, memory management is rather unique. Aside from simple types that implement
<a href="https://doc.rust-lang.org/std/marker/trait.Copy.html"><code>Copy</code></a> (e.g.,
<a href="https://doc.rust-lang.org/rust-by-example/primitives.html">primitives</a> like integers, floats,
booleans and <code>char</code>), every value is <em>owned</em> by some variable called the <em>owner</em>. Ownership can be
transferred from one variable to another and sometimes a value can be <em>borrowed</em>. Once the <em>owner</em>
variable goes out of scope, the value is <em>dropped</em>, which means that any memory it allocated can be
freed safely.</p>
<p>Because the method does not own the <code>self</code> and <code>device</code> variables, the values the references point
to will not be dropped when the reference stops being used (i.e., the scope of the method).</p>
<p>For more information on references and borrowing, be sure to read the
<a href="https://doc.rust-lang.org/book/ch04-02-references-and-borrowing.html">corresponding chapter</a> in the
Rust Book.</p>
</details><br>
<p>When creating a custom neural network module, it is often a good idea to create a config alongside
the model struct. This allows you to define default values for your network, thanks to the <code>Config</code>
attribute. The benefit of this attribute is that it makes the configuration serializable, enabling
you to painlessly save your model hyperparameters, enhancing your experimentation process. Note that
a constructor will automatically be generated for your configuration, which will take as input
values for the parameter which do not have default values:
<code>let config = ModelConfig::new(num_classes, hidden_size);</code>. The default values can be overridden
easily with builder-like methods: (e.g <code>config.with_dropout(0.2);</code>)</p>
<p>The first implementation block is related to the initialization method. As we can see, all fields
are set using the configuration of the corresponding neural network underlying module. In this
specific case, we have chosen to expand the tensor channels from 1 to 8 with the first layer, then
from 8 to 16 with the second layer, using a kernel size of 3 on all dimensions. We also use the
adaptive average pooling module to reduce the dimensionality of the images to an 8 by 8 matrix,
which we will flatten in the forward pass to have a 1024 (16 _ 8 _ 8) resulting tensor.</p>
<p>Now let's see how the forward pass is defined.</p>
<pre><code class="language-rust   ignore">impl&lt;B: Backend&gt; Model&lt;B&gt; {
    /// # Shapes
    ///   - Images [batch_size, height, width]
    ///   - Output [batch_size, num_classes]
    pub fn forward(&amp;self, images: Tensor&lt;B, 3&gt;) -&gt; Tensor&lt;B, 2&gt; {
        let [batch_size, height, width] = images.dims();

        // Create a channel at the second dimension.
        let x = images.reshape([batch_size, 1, height, width]);


        let x = self.conv1.forward(x); // [batch_size, 8, _, _]
        let x = self.dropout.forward(x);
        let x = self.conv2.forward(x); // [batch_size, 16, _, _]
        let x = self.dropout.forward(x);
        let x = self.activation.forward(x);

        let x = self.pool.forward(x); // [batch_size, 16, 8, 8]
        let x = x.reshape([batch_size, 16 * 8 * 8]);
        let x = self.linear1.forward(x);
        let x = self.dropout.forward(x);
        let x = self.activation.forward(x);

        self.linear2.forward(x) // [batch_size, num_classes]
    }
}</code></pre>
<p>For former PyTorch users, this might feel very intuitive, as each module is directly incorporated
into the code using an eager API. Note that no abstraction is imposed for the forward method. You
are free to define multiple forward functions with the names of your liking. Most of the neural
network modules already built with Burn use the <code>forward</code> nomenclature, simply because it is
standard in the field.</p>
<p>Similar to neural network modules, the <a href="basic-workflow/../building-blocks/tensor.html"><code>Tensor</code></a> struct given as a
parameter also takes the Backend trait as a generic argument, alongside its dimensionality. Even if it is not
used in this specific example, it is possible to add the kind of the tensor as a third generic
argument. For example, a 3-dimensional Tensor of different data types(float, int, bool) would be defined as following:</p>
<pre><code class="language-rust   ignore">Tensor&lt;B, 3&gt; // Float tensor (default)
Tensor&lt;B, 3, Float&gt; // Float tensor (explicit)
Tensor&lt;B, 3, Int&gt; // Int tensor
Tensor&lt;B, 3, Bool&gt; // Bool tensor</code></pre>
<p>Note that the specific element type, such as <code>f16</code>, <code>f32</code> and the likes, will be defined later with
the backend.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data"><a class="header" href="#data">Data</a></h1>
<p>Typically, one trains a model on some dataset. Burn provides a library of very useful dataset
sources and transformations. In particular, there are Hugging Face dataset utilities that allow to
download and store data from Hugging Face into an SQLite database for extremely efficient data
streaming and storage. For this guide, we will use the MNIST dataset provided by Hugging Face.</p>
<p>To iterate over a dataset efficiently, we will define a struct which will implement the <code>Batcher</code>
trait. The goal of a batcher is to map individual dataset items into a batched tensor that can be
used as input to our previously defined model.</p>
<p>Let us start by defining our dataset functionalities in a file <code>src/data.rs</code>. We shall omit some of the imports for
brevity,
but the full code for following this guide can be found
at <code>examples/guide/</code> <a href="https://github.com/tracel-ai/burn/tree/main/examples/guide">directory</a>.</p>
<pre><code class="language-rust   ignore">use burn::{
    data::{dataloader::batcher::Batcher, dataset::vision::MNISTItem},
    tensor::{backend::Backend, Data, ElementConversion, Int, Tensor},
};

pub struct MNISTBatcher&lt;B: Backend&gt; {
    device: B::Device,
}

impl&lt;B: Backend&gt; MNISTBatcher&lt;B&gt; {
    pub fn new(device: B::Device) -&gt; Self {
        Self { device }
    }
}
</code></pre>
<p>This codeblock defines a batcher struct with the device in which the tensor should be sent before
being passed to the model. Note that the device is an associative type of the <code>Backend</code> trait since
not all backends expose the same devices. As an example, the Libtorch-based backend exposes
<code>Cuda(gpu_index)</code>, <code>Cpu</code>, <code>Vulkan</code> and <code>Metal</code> devices, while the ndarray backend only exposes the
<code>Cpu</code> device.</p>
<p>Next, we need to actually implement the batching logic.</p>
<pre><code class="language-rust   ignore">#[derive(Clone, Debug)]
pub struct MNISTBatch&lt;B: Backend&gt; {
    pub images: Tensor&lt;B, 3&gt;,
    pub targets: Tensor&lt;B, 1, Int&gt;,
}

impl&lt;B: Backend&gt; Batcher&lt;MNISTItem, MNISTBatch&lt;B&gt;&gt; for MNISTBatcher&lt;B&gt; {
    fn batch(&amp;self, items: Vec&lt;MNISTItem&gt;) -&gt; MNISTBatch&lt;B&gt; {
        let images = items
            .iter()
            .map(|item| Data::&lt;f32, 2&gt;::from(item.image))
            .map(|data| Tensor::&lt;B, 2&gt;::from_data(data.convert(), &amp;self.device))
            .map(|tensor| tensor.reshape([1, 28, 28]))
            // Normalize: make between [0,1] and make the mean=0 and std=1
            // values mean=0.1307,std=0.3081 are from the PyTorch MNIST example
            // https://github.com/pytorch/examples/blob/54f4572509891883a947411fd7239237dd2a39c3/mnist/main.py#L122
            .map(|tensor| ((tensor / 255) - 0.1307) / 0.3081)
            .collect();

        let targets = items
            .iter()
            .map(|item| Tensor::&lt;B, 1, Int&gt;::from_data(
                Data::from([(item.label as i64).elem()]),
                &amp;self.device
            ))
            .collect();

        let images = Tensor::cat(images, 0).to_device(&amp;self.device);
        let targets = Tensor::cat(targets, 0).to_device(&amp;self.device);

        MNISTBatch { images, targets }
    }
}</code></pre>
<details>
<summary><strong>ðŸ¦€ Iterators and Closures</strong></summary>
<p>The iterator pattern allows you to perform some tasks on a sequence of items in turn.</p>
<p>In this example, an iterator is created over the <code>MNISTItem</code>s in the vector <code>items</code> by calling the
<code>iter</code> method.</p>
<p><em>Iterator adaptors</em> are methods defined on the <code>Iterator</code> trait that produce different iterators by
changing some aspect of the original iterator. Here, the <code>map</code> method is called in a chain to
transform the original data before consuming the final iterator with <code>collect</code> to obtain the
<code>images</code> and <code>targets</code> vectors. Both vectors are then concatenated into a single tensor for the
current batch.</p>
<p>You probably noticed that each call to <code>map</code> is different, as it defines a function to execute on
the iterator items at each step. These anonymous functions are called
<a href="https://doc.rust-lang.org/book/ch13-01-closures.html"><em>closures</em></a> in Rust. They're easy to
recognize due to their syntax which uses vertical bars <code>||</code>. The vertical bars capture the input
variables (if applicable) while the rest of the expression defines the function to execute.</p>
<p>If we go back to the example, we can break down and comment the expression used to process the
images.</p>
<pre><code class="language-rust  ignore">let images = items                                                       // take items Vec&lt;MNISTItem&gt;
    .iter()                                                              // create an iterator over it
    .map(|item| Data::&lt;f32, 2&gt;::from(item.image))                        // for each item, convert the image to float32 data struct
    .map(|data| Tensor::&lt;B, 2&gt;::from_data(data.convert(), &amp;self.device)) // for each data struct, create a tensor on the device
    .map(|tensor| tensor.reshape([1, 28, 28]))                           // for each tensor, reshape to the image dimensions [C, H, W]
    .map(|tensor| ((tensor / 255) - 0.1307) / 0.3081)                    // for each image tensor, apply normalization
    .collect();                                                          // consume the resulting iterator &amp; collect the values into a new vector</code></pre>
<p>For more information on iterators and closures, be sure to check out the
<a href="https://doc.rust-lang.org/book/ch13-00-functional-features.html">corresponding chapter</a> in the Rust
Book.</p>
</details><br>
<p>In the previous example, we implement the <code>Batcher</code> trait with a list of <code>MNISTItem</code> as input and a
single <code>MNISTBatch</code> as output. The batch contains the images in the form of a 3D tensor, along with
a targets tensor that contains the indexes of the correct digit class. The first step is to parse
the image array into a <code>Data</code> struct. Burn provides the <code>Data</code> struct to encapsulate tensor storage
information without being specific for a backend. When creating a tensor from data, we often need to
convert the data precision to the current backend in use. This can be done with the <code>.convert()</code>
method. While importing the <code>burn::tensor::ElementConversion</code> trait, you can call <code>.elem()</code> on a
specific number to convert it to the current backend element type in use.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="training"><a class="header" href="#training">Training</a></h1>
<p>We are now ready to write the necessary code to train our model on the MNIST dataset.
We shall define the code for this training section in the file: <code>src/training.rs</code>.</p>
<p>Instead of a simple tensor, the model should output an item that can be understood by the learner, a struct whose
responsibility is to apply an optimizer to the model. The output struct is used for all metrics
calculated during the training. Therefore it should include all the necessary information to
calculate any metric that you want for a task.</p>
<p>Burn provides two basic output types: <code>ClassificationOutput</code> and <code>RegressionOutput</code>. They implement
the necessary trait to be used with metrics. It is possible to create your own item, but it is
beyond the scope of this guide.</p>
<p>Since the MNIST task is a classification problem, we will use the <code>ClassificationOutput</code> type.</p>
<pre><code class="language-rust   ignore">impl&lt;B: Backend&gt; Model&lt;B&gt; {
    pub fn forward_classification(
        &amp;self,
        images: Tensor&lt;B, 3&gt;,
        targets: Tensor&lt;B, 1, Int&gt;,
    ) -&gt; ClassificationOutput&lt;B&gt; {
        let output = self.forward(images);
        let loss = CrossEntropyLoss::new(None).forward(output.clone(), targets.clone());

        ClassificationOutput::new(loss, output, targets)
    }
}</code></pre>
<p>As evident from the preceding code block, we employ the cross-entropy loss module for loss
calculation, without the inclusion of any padding token. We then return the classification output
containing the loss, the output tensor with all logits and the targets.</p>
<p>Please take note that tensor operations receive owned tensors as input. For reusing a tensor
multiple times, you need to use the <code>clone()</code> function. There's no need to worry; this process won't
involve actual copying of the tensor data. Instead, it will simply indicate that the tensor is
employed in multiple instances, implying that certain operations won't be performed in place. In
summary, our API has been designed with owned tensors to optimize performance.</p>
<p>Moving forward, we will proceed with the implementation of both the training and validation steps
for our model.</p>
<pre><code class="language-rust   ignore">impl&lt;B: AutodiffBackend&gt; TrainStep&lt;MNISTBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt; for Model&lt;B&gt; {
    fn step(&amp;self, batch: MNISTBatch&lt;B&gt;) -&gt; TrainOutput&lt;ClassificationOutput&lt;B&gt;&gt; {
        let item = self.forward_classification(batch.images, batch.targets);

        TrainOutput::new(self, item.loss.backward(), item)
    }
}

impl&lt;B: Backend&gt; ValidStep&lt;MNISTBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt; for Model&lt;B&gt; {
    fn step(&amp;self, batch: MNISTBatch&lt;B&gt;) -&gt; ClassificationOutput&lt;B&gt; {
        self.forward_classification(batch.images, batch.targets)
    }
}</code></pre>
<p>Here we define the input and output types as generic arguments in the <code>TrainStep</code> and <code>ValidStep</code>.
We will call them <code>MNISTBatch</code> and <code>ClassificationOutput</code>. In the training step, the computation of
gradients is straightforward, necessitating a simple invocation of <code>backward()</code> on the loss. Note
that contrary to PyTorch, gradients are not stored alongside each tensor parameter, but are rather
returned by the backward pass, as such: <code>let gradients = loss.backward();</code>. The gradient of a
parameter can be obtained with the grad function: <code>let grad = tensor.grad(&amp;gradients);</code>. Although it
is not necessary when using the learner struct and the optimizers, it can prove to be quite useful
when debugging or writing custom training loops. One of the differences between the training and the
validation steps is that the former requires the backend to implement <code>AutodiffBackend</code> and not just
<code>Backend</code>. Otherwise, the <code>backward</code> function is not available, as the backend does not support
autodiff. We will see later how to create a backend with autodiff support.</p>
<details>
<summary><strong>ðŸ¦€ Generic Type Constraints in Method Definitions</strong></summary>
<p>Although generic data types, trait and trait bounds were already introduced in previous sections of
this guide, the previous code snippet might be a lot to take in at first.</p>
<p>In the example above, we implement the <code>TrainStep</code> and <code>ValidStep</code> trait for our <code>Model</code> struct,
which is generic over the <code>Backend</code> trait as has been covered before. These traits are provided by
<code>burn::train</code> and define a common <code>step</code> method that should be implemented for all structs. Since
the trait is generic over the input and output types, the trait implementation must specify the
concrete types used. This is where the additional type constraints appear
<code>&lt;MNISTBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt;</code>. As we saw previously, the concrete input type for the
batch is <code>MNISTBatch</code>, and the output of the forward pass is <code>ClassificationOutput</code>. The <code>step</code>
method signature matches the concrete input and output types.</p>
<p>For more details specific to constraints on generic types when defining methods, take a look at
<a href="https://doc.rust-lang.org/book/ch10-01-syntax.html#in-method-definitions">this section</a> of the Rust
Book.</p>
</details><br>
<p>Let us move on to establishing the practical training configuration.</p>
<pre><code class="language-rust   ignore">#[derive(Config)]
pub struct TrainingConfig {
    pub model: ModelConfig,
    pub optimizer: AdamConfig,
    #[config(default = 10)]
    pub num_epochs: usize,
    #[config(default = 64)]
    pub batch_size: usize,
    #[config(default = 4)]
    pub num_workers: usize,
    #[config(default = 42)]
    pub seed: u64,
    #[config(default = 1.0e-4)]
    pub learning_rate: f64,
}

pub fn train&lt;B: AutodiffBackend&gt;(artifact_dir: &amp;str, config: TrainingConfig, device: B::Device) {
    std::fs::create_dir_all(artifact_dir).ok();
    config
        .save(format!(&quot;{artifact_dir}/config.json&quot;))
        .expect(&quot;Config should be saved successfully&quot;);

    B::seed(config.seed);

    let batcher_train = MNISTBatcher::&lt;B&gt;::new(device.clone());
    let batcher_valid = MNISTBatcher::&lt;B::InnerBackend&gt;::new(device.clone());

    let dataloader_train = DataLoaderBuilder::new(batcher_train)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MNISTDataset::train());

    let dataloader_test = DataLoaderBuilder::new(batcher_valid)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MNISTDataset::test());

    let learner = LearnerBuilder::new(artifact_dir)
        .metric_train_numeric(AccuracyMetric::new())
        .metric_valid_numeric(AccuracyMetric::new())
        .metric_train_numeric(LossMetric::new())
        .metric_valid_numeric(LossMetric::new())
        .with_file_checkpointer(CompactRecorder::new())
        .devices(vec![device])
        .num_epochs(config.num_epochs)
        .build(
            config.model.init::&lt;B&gt;(),
            config.optimizer.init(),
            config.learning_rate,
        );

    let model_trained = learner.fit(dataloader_train, dataloader_test);

    model_trained
        .save_file(format!(&quot;{artifact_dir}/model&quot;), &amp;CompactRecorder::new())
        .expect(&quot;Trained model should be saved successfully&quot;);
}</code></pre>
<p>It is a good practice to use the <code>Config</code> derive to create the experiment configuration. In the
<code>train</code> function, the first thing we are doing is making sure the <code>artifact_dir</code> exists, using the
standard rust library for file manipulation. All checkpoints, logging and metrics will be stored
under this directory. We then initialize our dataloaders using our previously created batcher. Since
no automatic differentiation is needed during the validation phase, the backend used for the
corresponding batcher is <code>B::InnerBackend</code> (see <a href="basic-workflow/./backend.html">Backend</a>). The autodiff capabilities
are available through a type system, making it nearly impossible to forget to deactivate gradient
calculation.</p>
<p>Next, we create our learner with the accuracy and loss metric on both training and validation steps
along with the device and the epoch. We also configure the checkpointer using the <code>CompactRecorder</code>
to indicate how weights should be stored. This struct implements the <code>Recorder</code> trait, which makes
it capable of saving records for persistency.</p>
<p>We then build the learner with the model, the optimizer and the learning rate. Notably, the third
argument of the build function should actually be a learning rate <em>scheduler</em>. When provided with a
float as in our example, it is automatically transformed into a <em>constant</em> learning rate scheduler.
The learning rate is not part of the optimizer config as it is often done in other frameworks, but
rather passed as a parameter when executing the optimizer step. This avoids having to mutate the
state of the optimizer and is therefore more functional. It makes no difference when using the
learner struct, but it will be an essential nuance to grasp if you implement your own training loop.</p>
<p>Once the learner is created, we can simply call <code>fit</code> and provide the training and validation
dataloaders. For the sake of simplicity in this example, we employ the test set as the validation
set; however, we do not recommend this practice for actual usage.</p>
<p>Finally, the trained model is returned by the <code>fit</code> method, and the only remaining task is saving
the trained weights using the <code>CompactRecorder</code>. This recorder employs the <code>MessagePack</code> format with
<code>gzip</code> compression, <code>f16</code> for floats and <code>i16</code> for integers. Other recorders are available, offering
support for various formats, such as <code>BinCode</code> and <code>JSON</code>, with or without compression. Any backend,
regardless of precision, can load recorded data of any kind.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend"><a class="header" href="#backend">Backend</a></h1>
<p>We have effectively written most of the necessary code to train our model. However, we have not
explicitly designated the backend to be used at any point. This will be defined in the main
entrypoint of our program, namely the <code>main</code> function defined in <code>src/main.rs</code>.</p>
<pre><code class="language-rust   ignore">use burn::optim::AdamConfig;
use burn::backend::{Autodiff, Wgpu, wgpu::AutoGraphicsApi};
use guide::model::ModelConfig;

fn main() {
    type MyBackend = Wgpu&lt;AutoGraphicsApi, f32, i32&gt;;
    type MyAutodiffBackend = Autodiff&lt;MyBackend&gt;;

    let device = burn::backend::wgpu::WgpuDevice::default();
    guide::training::train::&lt;MyAutodiffBackend&gt;(
        &quot;/tmp/guide&quot;,
        guide::training::TrainingConfig::new(ModelConfig::new(10, 512), AdamConfig::new()),
        device,
    );
}</code></pre>
<details>
<summary><strong>ðŸ¦€ Packages, Crates and Modules</strong></summary>
<p>You might be wondering why we use the <code>guide</code> prefix to bring the different modules we just
implemented into scope. Instead of including the code in the current guide in a single file, we
separated it into different files which group related code into <em>modules</em>. The <code>guide</code> is simply the
name we gave to our <em>crate</em>, which contains the different files. If you named your project crate
as <code>my-first-burn-model</code>,
you can equivalently replace all usages of <code>guide</code> above with <code>my-first-burn-model</code>. Below is a brief explanation of the
different parts of the Rust module system.</p>
<p>A <strong>package</strong> is a bundle of one or more crates that provides a set of functionality. A package
contains a <code>Cargo.toml</code> file that describes how to build those crates. Burn is a package.</p>
<p>A <strong>crate</strong> is a compilation unit in Rust. It could be a single file, but it is often easier to
split up crates into multiple <em>modules</em> and possibly multiple files. A crate can come in one of two
forms: a binary crate or a library crate. When compiling a crate, the compiler first looks in the
crate root file (usually <code>src/lib.rs</code> for a library crate or <code>src/main.rs</code> for a binary crate). Any
module declared in the crate root file will be inserted in the crate for compilation. For this demo example, we will
define a library crate where all the individual modules (model, data, training, etc.) are listed inside <code>src/lib.rs</code> as
follows:</p>
<pre><code>pub mod data;
pub mod inference;
pub mod model;
pub mod training;
</code></pre>
<p>A <strong>module</strong> lets us organize code within a crate for readability and easy reuse. Modules also allow
us to control the <em>privacy</em> of items. The <code>pub</code> keyword used above, for example, is employed to make a module publicly
available inside the crate.</p>
<p>The entry point of our program is the <code>main</code> function, defined in the <code>examples/guide.rs</code> file. The file structure
for this example is illustrated below:</p>
<pre><code>guide
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ examples
â”‚   â””â”€â”€ guide.rs
â””â”€â”€ src
    â”œâ”€â”€ data.rs
    â”œâ”€â”€ inference.rs
    â”œâ”€â”€ lib.rs
    â”œâ”€â”€ model.rs
    â””â”€â”€ training.rs
</code></pre>
<p>The source for this guide can be found in our
<a href="https://github.com/tracel-ai/burn/tree/main/examples/guide">GitHub repository</a> which can be used to run this basic
workflow example end-to-end.</p>
</details><br>
<p>In this example, we use the <code>Wgpu</code> backend which is compatible with any operating system and will
use the GPU. For other options, see the Burn README. This backend type takes the graphics api, the
float type and the int type as generic arguments that will be used during the training. By leaving
the graphics API as <code>AutoGraphicsApi</code>, it should automatically use an API available on your machine.
The autodiff backend is simply the same backend, wrapped within the <code>Autodiff</code> struct which imparts
differentiability to any backend.</p>
<p>We call the <code>train</code> function defined earlier with a directory for artifacts, the configuration of
the model (the number of digit classes is 10 and the hidden dimension is 512), the optimizer
configuration which in our case will be the default Adam configuration, and the device which can be
obtained from the backend.</p>
<p>When running the example, we can see the training progression through a basic CLI dashboard:</p>
<img title="a title" alt="Alt text" src="basic-workflow/./training-output.png">
<div style="break-before: page; page-break-before: always;"></div><h1 id="inference"><a class="header" href="#inference">Inference</a></h1>
<p>Now that we have trained our model, the next natural step is to use it for inference.</p>
<p>For loading a model primed for inference, it is of course more efficient to directly load the
weights into the model, bypassing the need to initially set arbitrary weights or worse, weights
computed from a Xavier normal initialization only to then promptly replace them with the stored
weights. With that in mind, let's create a new initialization function receiving the record as
input. This new function can be defined alongside the <code>init</code> function for the <code>ModelConfig</code> struct in <code>src/model.rs</code>.</p>
<pre><code class="language-rust   ignore">impl ModelConfig {
    /// Returns the initialized model using the recorded weights.
    pub fn init_with&lt;B: Backend&gt;(&amp;self, record: ModelRecord&lt;B&gt;) -&gt; Model&lt;B&gt; {
        Model {
            conv1: Conv2dConfig::new([1, 8], [3, 3]).init_with(record.conv1),
            conv2: Conv2dConfig::new([8, 16], [3, 3]).init_with(record.conv2),
            pool: AdaptiveAvgPool2dConfig::new([8, 8]).init(),
            activation: ReLU::new(),
            linear1: LinearConfig::new(16 * 8 * 8, self.hidden_size).init_with(record.linear1),
            linear2: LinearConfig::new(self.hidden_size, self.num_classes)
                .init_with(record.linear2),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }
}</code></pre>
<p>It is important to note that the <code>ModelRecord</code> was automatically generated thanks to the <code>Module</code>
trait. It allows us to load the module state without having to deal with fetching the correct type
manually. Everything is validated when loading the model with the record.</p>
<p>Now let's create a simple <code>infer</code> method in a new file <code>src/inference.rs</code> which we will use to load our trained model.</p>
<pre><code class="language-rust   ignore">pub fn infer&lt;B: Backend&gt;(artifact_dir: &amp;str, device: B::Device, item: MNISTItem) {
    let config = TrainingConfig::load(format!(&quot;{artifact_dir}/config.json&quot;))
        .expect(&quot;Config should exist for the model&quot;);
    let record = CompactRecorder::new()
        .load(format!(&quot;{artifact_dir}/model&quot;).into(), &amp;device)
        .expect(&quot;Trained model should exist&quot;);

    let model = config.model.init_with::&lt;B&gt;(record);

    let label = item.label;
    let batcher = MNISTBatcher::new(device);
    let batch = batcher.batch(vec![item]);
    let output = model.forward(batch.images);
    let predicted = output.argmax(1).flatten::&lt;1&gt;(0, 1).into_scalar();

    println!(&quot;Predicted {} Expected {}&quot;, predicted, label);
}</code></pre>
<p>The first step is to load the configuration of the training to fetch the correct model
configuration. Then we can fetch the record using the same recorder as we used during training.
Finally we can init the model with the configuration and the record before sending it to the wanted
device for inference. For simplicity we can use the same batcher used during the training to pass
from a MNISTItem to a tensor.</p>
<p>By running the infer function, you should see the predictions of your model!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h1>
<p>In this short guide, we've introduced you to the fundamental building blocks for getting started
with Burn. While there's still plenty to explore, our goal has been to provide you with the
essential knowledge to kickstart your productivity within the framework.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building-blocks"><a class="header" href="#building-blocks">Building Blocks</a></h1>
<p>In this section, we'll guide you through the core elements that make up Burn. We'll walk you through
the key components that serve as the building blocks of the framework and your future projects.</p>
<p>As you explore Burn, you might notice that we occasionally draw comparisons to PyTorch. We believe
it can provide a smoother learning curve and help you grasp the nuances more effectively.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend-1"><a class="header" href="#backend-1">Backend</a></h1>
<p>Nearly everything in Burn is based on the <code>Backend</code> trait, which enables you to run tensor
operations using different implementations without having to modify your code. While a backend may
not necessarily have autodiff capabilities, the <code>AutodiffBackend</code> trait specifies when autodiff is
needed. This trait not only abstracts operations but also tensor, device, and element types,
providing each backend the flexibility they need. It's worth noting that the trait assumes eager
mode since burn fully supports dynamic graphs. However, we may create another API to assist with
integrating graph-based backends, without requiring any changes to the user's code.</p>
<p>Users are not expected to directly use the backend trait methods, as it is primarily designed with
backend developers in mind rather than Burn users. Therefore, most Burn userland APIs are generic
across backends. This approach helps users discover the API more organically with proper
autocomplete and documentation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensor"><a class="header" href="#tensor">Tensor</a></h1>
<p>As previously explained in the <a href="building-blocks/../basic-workflow/model.html">model section</a>, the Tensor struct has 3
generic arguments: the backend B, the dimensionality D, and the data type.</p>
<pre><code class="language-rust   ignore">Tensor&lt;B, D&gt;           // Float tensor (default)
Tensor&lt;B, D, Float&gt;    // Explicit float tensor
Tensor&lt;B, D, Int&gt;      // Int tensor
Tensor&lt;B, D, Bool&gt;     // Bool tensor</code></pre>
<p>Note that the specific element types used for <code>Float</code>, <code>Int</code>, and <code>Bool</code> tensors are defined by
backend implementations.</p>
<p>Burn Tensors are defined by the number of dimensions D in its declaration as opposed to its shape. The
actual shape of the tensor is inferred from its initialization. For example, a Tensor of size (5,) is initialized as
below:</p>
<pre><code class="language-rust  ignore">// correct: Tensor is 1-Dimensional with 5 elements
let tensor_1 = Tensor::&lt;Backend, 1&gt;::from_floats([1.0, 2.0, 3.0, 4.0, 5.0]);

// incorrect: let tensor_1 = Tensor::&lt;Backend, 5&gt;::from_floats([1.0, 2.0, 3.0, 4.0, 5.0]);
// this will lead to an error and is for creating a 5-D tensor</code></pre>
<h3 id="initialization"><a class="header" href="#initialization">Initialization</a></h3>
<p>Burn Tensors are primarily initialized using the <code>from_data()</code> method which takes the <code>Data</code> struct as input.
The <code>Data</code> struct has two fields: value &amp; shape. To retrieve the data from a tensor, the method <code>.to_data()</code> should be
employed when intending to reuse the tensor afterward. Alternatively, <code>.into_data()</code> is recommended for one-time use.
Let's look at a couple of examples for initializing a tensor from different inputs.</p>
<pre><code class="language-rust  ignore">
// Initialization from a given Backend (Wgpu)
let tensor_1 = Tensor::&lt;Wgpu, 1&gt;::from_data([1.0, 2.0, 3.0]);

// Initialization from a generic Backend
let tensor_2 = Tensor::&lt;Backend, 1&gt;::from_data(Data::from([1.0, 2.0, 3.0]).convert());

// Initialization using from_floats (Recommended for f32 ElementType)
// Will be converted to Data internally. `.convert()` not needed as from_floats() defined for fixed ElementType
let tensor_3 = Tensor::&lt;Backend, 1&gt;::from_floats([1.0, 2.0, 3.0]);

// Initialization of Int Tensor from array slices
let arr: [i32; 6] = [1, 2, 3, 4, 5, 6];
let tensor_4 = Tensor::&lt;Backend, 1, Int&gt;::from_data(Data::from(&amp;arr[0..3]).convert());

// Initialization from a custom type

struct BodyMetrics {
    age: i8,
    height: i16,
    weight: f32
}

let bmi = BodyMetrics{
        age: 25,
        height: 180,
        weight: 80.0
    };
let tensor_5 = Tensor::&lt;Backend, 1&gt;::from_data(Data::from([bmi.age as f32, bmi.height as f32, bmi.weight]).convert());
</code></pre>
<p>The <code>.convert()</code> method for Data struct is called to ensure that the data's primitive type is
consistent across all backends. With <code>.from_floats()</code> method the ElementType is fixed as f32
and therefore no convert operation is required across backends. This operation can also be done at element wise
level as:
<code>let tensor_6 = Tensor::&lt;B, 1, Int&gt;::from_data(Data::from([(item.age as i64).elem()])</code>. The <code>ElementConversion</code> trait
however needs to be imported for the element wise operation.</p>
<h2 id="ownership-and-cloning"><a class="header" href="#ownership-and-cloning">Ownership and Cloning</a></h2>
<p>Almost all Burn operations take ownership of the input tensors. Therefore, reusing a tensor multiple
times will necessitate cloning it. Let's look at an example to understand the ownership rules and cloning better.
Suppose we want to do a simple min-max normalization of an input tensor.</p>
<pre><code class="language-rust  ignore">    let input = Tensor::&lt;Wgpu, 1&gt;::from_floats([1.0, 2.0, 3.0, 4.0]);
    let min = input.min();
    let max = input.max();
    let input = (input - min).div(max - min);</code></pre>
<p>With PyTorch tensors, the above code would work as expected. However, Rust's strict ownership rules will give an error
and prevent using the input tensor after the first <code>.min()</code> operation. The ownership of the input tensor is transferred
to the variable <code>min</code> and the input tensor is no longer available for further operations. Burn Tensors like most
complex primitives do not implement the <code>Copy</code> trait and therefore have to be cloned explicitly. Now let's rewrite
a working example of doing min-max normalization with cloning.</p>
<pre><code class="language-rust  ignore">    let input = Tensor::&lt;Wgpu, 1&gt;::from_floats([1.0, 2.0, 3.0, 4.0]);
    let min = input.clone().min();
    let max = input.clone().max();
    let input = (input.clone() - min.clone()).div(max - min);
    println!(&quot;{:?}&quot;, input.to_data());      // Success: [0.0, 0.33333334, 0.6666667, 1.0]

    // Notice that max, min have been moved in last operation so the below print will give an error.
    // If we want to use them for further operations, they will need to be cloned in similar fashion.
    // println!(&quot;{:?}&quot;, min.to_data());</code></pre>
<p>We don't need to be worried about memory overhead because with cloning, the tensor's buffer isn't copied,
and only a reference to it is increased. This makes it possible to determine exactly how many times a tensor is used,
which is very convenient for reusing tensor buffers or even fusing operations into a single
kernel (<a href="https://burn.dev/docs/burn_fusion/index.htmls">burn-fusion</a>).
For that reason, we don't provide explicit inplace operations. If a tensor is used only one time, inplace operations
will always be used when available.</p>
<h2 id="tensor-operations"><a class="header" href="#tensor-operations">Tensor Operations</a></h2>
<p>Normally with PyTorch, explicit inplace operations aren't supported during the backward pass, making
them useful only for data preprocessing or inference-only model implementations. With Burn, you can
focus more on <em>what</em> the model should do, rather than on <em>how</em> to do it. We take the responsibility
of making your code run as fast as possible during training as well as inference. The same
principles apply to broadcasting; all operations support broadcasting unless specified otherwise.</p>
<p>Here, we provide a list of all supported operations along with their PyTorch equivalents. Note that
for the sake of simplicity, we ignore type signatures. For more details, refer to the
<a href="https://docs.rs/burn/latest/burn/tensor/struct.Tensor.html">full documentation</a>.</p>
<h3 id="basic-operations"><a class="header" href="#basic-operations">Basic Operations</a></h3>
<p>Those operations are available for all tensor kinds: <code>Int</code>, <code>Float</code>, and <code>Bool</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Tensor::empty(shape, device)</code></td><td><code>torch.empty(shape, device=device)</code></td></tr>
<tr><td><code>tensor.dims()</code></td><td><code>tensor.size()</code></td></tr>
<tr><td><code>tensor.shape()</code></td><td><code>tensor.shape</code></td></tr>
<tr><td><code>tensor.reshape(shape)</code></td><td><code>tensor.view(shape)</code></td></tr>
<tr><td><code>tensor.flatten(start_dim, end_dim)</code></td><td><code>tensor.flatten(start_dim, end_dim)</code></td></tr>
<tr><td><code>tensor.squeeze(dim)</code></td><td><code>tensor.squeeze(dim)</code></td></tr>
<tr><td><code>tensor.unsqueeze()</code></td><td><code>tensor.unsqueeze(0)</code></td></tr>
<tr><td><code>tensor.unsqueeze_dim(dim)</code></td><td><code>tensor.unsqueeze(dim)</code></td></tr>
<tr><td><code>tensor.slice(ranges)</code></td><td><code>tensor[(*ranges,)]</code></td></tr>
<tr><td><code>tensor.slice_assign(ranges, values)</code></td><td><code>tensor[(*ranges,)] = values</code></td></tr>
<tr><td><code>tensor.narrow(dim, start, length)</code></td><td><code>tensor.narrow(dim, start, length)</code></td></tr>
<tr><td><code>tensor.chunk(num_chunks, dim)</code></td><td><code>tensor.chunk(num_chunks, dim)</code></td></tr>
<tr><td><code>tensor.device()</code></td><td><code>tensor.device</code></td></tr>
<tr><td><code>tensor.to_device(device)</code></td><td><code>tensor.to(device)</code></td></tr>
<tr><td><code>tensor.repeat(2, 4)</code></td><td><code>tensor.repeat([1, 1, 4])</code></td></tr>
<tr><td><code>tensor.equal(other)</code></td><td><code>x == y</code></td></tr>
<tr><td><code>Tensor::cat(tensors, dim)</code></td><td><code>torch.cat(tensors, dim)</code></td></tr>
<tr><td><code>tensor.into_data()</code></td><td>N/A</td></tr>
<tr><td><code>tensor.to_data()</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::from_data(data, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.into_primitive()</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::from_primitive(primitive)</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::stack(tensors, dim)</code></td><td><code>torch.stack(tensors, dim)</code></td></tr>
</tbody></table>
</div>
<h3 id="numeric-operations"><a class="header" href="#numeric-operations">Numeric Operations</a></h3>
<p>Those operations are available for numeric tensor kinds: <code>Float</code> and <code>Int</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.into_scalar()</code></td><td><code>tensor.item()</code> (for single-element tensors)</td></tr>
<tr><td><code>tensor + other</code> or <code>tensor.add(other)</code></td><td><code>tensor + other</code></td></tr>
<tr><td><code>tensor + scalar</code> or <code>tensor.add_scalar(scalar)</code></td><td><code>tensor + scalar</code></td></tr>
<tr><td><code>tensor - other</code> or <code>tensor.sub(other)</code></td><td><code>tensor - other</code></td></tr>
<tr><td><code>tensor - scalar</code> or <code>tensor.sub_scalar(scalar)</code></td><td><code>tensor - scalar</code></td></tr>
<tr><td><code>tensor / other</code> or <code>tensor.div(other)</code></td><td><code>tensor / other</code></td></tr>
<tr><td><code>tensor / scalar</code> or <code>tensor.div_scalar(scalar)</code></td><td><code>tensor / scalar</code></td></tr>
<tr><td><code>tensor * other</code> or <code>tensor.mul(other)</code></td><td><code>tensor * other</code></td></tr>
<tr><td><code>tensor * scalar</code> or <code>tensor.mul_scalar(scalar)</code></td><td><code>tensor * scalar</code></td></tr>
<tr><td><code>tensor.powf(other)</code>   or <code>tensor.powi(intother)</code></td><td><code>tensor.pow(other)</code></td></tr>
<tr><td><code>tensor.powf_scalar(scalar)</code>  or <code>tensor.powi_scalar(intscalar)</code></td><td><code>tensor.pow(scalar)</code></td></tr>
<tr><td><code>-tensor</code> or <code>tensor.neg()</code></td><td><code>-tensor</code></td></tr>
<tr><td><code>Tensor::zeros(shape)</code></td><td><code>torch.zeros(shape)</code></td></tr>
<tr><td><code>Tensor::zeros(shape, device)</code></td><td><code>torch.zeros(shape, device=device)</code></td></tr>
<tr><td><code>Tensor::ones(shape, device)</code></td><td><code>torch.ones(shape, device=device)</code></td></tr>
<tr><td><code>Tensor::full(shape, fill_value, device)</code></td><td><code>torch.full(shape, fill_value, device=device)</code></td></tr>
<tr><td><code>tensor.mean()</code></td><td><code>tensor.mean()</code></td></tr>
<tr><td><code>tensor.sum()</code></td><td><code>tensor.sum()</code></td></tr>
<tr><td><code>tensor.mean_dim(dim)</code></td><td><code>tensor.mean(dim)</code></td></tr>
<tr><td><code>tensor.sum_dim(dim)</code></td><td><code>tensor.sum(dim)</code></td></tr>
<tr><td><code>tensor.equal_elem(other)</code></td><td><code>tensor.eq(other)</code></td></tr>
<tr><td><code>tensor.greater(other)</code></td><td><code>tensor.gt(other)</code></td></tr>
<tr><td><code>tensor.greater_elem(scalar)</code></td><td><code>tensor.gt(scalar)</code></td></tr>
<tr><td><code>tensor.greater_equal(other)</code></td><td><code>tensor.ge(other)</code></td></tr>
<tr><td><code>tensor.greater_equal_elem(scalar)</code></td><td><code>tensor.ge(scalar)</code></td></tr>
<tr><td><code>tensor.lower(other)</code></td><td><code>tensor.lt(other)</code></td></tr>
<tr><td><code>tensor.lower_elem(scalar)</code></td><td><code>tensor.lt(scalar)</code></td></tr>
<tr><td><code>tensor.lower_equal(other)</code></td><td><code>tensor.le(other)</code></td></tr>
<tr><td><code>tensor.lower_equal_elem(scalar)</code></td><td><code>tensor.le(scalar)</code></td></tr>
<tr><td><code>tensor.mask_where(mask, value_tensor)</code></td><td><code>torch.where(mask, value_tensor, tensor)</code></td></tr>
<tr><td><code>tensor.mask_fill(mask, value)</code></td><td><code>tensor.masked_fill(mask, value)</code></td></tr>
<tr><td><code>tensor.gather(dim, indices)</code></td><td><code>torch.gather(tensor, dim, indices)</code></td></tr>
<tr><td><code>tensor.scatter(dim, indices, values)</code></td><td><code>tensor.scatter_add(dim, indices, values)</code></td></tr>
<tr><td><code>tensor.select(dim, indices)</code></td><td><code>tensor.index_select(dim, indices)</code></td></tr>
<tr><td><code>tensor.select_assign(dim, indices, values)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.argmax(dim)</code></td><td><code>tensor.argmax(dim)</code></td></tr>
<tr><td><code>tensor.max()</code></td><td><code>tensor.max()</code></td></tr>
<tr><td><code>tensor.max_dim(dim)</code></td><td><code>tensor.max(dim)</code></td></tr>
<tr><td><code>tensor.max_dim_with_indices(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.argmin(dim)</code></td><td><code>tensor.argmin(dim)</code></td></tr>
<tr><td><code>tensor.min()</code></td><td><code>tensor.min()</code></td></tr>
<tr><td><code>tensor.min_dim(dim)</code></td><td><code>tensor.min(dim)</code></td></tr>
<tr><td><code>tensor.min_dim_with_indices(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.clamp(min, max)</code></td><td><code>torch.clamp(tensor, min=min, max=max)</code></td></tr>
<tr><td><code>tensor.clamp_min(min)</code></td><td><code>torch.clamp(tensor, min=min)</code></td></tr>
<tr><td><code>tensor.clamp_max(max)</code></td><td><code>torch.clamp(tensor, max=max)</code></td></tr>
<tr><td><code>tensor.abs()</code></td><td><code>torch.abs(tensor)</code></td></tr>
<tr><td><code>tensor.triu(diagonal)</code></td><td><code>torch.triu(tensor, diagonal)</code></td></tr>
<tr><td><code>tensor.tril(diagonal)</code></td><td><code>torch.tril(tensor, diagonal)</code></td></tr>
</tbody></table>
</div>
<h3 id="float-operations"><a class="header" href="#float-operations">Float Operations</a></h3>
<p>Those operations are only available for <code>Float</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.exp()</code></td><td><code>tensor.exp()</code></td></tr>
<tr><td><code>tensor.log()</code></td><td><code>tensor.log()</code></td></tr>
<tr><td><code>tensor.log1p()</code></td><td><code>tensor.log1p()</code></td></tr>
<tr><td><code>tensor.erf()</code></td><td><code>tensor.erf()</code></td></tr>
<tr><td><code>tensor.sqrt()</code></td><td><code>tensor.sqrt()</code></td></tr>
<tr><td><code>tensor.recip()</code></td><td><code>tensor.reciprocal()</code></td></tr>
<tr><td><code>tensor.cos()</code></td><td><code>tensor.cos()</code></td></tr>
<tr><td><code>tensor.sin()</code></td><td><code>tensor.sin()</code></td></tr>
<tr><td><code>tensor.tanh()</code></td><td><code>tensor.tanh()</code></td></tr>
<tr><td><code>tensor.from_floats(floats, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.int()</code></td><td>Similar to <code>tensor.to(torch.long)</code></td></tr>
<tr><td><code>tensor.zeros_like()</code></td><td><code>torch.zeros_like(tensor)</code></td></tr>
<tr><td><code>tensor.ones_like()</code></td><td><code>torch.ones_like(tensor)</code></td></tr>
<tr><td><code>tensor.random_like(distribution)</code></td><td><code>torch.rand_like()</code> only uniform</td></tr>
<tr><td><code>tensor.one_hot(index, num_classes, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.transpose()</code></td><td><code>tensor.T</code></td></tr>
<tr><td><code>tensor.swap_dims(dim1, dim2)</code></td><td><code>tensor.transpose(dim1, dim2)</code></td></tr>
<tr><td><code>tensor.matmul(other)</code></td><td><code>tensor.matmul(other)</code></td></tr>
<tr><td><code>tensor.var(dim)</code></td><td><code>tensor.var(dim)</code></td></tr>
<tr><td><code>tensor.var_bias(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.var_mean(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.var_mean_bias(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.random(shape, distribution, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.to_full_precision()</code></td><td><code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.from_full_precision(tensor)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<h1 id="int-operations"><a class="header" href="#int-operations">Int Operations</a></h1>
<p>Those operations are only available for <code>Int</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.from_ints(ints)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.float()</code></td><td>Similar to <code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.arange(5..10, device)       </code></td><td><code>tensor.arange(start=5, end=10, device=device)</code></td></tr>
<tr><td><code>tensor.arange_step(5..10, 2, device)</code></td><td><code>tensor.arange(start=5, end=10, step=2, device=device)</code></td></tr>
</tbody></table>
</div>
<h1 id="bool-operations"><a class="header" href="#bool-operations">Bool Operations</a></h1>
<p>Those operations are only available for <code>Bool</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.float()</code></td><td>Similar to <code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.int()</code></td><td>Similar to <code>tensor.to(torch.long)</code></td></tr>
<tr><td><code>tensor.not()</code></td><td><code>tensor.logical_not()</code></td></tr>
</tbody></table>
</div>
<h2 id="activation-functions"><a class="header" href="#activation-functions">Activation Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>activation::gelu(tensor)</code></td><td>Similar to <code>nn.functional.gelu(tensor)</code></td></tr>
<tr><td><code>activation::log_sigmoid(tensor)</code></td><td>Similar to <code>nn.functional.log_sigmoid(tensor)</code></td></tr>
<tr><td><code>activation::log_softmax(tensor, dim)</code></td><td>Similar to <code>nn.functional.log_softmax(tensor, dim)</code></td></tr>
<tr><td><code>activation::mish(tensor)</code></td><td>Similar to <code>nn.functional.mish(tensor)</code></td></tr>
<tr><td><code>activation::quiet_softmax(tensor, dim)</code></td><td>Similar to <code>nn.functional.quiet_softmax(tensor, dim)</code></td></tr>
<tr><td><code>activation::relu(tensor)</code></td><td>Similar to <code>nn.functional.relu(tensor)</code></td></tr>
<tr><td><code>activation::sigmoid(tensor)</code></td><td>Similar to <code>nn.functional.sigmoid(tensor)</code></td></tr>
<tr><td><code>activation::silu(tensor)</code></td><td>Similar to <code>nn.functional.silu(tensor)</code></td></tr>
<tr><td><code>activation::softmax(tensor, dim)</code></td><td>Similar to <code>nn.functional.softmax(tensor, dim)</code></td></tr>
<tr><td><code>activation::softplus(tensor, beta)</code></td><td>Similar to <code>nn.functional.softplus(tensor, beta)</code></td></tr>
<tr><td><code>activation::tanh(tensor)</code></td><td>Similar to <code>nn.functional.tanh(tensor)</code></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="autodiff"><a class="header" href="#autodiff">Autodiff</a></h1>
<p>Burn's tensor also supports autodifferentiation, which is an essential part of any deep learning
framework. We introduced the <code>Backend</code> trait in the <a href="building-blocks/./backend.html">previous section</a>, but Burn also
has another trait for autodiff: <code>AutodiffBackend</code>.</p>
<p>However, not all tensors support auto-differentiation; you need a backend that implements both the
<code>Backend</code> and <code>AutodiffBackend</code> traits. Fortunately, you can add autodifferentiation capabilities to any
backend using a backend decorator: <code>type MyAutodiffBackend = Autodiff&lt;MyBackend&gt;</code>. This
decorator implements both the <code>AutodiffBackend</code> and <code>Backend</code> traits by maintaining a dynamic
computational graph and utilizing the inner backend to execute tensor operations.</p>
<p>The <code>AutodiffBackend</code> trait adds new operations on float tensors that can't be called otherwise. It also
provides a new associated type, <code>B::Gradients</code>, where each calculated gradient resides.</p>
<pre><code class="language-rust  ignore">fn calculate_gradients&lt;B: AutodiffBackend&gt;(tensor: Tensor&lt;B, 2&gt;) -&gt; B::Gradients {
    let mut gradients = tensor.clone().backward();

    let tensor_grad = tensor.grad(&amp;gradients);        // get
    let tensor_grad = tensor.grad_remove(&amp;mut gradients); // pop

    gradients
}</code></pre>
<p>Note that some functions will always be available even if the backend doesn't implement the
<code>AutodiffBackend</code> trait. In such cases, those functions will do nothing.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.detach()</code></td><td><code>tensor.detach()</code></td></tr>
<tr><td><code>tensor.require_grad()</code></td><td><code>tensor.requires_grad()</code></td></tr>
<tr><td><code>tensor.is_require_grad()</code></td><td><code>tensor.requires_grad</code></td></tr>
<tr><td><code>tensor.set_require_grad(require_grad)</code></td><td><code>tensor.requires_grad(False)</code></td></tr>
</tbody></table>
</div>
<p>However, you're unlikely to make any mistakes since you can't call <code>backward</code> on a tensor that is on
a backend that doesn't implement <code>AutodiffBackend</code>. Additionally, you can't retrieve the gradient of a
tensor without an autodiff backend.</p>
<h2 id="difference-with-pytorch"><a class="header" href="#difference-with-pytorch">Difference with PyTorch</a></h2>
<p>The way Burn handles gradients is different from PyTorch. First, when calling <code>backward</code>, each
parameter doesn't have its <code>grad</code> field updated. Instead, the backward pass returns all the
calculated gradients in a container. This approach offers numerous benefits, such as the ability to
easily send gradients to other threads.</p>
<p>You can also retrieve the gradient for a specific parameter using the <code>grad</code> method on a tensor.
Since this method takes the gradients as input, it's hard to forget to call <code>backward</code> beforehand.
Note that sometimes, using <code>grad_remove</code> can improve performance by allowing inplace operations.</p>
<p>In PyTorch, when you don't need gradients for inference or validation, you typically need to scope
your code using a block.</p>
<pre><code class="language-python"># Inference mode
torch.inference():
   # your code
   ...

# Or no grad
torch.no_grad():
   # your code
   ...
</code></pre>
<p>With Burn, you don't need to wrap the backend with the <code>Autodiff</code> for inference, and you
can call <code>inner()</code> to obtain the inner tensor, which is useful for validation.</p>
<pre><code class="language-rust  ignore">/// Use `B: AutodiffBackend`
fn example_validation&lt;B: AutodiffBackend&gt;(tensor: Tensor&lt;B, 2&gt;) {
    let inner_tensor: Tensor&lt;B::InnerBackend, 2&gt; = tensor.inner();
    let _ = inner_tensor + 5;
}

/// Use `B: Backend`
fn example_inference&lt;B: Backend&gt;(tensor: Tensor&lt;B, 2&gt;) {
    let _ = tensor + 5;
    ...
}</code></pre>
<p><strong>Gradients with Optimizers</strong></p>
<p>We've seen how gradients can be used with tensors, but the process is a bit different when working
with optimizers from <code>burn-core</code>. To work with the <code>Module</code> trait, a translation step is required to
link tensor parameters with their gradients. This step is necessary to easily support gradient
accumulation and training on multiple devices, where each module can be forked and run on different
devices in parallel. We'll explore deeper into this topic in the <a href="building-blocks/./module.html">Module</a> section.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="module"><a class="header" href="#module">Module</a></h1>
<p>The <code>Module</code> derive allows you to create your own neural network modules, similar to PyTorch. The
derive function only generates the necessary methods to essentially act as a parameter container for
your type, it makes no assumptions about how the forward pass is declared.</p>
<pre><code class="language-rust  ignore">use burn::nn;
use burn::module::Module;
use burn::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&lt;B: Backend&gt; {
    linear_inner: Linear&lt;B&gt;,
    linear_outer: Linear&lt;B&gt;,
    dropout: Dropout,
    gelu: GELU,
}

impl&lt;B: Backend&gt; PositionWiseFeedForward&lt;B&gt; {
    /// Normal method added to a struct.
    pub fn forward&lt;const D: usize&gt;(&amp;self, input: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt; {
        let x = self.linear_inner.forward(input);
        let x = self.gelu.forward(x);
        let x = self.dropout.forward(x);

        self.linear_outer.forward(x)
    }
}</code></pre>
<p>Note that all fields declared in the struct must also implement the <code>Module</code> trait.</p>
<h2 id="tensor-1"><a class="header" href="#tensor-1">Tensor</a></h2>
<p>If you want to create your own module that contains tensors, and not just other modules defined with
the <code>Module</code> derive, you need to be careful to achieve the behavior you want.</p>
<ul>
<li>
<p><code>Param&lt;Tensor&lt;B, D&gt;&gt;</code>: If you want the tensor to be included as a parameter of your modules, you
need to wrap the tensor in a <code>Param</code> struct. This will create an ID that will be used to identify
this parameter. This is essential when performing module optimization and when saving states such
as optimizer and module checkpoints. Note that a module's record only contains parameters.</p>
</li>
<li>
<p><code>Param&lt;Tensor&lt;B, D&gt;&gt;.set_require_grad(false)</code>: If you want the tensor to be included as a
parameter of your modules, and therefore saved with the module's weights, but you don't want it to
be updated by the optimizer.</p>
</li>
<li>
<p><code>Tensor&lt;B, D&gt;</code>: If you want the tensor to act as a constant that can be recreated when
instantiating a module. This can be useful when generating sinusoidal embeddings, for example.</p>
</li>
</ul>
<h2 id="methods"><a class="header" href="#methods">Methods</a></h2>
<p>These methods are available for all modules.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>module.devices()</code></td><td>N/A</td></tr>
<tr><td><code>module.fork(device)</code></td><td>Similar to <code>module.to(device).detach()</code></td></tr>
<tr><td><code>module.to_device(device)</code></td><td><code>module.to(device)</code></td></tr>
<tr><td><code>module.no_grad()</code></td><td><code>module.require_grad_(False)</code></td></tr>
<tr><td><code>module.num_params()</code></td><td>N/A</td></tr>
<tr><td><code>module.visit(visitor)</code></td><td>N/A</td></tr>
<tr><td><code>module.map(mapper)</code></td><td>N/A</td></tr>
<tr><td><code>module.into_record()</code></td><td>Similar to <code>state_dict</code></td></tr>
<tr><td><code>module.load_record(record)</code></td><td>Similar to <code>load_state_dict(state_dict)</code></td></tr>
<tr><td><code>module.save_file(file_path, recorder)</code></td><td>N/A</td></tr>
<tr><td><code>module.load_file(file_path, recorder)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<p>Similar to the backend trait, there is also the <code>AutodiffModule</code> trait to signify a module with
autodiff support.</p>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>module.valid()</code></td><td><code>module.eval()</code></td></tr>
</tbody></table>
</div>
<h2 id="visitor--mapper"><a class="header" href="#visitor--mapper">Visitor &amp; Mapper</a></h2>
<p>As mentioned earlier, modules primarily function as parameter containers. Therefore, we naturally
offer several ways to perform functions on each parameter. This is distinct from PyTorch, where
extending module functionalities is not as straightforward.</p>
<p>The <code>map</code> and <code>visitor</code> methods are quite similar but serve different purposes. Mapping is used for
potentially mutable operations where each parameter of a module can be updated to a new value. In
Burn, optimizers are essentially just sophisticated module mappers. Visitors, on the other hand, are
used when you don't intend to modify the module but need to retrieve specific information from it,
such as the number of parameters or a list of devices in use.</p>
<p>You can implement your own mapper or visitor by implementing these simple traits:</p>
<pre><code class="language-rust  ignore">/// Module visitor trait.
pub trait ModuleVisitor&lt;B: Backend&gt; {
    /// Visit a tensor in the module.
    fn visit&lt;const D: usize&gt;(&amp;mut self, id: &amp;ParamId, tensor: &amp;Tensor&lt;B, D&gt;);
}

/// Module mapper trait.
pub trait ModuleMapper&lt;B: Backend&gt; {
    /// Map a tensor in the module.
    fn map&lt;const D: usize&gt;(&amp;mut self, id: &amp;ParamId, tensor: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt;;
}</code></pre>
<h2 id="built-in-modules"><a class="header" href="#built-in-modules">Built-in Modules</a></h2>
<p>Burn comes with built-in modules that you can use to build your own modules.</p>
<h3 id="general"><a class="header" href="#general">General</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>BatchNorm</code></td><td><code>nn.BatchNorm1d</code>, <code>nn.BatchNorm2d</code> etc.</td></tr>
<tr><td><code>LayerNorm</code></td><td><code>nn.LayerNorm</code></td></tr>
<tr><td><code>GroupNorm</code></td><td><code>nn.GroupNorm</code></td></tr>
<tr><td><code>Dropout</code></td><td><code>nn.Dropout</code></td></tr>
<tr><td><code>GELU</code></td><td><code>nn.GELU</code></td></tr>
<tr><td><code>Linear</code></td><td><code>nn.Linear</code></td></tr>
<tr><td><code>Embedding</code></td><td><code>nn.Embedding</code></td></tr>
<tr><td><code>Relu</code></td><td><code>nn.ReLU</code></td></tr>
</tbody></table>
</div>
<h3 id="convolutions"><a class="header" href="#convolutions">Convolutions</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Conv1d</code></td><td><code>nn.Conv1d</code></td></tr>
<tr><td><code>Conv2d</code></td><td><code>nn.Conv2d</code></td></tr>
<tr><td><code>ConvTranspose1d</code></td><td><code>nn.ConvTranspose1d</code></td></tr>
<tr><td><code>ConvTranspose2d</code></td><td><code>nn.ConvTranspose2d</code></td></tr>
</tbody></table>
</div>
<h3 id="pooling"><a class="header" href="#pooling">Pooling</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>AdaptiveAvgPool1d</code></td><td><code>nn.AdaptiveAvgPool1d</code></td></tr>
<tr><td><code>AdaptiveAvgPool2d</code></td><td><code>nn.AdaptiveAvgPool2d</code></td></tr>
<tr><td><code>AvgPool1d</code></td><td><code>nn.AvgPool1d</code></td></tr>
<tr><td><code>AvgPool2d</code></td><td><code>nn.AvgPool2d</code></td></tr>
<tr><td><code>MaxPool1d</code></td><td><code>nn.MaxPool1d</code></td></tr>
<tr><td><code>MaxPool2d</code></td><td><code>nn.MaxPool2d</code></td></tr>
</tbody></table>
</div>
<h3 id="rnns"><a class="header" href="#rnns">RNNs</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Gru</code></td><td><code>nn.GRU</code></td></tr>
<tr><td><code>Lstm</code></td><td><code>nn.LSTM</code></td></tr>
<tr><td><code>GateController</code></td><td><em>No direct equivalent</em></td></tr>
</tbody></table>
</div>
<h3 id="transformer"><a class="header" href="#transformer">Transformer</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>MultiHeadAttention</code></td><td><code>nn.MultiheadAttention</code></td></tr>
<tr><td><code>TransformerDecoder</code></td><td><code>nn.TransformerDecoder</code></td></tr>
<tr><td><code>TransformerEncoder</code></td><td><code>nn.TransformerEncoder</code></td></tr>
<tr><td><code>PositionalEncoding</code></td><td><em>No direct equivalent</em></td></tr>
</tbody></table>
</div>
<h3 id="loss"><a class="header" href="#loss">Loss</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Burn API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>CrossEntropyLoss</code></td><td><code>nn.CrossEntropyLoss</code></td></tr>
<tr><td><code>MSELoss</code></td><td><code>nn.MSELoss</code></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="learner"><a class="header" href="#learner">Learner</a></h1>
<p>The <a href="https://github.com/tracel-ai/burn/tree/main/burn-train">burn-train</a> crate encapsulates multiple
utilities for training deep learning models. The goal of the crate is to provide users with a
well-crafted and flexible training loop, so that projects do not have to write such components from
the ground up. Most of the interactions with <code>burn-train</code> will be with the <code>LearnerBuilder</code> struct,
briefly presented in the previous <a href="building-blocks/../basic-workflow/training.html">training section</a>. This struct
enables you to configure the training loop, offering support for registering metrics, enabling
logging, checkpointing states, using multiple devices, and so on.</p>
<p>There are still some assumptions in the current provided APIs, which may make them inappropriate for
your learning requirements. Indeed, they assume your model will learn from a training dataset and be
validated against another dataset. This is the most common paradigm, allowing users to do both
supervised and unsupervised learning as well as fine-tuning. However, for more complex requirements,
creating a <a href="building-blocks/../custom-training-loop.html">custom training loop</a> might be what you need.</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>The learner builder provides numerous options when it comes to configurations.</p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Description</th></tr></thead><tbody>
<tr><td>Training Metric</td><td>Register a training metric</td></tr>
<tr><td>Validation Metric</td><td>Register a validation metric</td></tr>
<tr><td>Training Metric Plot</td><td>Register a training metric with plotting (requires the metric to be numeric)</td></tr>
<tr><td>Validation Metric Plot</td><td>Register a validation metric with plotting (requires the metric to be numeric)</td></tr>
<tr><td>Metric Logger</td><td>Configure the metric loggers (default is saving them to files)</td></tr>
<tr><td>Renderer</td><td>Configure how to render metrics (default is CLI)</td></tr>
<tr><td>Grad Accumulation</td><td>Configure the number of steps before applying gradients</td></tr>
<tr><td>File Checkpointer</td><td>Configure how the model, optimizer and scheduler states are saved</td></tr>
<tr><td>Num Epochs</td><td>Set the number of epochs.</td></tr>
<tr><td>Devices</td><td>Set the devices to be used</td></tr>
<tr><td>Checkpoint</td><td>Restart training from a checkpoint</td></tr>
</tbody></table>
</div>
<p>When the builder is configured at your liking, you can then move forward to build the learner. The
build method requires three inputs: the model, the optimizer and the learning rate scheduler. Note
that the latter can be a simple float if you want it to be constant during training.</p>
<p>The result will be a newly created Learner struct, which has only one method, the <code>fit</code> function
which must be called with the training and validation dataloaders. This will start the training and
return the trained model once finished.</p>
<p>Again, please refer to the <a href="building-blocks/../basic-workflow/training.html">training section</a> for a relevant code
snippet.</p>
<h2 id="artifacts"><a class="header" href="#artifacts">Artifacts</a></h2>
<p>When creating a new builder, all the collected data will be saved under the directory provided as
the argument to the <code>new</code> method. Here is an example of the data layout for a model recorded using
the compressed message pack format, with the accuracy and loss metrics registered:</p>
<pre><code>â”œâ”€â”€ experiment.log
â”œâ”€â”€ checkpoint
â”‚Â Â  â”œâ”€â”€ model-1.mpk.gz
â”‚Â Â  â”œâ”€â”€ optim-1.mpk.gz
â”‚Â Â  â””â”€â”€ scheduler-1.mpk.gz
â”‚Â Â  â”œâ”€â”€ model-2.mpk.gz
â”‚Â Â  â”œâ”€â”€ optim-2.mpk.gz
â”‚Â Â  â””â”€â”€ scheduler-2.mpk.gz
â”œâ”€â”€ train
â”‚Â Â  â”œâ”€â”€ epoch-1
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Accuracy.log
â”‚Â Â  â”‚Â Â  â””â”€â”€ Loss.log
â”‚Â Â  â””â”€â”€ epoch-2
â”‚Â Â      â”œâ”€â”€ Accuracy.log
â”‚Â Â      â””â”€â”€ Loss.log
â””â”€â”€ valid
    â”œâ”€â”€ epoch-1
    â”‚Â Â  â”œâ”€â”€ Accuracy.log
    â”‚Â Â  â””â”€â”€ Loss.log
    â””â”€â”€ epoch-2
        â”œâ”€â”€ Accuracy.log
        â””â”€â”€ Loss.log
</code></pre>
<p>You can choose to save or synchronize that local directory with a remote file system, if desired.
The file checkpointer is capable of automatically deleting old checkpoints according to a specified
configuration.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metric"><a class="header" href="#metric">Metric</a></h1>
<p>When working with the learner, you have the option to record metrics that will be monitored
throughout the training process. We currently offer a restricted range of metrics.</p>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Description</th></tr></thead><tbody>
<tr><td>Accuracy</td><td>Calculate the accuracy in percentage</td></tr>
<tr><td>Loss</td><td>Output the loss used for the backward pass</td></tr>
<tr><td>CPU Temperature</td><td>Fetch the temperature of CPUs</td></tr>
<tr><td>CPU Usage</td><td>Fetch the CPU utilization</td></tr>
<tr><td>CPU Memory Usage</td><td>Fetch the CPU RAM usage</td></tr>
<tr><td>GPU Temperature</td><td>Fetch the GPU temperature</td></tr>
<tr><td>Learning Rate</td><td>Fetch the current learning rate for each optimizer step</td></tr>
<tr><td>CUDA</td><td>Fetch general CUDA metrics such as utilization</td></tr>
</tbody></table>
</div>
<p>In order to use a metric, the output of your training step has to implement the <code>Adaptor</code> trait from
<code>burn-train::metric</code>. Here is an example for the classification output, already provided with the
crate.</p>
<pre><code class="language-rust   ignore">/// Simple classification output adapted for multiple metrics.
#[derive(new)]
pub struct ClassificationOutput&lt;B: Backend&gt; {
    /// The loss.
    pub loss: Tensor&lt;B, 1&gt;,

    /// The output.
    pub output: Tensor&lt;B, 2&gt;,

    /// The targets.
    pub targets: Tensor&lt;B, 1, Int&gt;,
}

impl&lt;B: Backend&gt; Adaptor&lt;AccuracyInput&lt;B&gt;&gt; for ClassificationOutput&lt;B&gt; {
    fn adapt(&amp;self) -&gt; AccuracyInput&lt;B&gt; {
        AccuracyInput::new(self.output.clone(), self.targets.clone())
    }
}

impl&lt;B: Backend&gt; Adaptor&lt;LossInput&lt;B&gt;&gt; for ClassificationOutput&lt;B&gt; {
    fn adapt(&amp;self) -&gt; LossInput&lt;B&gt; {
        LossInput::new(self.loss.clone())
    }
}</code></pre>
<h1 id="custom-metric"><a class="header" href="#custom-metric">Custom Metric</a></h1>
<p>Generating your own custom metrics is done by implementing the <code>Metric</code> trait.</p>
<pre><code class="language-rust   ignore">/// Metric trait.
///
/// Implementations should define their own input type only used by the metric.
/// This is important since some conflict may happen when the model output is adapted for each
/// metric's input type.
///
/// The only exception is for metrics that don't need any input, setting the associated type
/// to the null type `()`.
pub trait Metric: Send + Sync {
    /// The input type of the metric.
    type Input;

    /// Updates the metric state and returns the current metric entry.
    fn update(&amp;mut self, item: &amp;Self::Input, metadata: &amp;MetricMetadata) -&gt; MetricEntry;
    /// Clear the metric state.
    fn clear(&amp;mut self);
}</code></pre>
<p>As an example, let's see how the loss metric is implemented.</p>
<pre><code class="language-rust  ignore">/// The loss metric.
#[derive(Default)]
pub struct LossMetric&lt;B: Backend&gt; {
    state: NumericMetricState,
    _b: B,
}

/// The loss metric input type.
#[derive(new)]
pub struct LossInput&lt;B: Backend&gt; {
    tensor: Tensor&lt;B, 1&gt;,
}

impl&lt;B: Backend&gt; Metric for LossMetric&lt;B&gt; {
    type Input = LossInput&lt;B&gt;;

    fn update(&amp;mut self, loss: &amp;Self::Input, _metadata: &amp;MetricMetadata) -&gt; MetricEntry {
        let loss = f64::from_elem(loss.tensor.clone().mean().into_data().value[0]);

        self.state
            .update(loss, 1, FormatOptions::new(&quot;Loss&quot;).precision(2))
    }

    fn clear(&amp;mut self) {
        self.state.reset()
    }
}</code></pre>
<p>When the metric you are implementing is numeric in nature, you may want to also implement the
<code>Numeric</code> trait. This will allow your metric to be plotted.</p>
<pre><code class="language-rust  ignore">impl&lt;B: Backend&gt; Numeric for LossMetric&lt;B&gt; {
    fn value(&amp;self) -&gt; f64 {
        self.state.value()
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="config"><a class="header" href="#config">Config</a></h1>
<p>When writing scientific code, you normally have a lot of values that are set, and Deep Learning is
no exception. Python has the possibility to define default parameters for functions, which helps
improve the developer experience. However, this has the downside of potentially breaking your code
when upgrading to a new version, as the default values might change without your knowledge, making
debugging very challenging.</p>
<p>With that in mind, we came up with the Config system. It's a simple Rust derive that you can apply
to your types, allowing you to define default values with ease. Additionally, all configs can be
serialized, reducing potential bugs when upgrading versions and improving reproducibility.</p>
<pre><code class="language-rust   ignore">#[derive(Config)]
use burn::config::Config;

#[derive(Config)]
pub struct MyModuleConfig {
    d_model: usize,
    d_ff: usize,
    #[config(default = 0.1)]
    dropout: f64,
}</code></pre>
<p>The derive also adds useful <code>with_</code> methods for every attribute of your config, similar to a builder
pattern, along with a <code>save</code> method.</p>
<pre><code class="language-rust  ignore">fn main() {
    let config = MyModuleConfig::new(512, 2048);
    println!(&quot;{}&quot;, config.d_model); // 512
    println!(&quot;{}&quot;, config.d_ff); // 2048
    println!(&quot;{}&quot;, config.dropout); // 0.1
    let config =  config.with_dropout(0.2);
    println!(&quot;{}&quot;, config.dropout); // 0.2

    config.save(&quot;config.json&quot;).unwrap();
}</code></pre>
<h2 id="good-practices"><a class="header" href="#good-practices">Good practices</a></h2>
<p>By using the Config pattern it is easy to create instances from this
config. Therefore, initialization methods should be implemented on the config struct.</p>
<pre><code class="language-rust  ignore">impl MyModuleConfig {
    /// Create a module with random weights.
    pub fn init&lt;B: Backend&gt;(&amp;self, device: &amp;B::Device) -&gt; MyModule {
        MyModule {
            linear: LinearConfig::new(self.d_model, self.d_ff).init(device),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }

    /// Create a module with a record, for inference and fine-tuning.
    pub fn init_with(&amp;self, record: MyModuleRecord&lt;B&gt;) -&gt; MyModule {
        MyModule {
            linear: LinearConfig::new(
                self.d_model,
                self.d_ff,
            ).init_with(record.linear),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }
}</code></pre>
<p>Then we could add this line to the above <code>main</code>:</p>
<pre><code class="language-rust  ignore">use burn::backend::Wgpu;
let device = Default::default();
let my_module = config.init::&lt;Wgpu&gt;(&amp;device);</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="record"><a class="header" href="#record">Record</a></h1>
<p>Records are how states are saved with Burn. Compared to most other frameworks, Burn has its own
advanced saving mechanism that allows interoperability between backends with minimal possible
runtime errors. There are multiple reasons why Burn decided to create its own saving formats.</p>
<p>First, Rust has <a href="https://serde.rs/">serde</a>, which is an extremely well-developed serialization and
deserialization library that also powers the <code>safetensors</code> format developed by Hugging Face. If used
properly, all the validations are done when deserializing, which removes the need to write
validation code. Since modules in Burn are created with configurations, they can't implement
serialization and deserialization. That's why the record system was created: allowing you to save
the state of modules independently of the backend in use extremely fast while still giving you all
the flexibility possible to include any non-serializable field within your module.</p>
<p><strong>Why not use safetensors?</strong></p>
<p><a href="https://github.com/huggingface/safetensors"><code>safetensors</code></a> uses serde with the JSON file format and
only supports serializing and deserializing tensors. The record system in Burn gives you the
possibility to serialize any type, which is very useful for optimizers that save their state, but
also for any non-standard, cutting-edge modeling needs you may have. Additionally, the record system
performs automatic precision conversion by using Rust types, making it more reliable with fewer
manual manipulations.</p>
<p>It is important to note that the <code>safetensors</code> format uses the word <em>safe</em> to distinguish itself
from Pickle, which is vulnerable to Python code injection. On our end, the simple fact that we use
Rust already ensures that no code injection is possible. If your storage mechanism doesn't handle
data corruption, you might prefer a recorder that performs checksum validation (i.e., any recorder
with Gzip compression).</p>
<h2 id="recorder"><a class="header" href="#recorder">Recorder</a></h2>
<p>Recorders are independent of the backend and serialize records with precision and a format. Note
that the format can also be in-memory, allowing you to save the records directly into bytes.</p>
<div class="table-wrapper"><table><thead><tr><th>Recorder</th><th>Format</th><th>Compression</th></tr></thead><tbody>
<tr><td>DefaultFileRecorder</td><td>File - Named Message Park</td><td>None</td></tr>
<tr><td>NamedMpkFileRecorder</td><td>File - Named Message Park</td><td>None</td></tr>
<tr><td>NamedMpkGzFileRecorder</td><td>File - Named Message Park</td><td>Gzip</td></tr>
<tr><td>BinFileRecorder</td><td>File - Binary</td><td>None</td></tr>
<tr><td>BinGzFileRecorder</td><td>File - Binary</td><td>Gzip</td></tr>
<tr><td>JsonGzFileRecorder</td><td>File - Json</td><td>Gzip</td></tr>
<tr><td>PrettyJsonFileRecorder</td><td>File - Pretty Json</td><td>Gzip</td></tr>
<tr><td>BinBytesRecorder</td><td>In Memory - Binary</td><td>None</td></tr>
</tbody></table>
</div>
<p>Each recorder supports precision settings decoupled from the precision used for training or
inference. These settings allow you to define the floating-point and integer types that will be used
for serialization and deserialization.</p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Float Precision</th><th>Integer Precision</th></tr></thead><tbody>
<tr><td><code>DoublePrecisionSettings</code></td><td><code>f64</code></td><td><code>i64</code></td></tr>
<tr><td><code>FullPrecisionSettings</code></td><td><code>f32</code></td><td><code>i32</code></td></tr>
<tr><td><code>HalfPrecisionSettings</code></td><td><code>f16</code></td><td><code>i16</code></td></tr>
</tbody></table>
</div>
<p>Note that when loading a record into a module, the type conversion is automatically handled, so you
can't encounter errors. The only crucial aspect is using the same recorder for both serialization
and deserialization; otherwise, you will encounter loading errors.</p>
<p><strong>Which recorder should you use?</strong></p>
<ul>
<li>If you want fast serialization and deserialization, choose a recorder without compression. The one
with the lowest file size without compression is the binary format; otherwise, the named message
park could be used.</li>
<li>If you want to save models for storage, you can use compression, but avoid using the binary
format, as it may not be backward compatible.</li>
<li>If you want to debug your model's weights, you can use the pretty JSON format.</li>
<li>If you want to deploy with <code>no-std</code>, use the in-memory binary format and include the bytes with
the compiled code.</li>
</ul>
<p>For examples on saving and loading records, take a look at
<a href="building-blocks/../saving-and-loading.html">Saving and Loading Models</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dataset"><a class="header" href="#dataset">Dataset</a></h1>
<p>Most deep learning training being done on datasets â€“with perhaps the exception of reinforcement learningâ€“, it is
essential to provide a convenient and performant API.
The dataset trait is quite similar to the dataset abstract class in PyTorch:</p>
<pre><code class="language-rust  ignore">pub trait Dataset&lt;I&gt;: Send + Sync {
    fn get(&amp;self, index: usize) -&gt; Option&lt;I&gt;;
    fn len(&amp;self) -&gt; usize;
}</code></pre>
<p>The dataset trait assumes a fixed-length set of items that can be randomly accessed in constant
time. This is a major difference from datasets that use Apache Arrow underneath to improve streaming
performance. Datasets in Burn don't assume <em>how</em> they are going to be accessed; it's just a
collection of items.</p>
<p>However, you can compose multiple dataset transformations to lazily obtain what you want with zero
pre-processing, so that your training can start instantly!</p>
<h2 id="transformation"><a class="header" href="#transformation">Transformation</a></h2>
<p>Transformations in Burn are all lazy and modify one or multiple input datasets. The goal of these
transformations is to provide you with the necessary tools so that you can model complex data
distributions.</p>
<div class="table-wrapper"><table><thead><tr><th>Transformation</th><th>Description</th></tr></thead><tbody>
<tr><td><code>SamplerDataset</code></td><td>Samples items from a dataset. This is a convenient way to model a dataset as a probability distribution of a fixed size.</td></tr>
<tr><td><code>ShuffledDataset</code></td><td>Maps each input index to a random index, similar to a dataset sampled without replacement.</td></tr>
<tr><td><code>PartialDataset</code></td><td>Returns a view of the input dataset with a specified range.</td></tr>
<tr><td><code>MapperDataset</code></td><td>Computes a transformation lazily on the input dataset.</td></tr>
<tr><td><code>ComposedDataset</code></td><td>Composes multiple datasets together to create a larger one without copying any data.</td></tr>
</tbody></table>
</div>
<p>Let us look at the basic usages of each dataset transform and how they can be composed together. These transforms
are lazy by default except when specified, reducing the need for unnecessary intermediate allocations and improving
performance. The full documentation of each transform can be found at
the <a href="https://burn.dev/docs/burn/data/dataset/transform/index.html">API reference</a>.</p>
<ul>
<li><strong>SamplerDataset</strong>: This transform can be used to sample items from a dataset with (default) or without replacement.
Transform is initialized with a sampling size which can be bigger or smaller than the input dataset size. This is
particularly useful in cases where we want to checkpoint larger datasets more often during training
and smaller datasets less often as the size of an epoch is now controlled by the sampling size. Sample usage:</li>
</ul>
<pre><code class="language-rust  ignore">type DbPedia = SqliteDataset&lt;DbPediaItem&gt;;
let dataset: DbPedia = HuggingfaceDatasetLoader::new(&quot;dbpedia_14&quot;)
        .dataset(&quot;train&quot;).
        .unwrap();
                
let dataset = SamplerDataset&lt;DbPedia, DbPediaItem&gt;::new(dataset, 10000);</code></pre>
<ul>
<li><strong>ShuffledDataset</strong>: This transform can be used to shuffle the items of a dataset. Particularly useful before
splitting
the raw dataset into train/test splits. Can be initialized with a seed to ensure reproducibility.</li>
</ul>
<pre><code class="language-rust  ignore">let dataset = ShuffledDataset&lt;DbPedia, DbPediaItem&gt;::with_seed(dataset, 42);</code></pre>
<ul>
<li><strong>PartialDataset</strong>: This transform is useful to return a view of the dataset with specified start and end indices.
Used
to create train/val/test splits. In the example below, we show how to chain ShuffledDataset and PartialDataset to
create
splits.</li>
</ul>
<pre><code class="language-rust  ignore">// define chained dataset type here for brevity
type PartialData = PartialDataset&lt;ShuffledDataset&lt;DbPedia, DbPediaItem&gt;&gt;;
let dataset_len = dataset.len();
let split == &quot;train&quot;; // or &quot;val&quot;/&quot;test&quot;

let data_split = match split {
            &quot;train&quot; =&gt; PartialData::new(dataset, 0, len * 8 / 10), // Get first 80% dataset
            &quot;test&quot; =&gt; PartialData::new(dataset, len * 8 / 10, len), // Take remaining 20%
            _ =&gt; panic!(&quot;Invalid split type&quot;),                     // Handle unexpected split types
        };</code></pre>
<ul>
<li>
<p><strong>MapperDataset</strong>: This transform is useful to apply a transformation on each of the items of a dataset. Particularly
useful for normalization of image data when channel means are known.</p>
</li>
<li>
<p><strong>ComposedDataset</strong>: This transform is useful to compose multiple datasets downloaded from multiple sources (say
different HuggingfaceDatasetLoader sources) into a single bigger dataset which can be sampled from one source.</p>
</li>
</ul>
<h2 id="storage"><a class="header" href="#storage">Storage</a></h2>
<p>There are multiple dataset storage options available for you to choose from. The choice of the
dataset to use should be based on the dataset's size as well as its intended purpose.</p>
<div class="table-wrapper"><table><thead><tr><th>Storage</th><th>Description</th></tr></thead><tbody>
<tr><td><code>InMemDataset</code></td><td>In-memory dataset that uses a vector to store items. Well-suited for smaller datasets.</td></tr>
<tr><td><code>SqliteDataset</code></td><td>Dataset that uses SQLite to index items that can be saved in a simple SQL database file. Well-suited for larger datasets.</td></tr>
</tbody></table>
</div>
<h2 id="sources"><a class="header" href="#sources">Sources</a></h2>
<p>For now, there is only one dataset source available with Burn, but more to come!</p>
<h3 id="hugging-face"><a class="header" href="#hugging-face">Hugging Face</a></h3>
<p>You can easily import any Hugging Face dataset with Burn. We use SQLite as the storage to avoid
downloading the model each time or starting a Python process. You need to know the format of each
item in the dataset beforehand. Here's an example with the
<a href="https://huggingface.co/datasets/dbpedia_14">dbpedia dataset</a>.</p>
<pre><code class="language-rust  ignore">#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct DbPediaItem {
    pub title: String,
    pub content: String,
    pub label: usize,
}

fn main() {
    let dataset: SqliteDataset&lt;DbPediaItem&gt; = HuggingfaceDatasetLoader::new(&quot;dbpedia_14&quot;)
        .dataset(&quot;train&quot;) // The training split.
        .unwrap();
}</code></pre>
<p>We see that items must derive <code>serde::Serialize</code>, <code>serde::Deserialize</code>, <code>Clone</code>, and <code>Debug</code>, but
those are the only requirements.</p>
<p><strong>What about streaming datasets?</strong></p>
<p>There is no streaming dataset API with Burn, and this is by design! The learner struct will iterate
multiple times over the dataset and only checkpoint when done. You can consider the length of the
dataset as the number of iterations before performing checkpointing and running the validation.
There is nothing stopping you from returning different items even when called with the same <code>index</code>
multiple times.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-training-loops"><a class="header" href="#custom-training-loops">Custom Training Loops</a></h1>
<p>Even though Burn comes with a project dedicated to simplifying training, it doesn't mean that you
have to use it. Sometimes you may have special needs for your training, and it might be faster to
just reimplement the training loop yourself. Also, you may just prefer implementing your own
training loop instead of using a pre-built one in general.</p>
<p>Burn's got you covered!</p>
<p>We will start from the same example shown in the <a href="./basic-workflow">basic workflow</a> section, but
without using the <code>Learner</code> struct.</p>
<pre><code class="language-rust  ignore">#[derive(Config)]
pub struct MnistTrainingConfig {
    #[config(default = 10)]
    pub num_epochs: usize,
    #[config(default = 64)]
    pub batch_size: usize,
    #[config(default = 4)]
    pub num_workers: usize,
    #[config(default = 42)]
    pub seed: u64,
    #[config(default = 1e-4)]
    pub lr: f64,
    pub model: ModelConfig,
    pub optimizer: AdamConfig,
}

pub fn run&lt;B: AutodiffBackend&gt;(device: &amp;B::Device) {
    // Create the configuration.
    let config_model = ModelConfig::new(10, 1024);
    let config_optimizer = AdamConfig::new();
    let config = MnistTrainingConfig::new(config_model, config_optimizer);

    B::seed(config.seed);

    // Create the model and optimizer.
    let mut model = config.model.init(device);
    let mut optim = config.optimizer.init();

    // Create the batcher.
    let batcher_train = MNISTBatcher::&lt;B&gt;::new(device.clone());
    let batcher_valid = MNISTBatcher::&lt;B::InnerBackend&gt;::new(device.clone());

    // Create the dataloaders.
    let dataloader_train = DataLoaderBuilder::new(batcher_train)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MNISTDataset::train());

    let dataloader_test = DataLoaderBuilder::new(batcher_valid)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MNISTDataset::test());

    ...
}</code></pre>
<p>As seen with the previous example, setting up the configurations and the dataloader hasn't changed.
Now, let's move forward and write our own training loop:</p>
<pre><code class="language-rust  ignore">pub fn run&lt;B: AutodiffBackend&gt;(device: B::Device) {
    ...

    // Iterate over our training and validation loop for X epochs.
    for epoch in 1..config.num_epochs + 1 {
        // Implement our training loop.
        for (iteration, batch) in dataloader_train.iter().enumerate() {
            let output = model.forward(batch.images);
            let loss = CrossEntropyLoss::new(None).forward(output.clone(), batch.targets.clone());
            let accuracy = accuracy(output, batch.targets);

            println!(
                &quot;[Train - Epoch {} - Iteration {}] Loss {:.3} | Accuracy {:.3} %&quot;,
                iteration,
                epoch,
                loss.clone().into_scalar(),
                accuracy,
            );

            // Gradients for the current backward pass
            let grads = loss.backward();
            // Gradients linked to each parameter of the model.
            let grads = GradientsParams::from_grads(grads, &amp;model);
            // Update the model using the optimizer.
            model = optim.step(config.lr, model, grads);
        }

        // Get the model without autodiff.
        let model_valid = model.valid();

        // Implement our validation loop.
        for (iteration, batch) in dataloader_test.iter().enumerate() {
            let output = model_valid.forward(batch.images);
            let loss = CrossEntropyLoss::new(None).forward(output.clone(), batch.targets.clone());
            let accuracy = accuracy(output, batch.targets);

            println!(
                &quot;[Valid - Epoch {} - Iteration {}] Loss {} | Accuracy {}&quot;,
                iteration,
                epoch,
                loss.clone().into_scalar(),
                accuracy,
            );
        }
    }
}</code></pre>
<p>In the previous code snippet, we can observe that the loop starts from epoch <code>1</code> and goes up to
<code>num_epochs</code>. Within each epoch, we iterate over the training dataloader. During this process, we
execute the forward pass, which is necessary for computing both the loss and accuracy. To maintain
simplicity, we print the results to stdout.</p>
<p>Upon obtaining the loss, we can invoke the <code>backward()</code> function, which returns the gradients
specific to each variable. It's important to note that we need to map these gradients to their
corresponding parameters using the <code>GradientsParams</code> type. This step is essential because you might
run multiple different autodiff graphs and accumulate gradients for each parameter id.</p>
<p>Finally, we can perform the optimization step using the learning rate, the model, and the computed
gradients. It's worth mentioning that, unlike PyTorch, there's no need to register the gradients
with the optimizer, nor do you have to call <code>zero_grad</code>. The gradients are automatically consumed
during the optimization step. If you're interested in gradient accumulation, you can easily achieve
this by using the <code>GradientsAccumulator</code>.</p>
<pre><code class="language-rust  ignore">let mut accumulator = GradientsAccumulator::new();
let grads = model.backward();
let grads = GradientsParams::from_grads(grads, &amp;model);
accumulator.accumulate(&amp;model, grads); ...
let grads = accumulator.grads(); // Pop the accumulated gradients.</code></pre>
<p>Note that after each epoch, we include a validation loop to assess our model's performance on
previously unseen data. To disable gradient tracking during this validation step, we can invoke
<code>model.valid()</code>, which provides a model on the inner backend without autodiff capabilities. It's
important to emphasize that we've declared our validation batcher to be on the inner backend,
specifically <code>MNISTBatcher&lt;B::InnerBackend&gt;</code>; not using <code>model.valid()</code> will result in a compilation
error.</p>
<p>You can find the code above available as an
<a href="https://github.com/tracel-ai/burn/tree/main/examples/custom-training-loop">example</a> for you to
test.</p>
<h2 id="custom-type"><a class="header" href="#custom-type">Custom Type</a></h2>
<p>The explanations above demonstrate how to create a basic training loop. However, you may find it
beneficial to organize your program using intermediary types. There are various ways to do this, but
it requires getting comfortable with generics.</p>
<p>If you wish to group the optimizer and the model into the same structure, you have several options.
It's important to note that the optimizer trait depends on both the <code>AutodiffModule</code> trait and the
<code>AutodiffBackend</code> trait, while the module only depends on the <code>AutodiffBackend</code> trait.</p>
<p>Here's a closer look at how you can create your types:</p>
<p><strong>Create a struct that is generic over the backend and the optimizer, with a predefined model.</strong></p>
<pre><code class="language-rust  ignore">struct Learner&lt;B, O&gt;
where
    B: AutodiffBackend,
{
    model: Model&lt;B&gt;,
    optim: O,
}</code></pre>
<p>This is quite straightforward. You can be generic over the backend since it's used with the concrete
type <code>Model</code> in this case.</p>
<p><strong>Create a struct that is generic over the model and the optimizer.</strong></p>
<pre><code class="language-rust  ignore">struct Learner&lt;M, O&gt; {
    model: M,
    optim: O,
}</code></pre>
<p>This option is a quite intuitive way to declare the struct. You don't need to write type constraints
with a <code>where</code> statement when defining a struct; you can wait until you implement the actual
function. However, with this struct, you may encounter some issues when trying to implement code
blocks to your struct.</p>
<pre><code class="language-rust  ignore">impl&lt;B, M, O&gt; Learner&lt;M, O&gt;
where
    B: AutodiffBackend,
    M: AutodiffModule&lt;B&gt;,
    O: Optimizer&lt;M, B&gt;,
{
    pub fn step(&amp;mut self, _batch: MNISTBatch&lt;B&gt;) {
        //
    }
}</code></pre>
<p>This will result in the following compilation error:</p>
<pre><code class="language-console">1. the type parameter `B` is not constrained by the impl trait, self type, or predicates
   unconstrained type parameter [E0207]
</code></pre>
<p>To resolve this issue, you have two options. The first one is to make your function generic over
the backend and add your trait constraint within its definition:</p>
<pre><code class="language-rust  ignore">#[allow(dead_code)]
impl&lt;M, O&gt; Learner2&lt;M, O&gt; {
    pub fn step&lt;B: AutodiffBackend&gt;(&amp;mut self, _batch: MNISTBatch&lt;B&gt;)
    where
        B: AutodiffBackend,
        M: AutodiffModule&lt;B&gt;,
        O: Optimizer&lt;M, B&gt;,
    {
        //
    }
}</code></pre>
<p>However, some people may prefer to have the constraints on the implementation block itself. In that
case, you can make your struct generic over the backend using <code>PhantomData&lt;B&gt;</code>.</p>
<p><strong>Create a struct that is generic over the backend, the model, and the optimizer.</strong></p>
<pre><code class="language-rust  ignore">struct Learner3&lt;B, M, O&gt; {
    model: M,
    optim: O,
    _b: PhantomData&lt;B&gt;,
}</code></pre>
<p>You might wonder why <code>PhantomData</code> is required. Each generic argument must be used as a field when
declaring a struct. When you don't need the generic argument, you can use <code>PhantomData</code> to mark it
as a zero sized type.</p>
<p>These are just some suggestions on how to define your own types, but you are free to use any pattern
that you prefer.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="saving-and-loading-models"><a class="header" href="#saving-and-loading-models">Saving and Loading Models</a></h1>
<p>Saving your trained machine learning model is quite easy, no matter the output format you choose. As
mentioned in the <a href="./building-blocks/record.html">Record</a> section, different formats are supported to
serialize/deserialize models. By default, we use the <code>NamedMpkFileRecorder</code> which uses the
<a href="https://msgpack.org/">MessagePack</a> binary serialization format with the help of
<a href="https://docs.rs/rmp-serde/">smp_serde</a>.</p>
<pre><code class="language-rust  ignore">// Save model in MessagePack format with full precision
let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::new();
model
  .save_file(model_path, &amp;recorder)
  .expect(&quot;Should be able to save the model&quot;);</code></pre>
<p>Note that the file extension is automatically handled by the recorder depending on the one you
choose. Therefore, only the file path and base name should be provided.</p>
<p>Now that you have a trained model saved to your disk, you can easily load it in a similar fashion.</p>
<pre><code class="language-rust  ignore">// Load model in full precision from MessagePack file
let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::new();
model
  .load_file(model_path, &amp;recorder, device)
  .expect(&quot;Should be able to load the model weights from the provided file&quot;);</code></pre>
<p><strong>Note:</strong> models can be saved in different output formats, just make sure you are using the correct
recorder type when loading the saved model. Type conversion between different precision settings is
automatically handled, but formats are not interchangeable. A model can be loaded from one format
and saved to another format, just as long as you load it back with the new recorder type afterwards.</p>
<h2 id="initialization-from-recorded-weights"><a class="header" href="#initialization-from-recorded-weights">Initialization from Recorded Weights</a></h2>
<p>While the first approach is very straightforward, it does require the model to already be
initialized. If instead you would like to skip the initialization and directly load the weights into
the modules of your model, you can create a new initialization function. Let's take the following
model definition as a simple example.</p>
<pre><code class="language-rust  ignore">#[derive(Module, Debug)]
pub struct Model&lt;B: Backend&gt; {
    linear_in: Linear&lt;B&gt;,
    linear_out: Linear&lt;B&gt;,
    activation: ReLU,
}</code></pre>
<p>Similar to the <a href="../basic-workflow/inference.html">basic workflow inference example</a>, we can define a
new initialization function which initializes the different parts of our model with the record
values.</p>
<pre><code class="language-rust  ignore">impl&lt;B: Backend&gt; Model&lt;B&gt; {
    /// Returns the initialized model using the recorded weights.
    pub fn init_with(record: ModelRecord&lt;B&gt;) -&gt; Model&lt;B&gt; {
        Model {
            linear_in: LinearConfig::new(10, 64).init_with(record.linear_in),
            linear_out: LinearConfig::new(64, 2).init_with(record.linear_out),
            activation: ReLU::new(),
        }
    }

    /// Returns the dummy model with randomly initialized weights.
    pub fn new(device: &amp;Device&lt;B&gt;) -&gt; Model&lt;B&gt; {
        let l1 = LinearConfig::new(10, 64).init(device);
        let l2 = LinearConfig::new(64, 2).init(device);
        Model {
            linear_in: l1,
            linear_out: l2,
            activation: ReLU::new(),
        }
    }
}</code></pre>
<p>Now, let's save a model that we can load later. In the following snippets, we use
<code>type MyBackend = NdArray&lt;f32&gt;</code> but you can use whatever backend you like.</p>
<pre><code class="language-rust  ignore">// Create a dummy initialized model to save
let device = Default::default();
let model = Model::&lt;MyBackend&gt;::new(&amp;device);

// Save model in MessagePack format with full precision
let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::new();
model
    .save_file(model_path, &amp;recorder)
    .expect(&quot;Should be able to save the model&quot;);</code></pre>
<p>Afterwards, the model can just as easily be loaded from the record saved on disk.</p>
<pre><code class="language-rust  ignore">// Load model record on the backend's default device
let record: ModelRecord&lt;MyBackend&gt; = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::new()
    .load(model_path.into(), device)
    .expect(&quot;Should be able to load the model weights from the provided file&quot;);

// Directly initialize a new model with the loaded record/weights
let model = Model::init_with(record);</code></pre>
<h2 id="no-storage-no-problem"><a class="header" href="#no-storage-no-problem">No Storage, No Problem!</a></h2>
<p>For applications where file storage may not be available (or desired) at runtime, you can use the
<code>BinBytesRecorder</code>.</p>
<p>In the previous examples we used a <code>FileRecorder</code> based on the MessagePack format, which could be
replaced with <a href="./building-blocks/record.html#recorder">another file recorder</a> of your choice. To embed
a model as part of your runtime application, first save the model to a binary file with
<code>BinFileRecorder</code>.</p>
<pre><code class="language-rust  ignore">// Save model in binary format with full precision
let recorder = BinFileRecorder::&lt;FullPrecisionSettings&gt;::new();
model
  .save_file(model_path, &amp;recorder)
  .expect(&quot;Should be able to save the model&quot;);</code></pre>
<p>Then, in your final application, include the model and use the <code>BinBytesRecorder</code> to load it.</p>
<p>Embedding the model as part of your application is especially useful for smaller models but not
recommended for very large models as it would significantly increase the binary size as well as
consume a lot more memory at runtime.</p>
<pre><code class="language-rust  ignore">// Include the model file as a reference to a byte array
static MODEL_BYTES: &amp;[u8] = include_bytes!(&quot;path/to/model.bin&quot;);

// Load model binary record in full precision
let record = BinBytesRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(MODEL_BYTES.to_vec(), device)
    .expect(&quot;Should be able to load model the model weights from bytes&quot;);

// Load that record with the model
model.load_record(record);</code></pre>
<p>This example assumes that the model was already created before loading the model record. If instead
you want to skip the random initialization and directly initialize the weights with the provided
record, you could adapt this like the <a href="saving-and-loading.html#initialization-from-recorded-weights">previous example</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="importing-models"><a class="header" href="#importing-models">Importing Models</a></h1>
<p>The Burn project supports the import of models from various frameworks, emphasizing efficiency and
compatibility. Currently, it handles two primary model formats:</p>
<ol>
<li>
<p><a href="import/./onnx-model">ONNX</a>: Facilitates direct import, ensuring the model's performance and structure
are maintained.</p>
</li>
<li>
<p><a href="import/./pytorch-model">PyTorch</a>: Enables the loading of PyTorch model weights into Burnâ€™s native model
architecture, ensuring seamless integration.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="import-onnx-model"><a class="header" href="#import-onnx-model">Import ONNX Model</a></h1>
<h2 id="why-importing-models-is-necessary"><a class="header" href="#why-importing-models-is-necessary">Why Importing Models is Necessary</a></h2>
<p>In the realm of deep learning, it's common to switch between different frameworks depending on your
project's specific needs. Maybe you've painstakingly fine-tuned a model in TensorFlow or PyTorch and
now you want to reap the benefits of Burn's unique features for deployment or further testing. This
is precisely the scenario where importing models into Burn can be a game-changer.</p>
<h2 id="traditional-methods-the-drawbacks"><a class="header" href="#traditional-methods-the-drawbacks">Traditional Methods: The Drawbacks</a></h2>
<p>If you've been working with other deep learning frameworks like PyTorch, it's likely that you've
exported model weights before. PyTorch, for instance, lets you save model weights using its
<code>torch.save()</code> function. Yet, to port this model to another framework, you face the arduous task of
manually recreating the architecture in the destination framework before loading in the weights. Not
only is this method tedious, but it's also error-prone and hinders smooth interoperability between
frameworks.</p>
<p>It's worth noting that for models using cutting-edge, framework-specific features, manual porting
might be the only option, as standards like ONNX might not yet support these new innovations.</p>
<h2 id="enter-onnx"><a class="header" href="#enter-onnx">Enter ONNX</a></h2>
<p><a href="https://onnx.ai/onnx/intro/index.html">ONNX (Open Neural Network Exchange)</a> is designed to solve
such complications. It's an open-standard format that exports both the architecture and the weights
of a deep learning model. This feature makes it exponentially easier to move models between
different frameworks, thereby significantly aiding interoperability. ONNX is supported by a number
of frameworks including but not limited to TensorFlow, PyTorch, Caffe2, and Microsoft Cognitive
Toolkit.</p>
<h3 id="advantages-of-onnx"><a class="header" href="#advantages-of-onnx">Advantages of ONNX</a></h3>
<p>ONNX stands out for encapsulating two key elements:</p>
<ol>
<li><strong>Model Information</strong>: It captures the architecture, detailing the layers, their connections, and
configurations.</li>
<li><strong>Weights</strong>: ONNX also contains the trained model's weights.</li>
</ol>
<p>This dual encapsulation not only simplifies the porting of models between frameworks but also allows
seamless deployment across different environments without compatibility concerns.</p>
<h2 id="burns-onnx-support-importing-made-easy"><a class="header" href="#burns-onnx-support-importing-made-easy">Burn's ONNX Support: Importing Made Easy</a></h2>
<p>Understanding the important role that ONNX plays in the contemporary deep learning landscape, Burn
simplifies the process of importing ONNX models via an intuitive API designed to mesh well with
Burn's ecosystem.</p>
<p>Burn's solution is to translate ONNX files into Rust source code as well as Burn-compatible weights.
This transformation is carried out through the burn-import crate's code generator during build time,
providing advantages for both executing and further training ONNX models.</p>
<h3 id="advantages-of-burns-onnx-approach"><a class="header" href="#advantages-of-burns-onnx-approach">Advantages of Burn's ONNX Approach</a></h3>
<ol>
<li>
<p><strong>Native Integration</strong>: The generated Rust code is fully integrated into Burn's architecture,
enabling your model to run on various backends without the need for a separate ONNX runtime.</p>
</li>
<li>
<p><strong>Trainability</strong>: The imported model is not just for inference; it can be further trained or
fine-tuned using Burn's native training loop.</p>
</li>
<li>
<p><strong>Portability</strong>: As the model is converted to Rust source code, it can be compiled into
WebAssembly for browser execution. Likewise, this approach is beneficial for no-std embedded
devices.</p>
</li>
<li>
<p><strong>Optimization</strong>: Rust's compiler can further optimize the generated code for target
architectures, thereby improving performance.</p>
</li>
</ol>
<h3 id="sample-code-for-importing-onnx-model"><a class="header" href="#sample-code-for-importing-onnx-model">Sample Code for Importing ONNX Model</a></h3>
<p>Below is a step-by-step guide to importing an ONNX model into a Burn-based project:</p>
<h4 id="step-1-update-buildrs"><a class="header" href="#step-1-update-buildrs">Step 1: Update <code>build.rs</code></a></h4>
<p>Include the <code>burn-import</code> crate and use the following Rust code in your <code>build.rs</code>:</p>
<pre><code class="language-rust  ignore">use burn_import::onnx::ModelGen;

fn main() {
    // Generate Rust code from the ONNX model file
    ModelGen::new()
        .input(&quot;src/model/mnist.onnx&quot;)
        .out_dir(&quot;model/&quot;)
        .run_from_script();
}</code></pre>
<h4 id="step-2-modify-modrs"><a class="header" href="#step-2-modify-modrs">Step 2: Modify <code>mod.rs</code></a></h4>
<p>Add this code to the <code>mod.rs</code> file located in <code>src/model</code>:</p>
<pre><code class="language-rust  ignore">pub mod mnist {
    include!(concat!(env!(&quot;OUT_DIR&quot;), &quot;/model/mnist.rs&quot;));
}</code></pre>
<h4 id="step-3-utilize-imported-model"><a class="header" href="#step-3-utilize-imported-model">Step 3: Utilize Imported Model</a></h4>
<p>Here's how to use the imported model in your application:</p>
<pre><code class="language-rust  ignore">mod model;

use burn::tensor;
use burn_ndarray::{NdArray, NdArrayDevice};
use model::mnist::Model;

fn main() {
    // Initialize a new model instance
    let device = NdArrayDevice::default();
    let model: Model&lt;NdArray&lt;f32&gt;&gt; = Model::new(&amp;device);

    // Create a sample input tensor (zeros for demonstration)
    let input = tensor::Tensor::&lt;NdArray&lt;f32&gt;, 4&gt;::zeros([1, 1, 28, 28], &amp;device);

    // Perform inference
    let output = model.forward(input);

    // Print the output
    println!(&quot;{:?}&quot;, output);
}</code></pre>
<h3 id="working-examples"><a class="header" href="#working-examples">Working Examples</a></h3>
<p>For practical examples, please refer to:</p>
<ol>
<li><a href="https://github.com/tracel-ai/burn/tree/main/examples/onnx-inference">MNIST Inference Example</a></li>
<li><a href="https://github.com/tracel-ai/models/tree/main/squeezenet-burn">SqueezeNet Image Classification</a></li>
</ol>
<p>By combining ONNX's robustness with Burn's unique features, you'll have the flexibility and power to
streamline your deep learning workflows like never before.</p>
<hr />
<blockquote>
<p>ðŸš¨<strong>Note</strong>: <code>burn-import</code> crate is in active development and currently supports a
<a href="https://github.com/tracel-ai/burn/blob/main/burn-import/SUPPORTED-ONNX-OPS.md">limited set of ONNX operators</a>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pytorch-model"><a class="header" href="#pytorch-model">PyTorch Model</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Whether you've trained your model in PyTorch or you want to use a pre-trained model from PyTorch,
you can import them into Burn. Burn supports importing PyTorch model weights with <code>.pt</code> file
extension. Compared to ONNX models, <code>.pt</code> files only contain the weights of the model, so you will
need to reconstruct the model architecture in Burn.</p>
<h2 id="how-to-export-a-pytorch-model"><a class="header" href="#how-to-export-a-pytorch-model">How to export a PyTorch model</a></h2>
<p>If you have a PyTorch model that you want to import into Burn, you will need to export it first,
unless you are using a pre-trained published model. To export a PyTorch model, you can use the
<code>torch.save</code> function.</p>
<p>Here is an example of how to export a PyTorch model:</p>
<pre><code class="language-python">import torch
import torch.nn as nn

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(2, 2, (2,2))
        self.conv2 = nn.Conv2d(2, 2, (2,2), bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

def main():
    torch.manual_seed(42)  # To make it reproducible
    model = Net().to(torch.device(&quot;cpu&quot;))
    model_weights = model.state_dict()
    torch.save(model_weights, &quot;conv2d.pt&quot;)
</code></pre>
<p>Use <a href="https://github.com/lutzroeder/netron">Netron</a> to view the exported model. You should see
something like this:</p>
<p><img src="import/./conv2d.svg" alt="image alt &gt;" /></p>
<h2 id="how-to-import-a-pytorch-model"><a class="header" href="#how-to-import-a-pytorch-model">How to import a PyTorch model</a></h2>
<ol>
<li>
<p>Define the model in Burn:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use burn::{
    module::Module,
    nn::conv::{Conv2d, Conv2dConfig},
    tensor::{backend::Backend, Tensor},
};

#[derive(Module, Debug)]
pub struct Net&lt;B: Backend&gt; {
    conv1: Conv2d&lt;B&gt;,
    conv2: Conv2d&lt;B&gt;,
}

impl&lt;B: Backend&gt; Net&lt;B&gt; {
    /// Create a new model from the given record.
    pub fn new_with(record: NetRecord&lt;B&gt;) -&gt; Self {
        let conv1 = Conv2dConfig::new([2, 2], [2, 2])
            .init_with(record.conv1);
        let conv2 = Conv2dConfig::new([2, 2], [2, 2])
            .with_bias(false)
            .init_with(record.conv2);
        Self { conv1, conv2 }
    }

    /// Forward pass of the model.
    pub fn forward(&amp;self, x: Tensor&lt;B, 4&gt;) -&gt; Tensor&lt;B, 4&gt; {
        let x = self.conv1.forward(x);
        self.conv2.forward(x)
    }
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p>Load the model weights from the exported PyTorch model (2 options):</p>
<p>a) <em>Dynamically</em>, but this requires burn-import runtime dependency:</p>
<pre><pre class="playground"><code class="language-rust">use crate::model;

use burn::record::{FullPrecisionSettings, Recorder};
use burn_import::pytorch::PyTorchFileRecorder;

type Backend = burn_ndarray::NdArray&lt;f32&gt;;

fn main() {
    let device = Default::default();
    let record = PyTorchFileRecorder::&lt;FullPrecisionSettings&gt;::default()
        .load(&quot;./conv2d.pt&quot;.into(), &amp;device)
        .expect(&quot;Should decode state successfully&quot;);

    let model = model::Net::&lt;Backend&gt;::new_with(record);
}</code></pre></pre>
<p>b) <em>Pre-converted</em> to Burn's binary format:</p>
<pre><pre class="playground"><code class="language-rust">// Convert the PyTorch model to Burn's binary format in
// build.rs or in a separate executable. Then, include the generated file
// in your project. See `examples/pytorch-import` for an example.

use crate::model;

use burn::record::{FullPrecisionSettings, NamedMpkFileRecorder, Recorder};
use burn_import::pytorch::PyTorchFileRecorder;

type Backend = burn_ndarray::NdArray&lt;f32&gt;;

fn main() {
    let device = Default::default();
    let recorder = PyTorchFileRecorder::&lt;FullPrecisionSettings&gt;::default()
    let record: model::NetRecord&lt;B&gt; = recorder
        .load(&quot;./conv2d.pt&quot;.into(), &amp;device)
        .expect(&quot;Should decode state successfully&quot;);

    // Save the model record to a file.
    let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::default();
    recorder
        .record(record, &quot;MY_FILE_OUTPUT_PATH&quot;.into())
        .expect(&quot;Failed to save model record&quot;);
}

/// Load the model from the file in your source code (not in build.rs or script).
fn load_model() -&gt; Net::&lt;Backend&gt; {
    let device = Default::default();
    let record = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::default()
        .load(&quot;./MY_FILE_OUTPUT_PATH&quot;.into(), &amp;device)
        .expect(&quot;Should decode state successfully&quot;);

    Net::&lt;Backend&gt;::new_with(record)
}</code></pre></pre>
</li>
</ol>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="adjusting-the-source-model-architecture"><a class="header" href="#adjusting-the-source-model-architecture">Adjusting the source model architecture</a></h3>
<p>If your target model differs structurally from the model you exported, <code>PyTorchFileRecorder</code> allows
changing the attribute names and the order of the attributes. For example, if you exported a model
with the following structure:</p>
<pre><code class="language-python">class ConvModule(nn.Module):
    def __init__(self):
        super(ConvModule, self).__init__()
        self.conv1 = nn.Conv2d(2, 2, (2,2))
        self.conv2 = nn.Conv2d(2, 2, (2,2), bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv = ConvModule()

    def forward(self, x):
        x = self.conv(x)
        return x
</code></pre>
<p>But you need to import it into a model with the following structure:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Module, Debug)]
pub struct Net&lt;B: Backend&gt; {
    conv1: Conv2d&lt;B&gt;,
    conv2: Conv2d&lt;B&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p>Which produces the following weights structure (viewed in
<a href="https://github.com/lutzroeder/netron">Netron</a>):</p>
<p><img src="import/./key_remap.svg" alt="image alt &gt;" /></p>
<p>You can use the <code>PyTorchFileRecorder</code> to change the attribute names and the order of the attributes
by specifying a regular expression (See
<a href="https://docs.rs/regex/latest/regex/struct.Regex.html#method.replace">regex::Regex::replace</a>) to
match the attribute name and a replacement string in <code>LoadArgs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let device = Default::default();
let load_args = LoadArgs::new(&quot;tests/key_remap/key_remap.pt&quot;.into())
    // Remove &quot;conv&quot; prefix, e.g. &quot;conv.conv1&quot; -&gt; &quot;conv1&quot;
    .with_key_remap(&quot;conv\\.(.*)&quot;, &quot;$1&quot;);

let record = PyTorchFileRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(load_args, &amp;device)
    .expect(&quot;Should decode state successfully&quot;);

let model = Net::&lt;Backend&gt;::new_with(record);
<span class="boring">}</span></code></pre></pre>
<h3 id="loading-the-model-weights-to-a-partial-model"><a class="header" href="#loading-the-model-weights-to-a-partial-model">Loading the model weights to a partial model</a></h3>
<p><code>PyTorchFileRecorder</code> enables selective weight loading into partial models. For instance, in a model
with both an encoder and a decoder, it's possible to load only the encoder weights. This is done by
defining the encoder in Burn, allowing the loading of its weights while excluding the decoder's.</p>
<h2 id="current-known-issues"><a class="header" href="#current-known-issues">Current known issues</a></h2>
<ol>
<li><a href="https://github.com/tracel-ai/burn/issues/1178">Candle's pickle library does not currently function on Windows due to a Candle bug</a>.</li>
<li><a href="https://github.com/tracel-ai/burn/issues/1179">Candle's pickle does not currently unpack boolean tensors</a>.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced"><a class="header" href="#advanced">Advanced</a></h1>
<p>In this section, we will go into advanced topics that extend beyond basic usage. Given Burn's
exceptional flexibility, a lot of advanced use cases become possible.</p>
<p>Before going through this section, we strongly recommend exploring the
<a href="advanced/../basic-workflow/">basic workflow</a> section and the
<a href="advanced/../building-blocks/">building blocks</a> section. Establishing a solid understanding of how
the framework operates is crucial to comprehending the advanced concepts presented here. While you
have the freedom to explore the advanced sections in any order you prefer, it's important to note
that this section is not intended to be linear, contrary to preceding sections. Instead, it serves
as a repository of use cases that you can refer to for guidance as needed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend-extension"><a class="header" href="#backend-extension">Backend Extension</a></h1>
<p>Burn aims to be the most flexible deep learning framework. While it's crucial to maintain
compatibility with a wide variety of backends, Burn provides the ability to extend the functionality
of a backend implementation to suit your modeling requirements. This versatility is advantageous in
numerous ways, such as supporting custom operations like flash attention or manually fusing
operations for enhanced performance.</p>
<p>In this section, we will go into the process of extending a backend, providing multiple examples.
But before we proceed, let's establish the fundamental principles that will empower you to craft
your own backend extensions.</p>
<p>As you can observe, most types in Burn are generic over the Backend trait. This might give the
impression that Burn operates at a high level over the backend layer. However, making the trait
explicit instead of being chosen via a compilation flag was a thoughtful design decision. This
explicitness does not imply that all backends must be identical; rather, it offers a great deal of
flexibility when composing backends. The autodifferentiation backend trait (see
<a href="advanced/backend-extension/../../building-blocks/autodiff.html">autodiff section</a>) is an example of how the backend trait has been
extended to enable gradient computation with backpropagation. Furthermore, this design allows you to
create your own backend extension. To achieve this, you need to design your own backend trait
specifying which functions should be supported.</p>
<pre><code class="language-rust  ignore">pub trait Backend: burn::tensor::backend::Backend {
    fn my_new_function(tensor: B::TensorPrimitive&lt;2&gt;) -&gt; B::TensorPrimitive&lt;2&gt; {
        // You can define a basic implementation reusing the Burn Backend API.
        // This can be useful since all backends will now automatically support
        // your model. But performance can be improved for this new
        // operation by implementing this block in specific backends.
    }
}</code></pre>
<p>You can then implement your new custom backend trait for any backend that you want to support:</p>
<pre><code class="language-rust  ignore">impl&lt;E: TchElement&gt; Backend for burn_tch::LibTorch&lt;E&gt; {
   fn my_new_function(tensor: TchTensor&lt;E, 2&gt;) -&gt; TchTensor&lt;E, 2&gt; {
      // My Tch implementation
   }
}

impl&lt;E: NdArrayElement&gt; Backend for burn_ndarray::NdArray&lt;E&gt; {
    // No specific implementation, but the backend can still be used.
}</code></pre>
<p>You can support the backward pass using the same pattern.</p>
<pre><code class="language-rust  ignore">impl&lt;B: Backend&gt; Backend for burn_autodiff::Autodiff&lt;B&gt; {
    // No specific implementation; autodiff will work with the default
    // implementation. Useful if you still want to train your model, but
    // observe performance gains mostly during inference.
}

impl&lt;B: Backend&gt; Backend for burn_autodiff::Autodiff&lt;B&gt; {
   fn my_new_function(tensor: AutodiffTensor&lt;E, 2&gt;) -&gt; AutodiffTensor&lt;E, 2&gt; {
      // My own backward implementation, generic over my custom Backend trait.
      //
      // You can add a new method `my_new_function_backward` to your custom backend
      // trait if you want to invoke a custom kernel during the backward pass.
   }
}

impl&lt;E: TchElement&gt; Backend for burn_autodiff::Autodiff&lt;burn_tch::LibTorch&lt;E&gt;&gt; {
   fn my_new_function(tensor: AutodiffTensor&lt;E, 2&gt;) -&gt; AutodiffTensor&lt;E, 2&gt; {
      // My own backward implementation, generic over a backend implementation.
      //
      // This is another way to call a custom kernel for the backward pass that
      // doesn't require the addition of a new `backward` function in the custom backend.
      // This is useful if you don't want all backends to support training, reducing
      // the need for extra code when you know your model will only be trained on one
      // specific backend.
   }
}</code></pre>
<p>The specificity of each implementation will be covered by the examples provided in this section.
Currently, we only have one example, but more are yet to come!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-wgpu-kernel"><a class="header" href="#custom-wgpu-kernel">Custom WGPU Kernel</a></h1>
<p>In this section, you will learn how to create your own custom operation by writing your own kernel
with the WGPU backend. We will take the example of a common workflow in the deep learning field,
where we create a kernel to fuse multiple operations together. We will fuse a matmul kernel followed
by an addition and the ReLU activation function, which is commonly found in various models. All the
code can be found under the
<a href="https://github.com/tracel-ai/burn/tree/main/examples/custom-wgpu-kernel">examples directory</a>.</p>
<h2 id="custom-backend-trait"><a class="header" href="#custom-backend-trait">Custom Backend Trait</a></h2>
<p>First, we need to determine the type signature of our newly created operation by defining our custom
backend traits. As we will use the associated type <code>TensorPrimitive</code> of the <code>Backend</code> trait, which
encapsulates the underlying tensor implementation of the backend, we will use a type alias to avoid
the ugly disambiguation with associated types.</p>
<pre><code class="language-rust  ignore">/// We use a type alias for better readability.
pub type FloatTensor&lt;B, const D: usize&gt; = &lt;B as burn::tensor::backend::Backend&gt;::TensorPrimitive&lt;D&gt;;

/// We create our own Backend trait that extends the Burn backend trait.
pub trait Backend: burn::tensor::backend::Backend {
    fn fused_matmul_add_relu&lt;const D: usize&gt;(
        lhs: FloatTensor&lt;Self, D&gt;,
        rhs: FloatTensor&lt;Self, D&gt;,
        bias: FloatTensor&lt;Self, D&gt;,
    ) -&gt; FloatTensor&lt;Self, D&gt;;
}

/// We create our own AutodiffBackend trait that extends the Burn autodiff backend trait.
pub trait AutodiffBackend: Backend + burn::tensor::backend::AutodiffBackend {}</code></pre>
<p>In our project, we can use these traits instead of the <code>burn::tensor::backend::{Backend, AutodiffBackend}</code>
traits provided by Burn. Burn's user APIs typically make use of the <code>Tensor</code> struct rather than
dealing directly with primitive tensor types. Therefore, we can encapsulate our newly defined
backend traits with functions that expose new operations while maintaining a consistent API.</p>
<pre><code class="language-rust  ignore">/// We define our custom implementation using the added function on our custom backend.
pub fn matmul_add_relu_custom&lt;B: Backend&gt;(
    lhs: Tensor&lt;B, 3&gt;,
    rhs: Tensor&lt;B, 3&gt;,
    bias: Tensor&lt;B, 3&gt;,
) -&gt; Tensor&lt;B, 3&gt; {
    let output = B::fused_matmul_add_relu(
        lhs.into_primitive(),
        rhs.into_primitive(),
        bias.into_primitive(),
    );

    Tensor::from_primitive(output)
}

/// We define a reference implementation using basic tensor operations.
pub fn matmul_add_relu_reference&lt;B: Backend&gt;(
    lhs: Tensor&lt;B, 3&gt;,
    rhs: Tensor&lt;B, 3&gt;,
    bias: Tensor&lt;B, 3&gt;,
) -&gt; Tensor&lt;B, 3&gt; {
    let x = lhs.matmul(rhs) + bias;

    activation::relu(x)
}
</code></pre>
<p>Note that we also provide a reference implementation for testing purposes, which allows us to easily
validate our new implementation. While not mandatory, having a reference implementation can be
valuable, especially in projects where creating a reference implementation solely using basic tensor
operations is feasible.</p>
<h2 id="forward-kernel"><a class="header" href="#forward-kernel">Forward Kernel</a></h2>
<p>Now, let's proceed to write the fused kernel using the WGSL shading language. To keep things simple,
we'll create a straightforward matmul kernel without employing any intricate techniques. Although we
won't delve into the details of the WGSL syntax, as it falls beyond the scope of this guide, we
still provide the implementation below for readers who are curious. The actual matmul, add and relu
computations are found at the end, after an extensive overhead whose use is to correctly map each
thread to the data it is responsible of, with support for batches.</p>
<pre><code class="language-wgsl  ignore">@group(0)
@binding(0)
var&lt;storage, read&gt; lhs: array&lt;{{ elem }}&gt;;

@group(0)
@binding(1)
var&lt;storage, read&gt; rhs: array&lt;{{ elem }}&gt;;

@group(0)
@binding(2)
var&lt;storage, read&gt; bias: array&lt;{{ elem }}&gt;;

@group(0)
@binding(3)
var&lt;storage, read_write&gt; output: array&lt;{{ elem }}&gt;;

@group(0)
@binding(4)
var&lt;storage, read&gt; info: array&lt;u32&gt;;

const BLOCK_SIZE = {{ workgroup_size_x }}u;

@compute
@workgroup_size({{ workgroup_size_x }}, {{ workgroup_size_y }}, 1)
fn main(
    @builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,
    @builtin(local_invocation_index) local_idx: u32,
    @builtin(workgroup_id) workgroup_id: vec3&lt;u32&gt;,
) {
    // Indices
    let row = workgroup_id.x * BLOCK_SIZE + (local_idx / BLOCK_SIZE);
    let col = workgroup_id.y * BLOCK_SIZE + (local_idx % BLOCK_SIZE);
    let batch = global_id.z;

    // Basic information
    let dim = info[0];
    let n_rows = info[6u * dim - 1u];
    let n_cols = info[6u * dim];
    let K = info[5u * dim - 1u];

    // Returns if outside the output dimension
    if row &gt;= n_rows || col &gt;= n_cols {
        return;
    }

    // Calculate the corresponding offsets with support for broadcasting.
    let offset_output = batch * n_rows * n_cols;
    var offset_lhs: u32 = 0u;
    var offset_rhs: u32 = 0u;

    let batch_dims = dim - 2u;
    for (var b: u32 = 1u; b &lt;= batch_dims; b++) {
        let stride_lhs = info[b];
        let stride_rhs = info[b + dim];
        let stride_output = info[b + 2u * dim];
        let shape_lhs = info[b + 3u * dim];
        let shape_rhs = info[b + 4u * dim];

        offset_lhs += offset_output / stride_output % shape_lhs * stride_lhs;
        offset_rhs += offset_output / stride_output % shape_rhs * stride_rhs;
    }

    // Basic matmul implementation
    var sum = 0.0;
    for (var k: u32 = 0u; k &lt; K; k++) {
        let lhs_index = row * K + k;
        let rhs_index = k * n_cols + col;

        sum += lhs[offset_lhs + lhs_index] * rhs[offset_rhs + rhs_index];
    }

    let output_index = row * n_cols + col;
    let index = offset_output + output_index;

    // Add and ReLU
    output[index] = max(sum + bias[index], 0.0);
}
</code></pre>
<p>Now, let's move on to the next step, which involves implementing the remaining code to launch the
kernel. The initial part entails loading the template and populating it with the appropriate
variables. The <code>register(name, value)</code> method simply replaces occurrences of <code>{{ name }}</code> in the
above WGSL code with some other string before it is compilated.</p>
<pre><code class="language-rust  ignore">// Source the kernel written in WGSL.
kernel_wgsl!(FusedMatmulAddReluRaw, &quot;./kernel.wgsl&quot;);

// Define our kernel type with workgroup information.
#[derive(new, Debug)]
struct FusedMatmulAddRelu&lt;E: FloatElement&gt; {
    workgroup_size_x: usize,
    workgroup_size_y: usize,
    _elem: PhantomData&lt;E&gt;,
}

// Implement the dynamic kernel trait for our kernel type.
impl&lt;E: FloatElement&gt; DynamicKernel for FusedMatmulAddRelu&lt;E&gt; {
    fn source_template(self) -&gt; SourceTemplate {
        // Extend our raw kernel with workgroup size information using the
        // `SourceTemplate` trait.
        FusedMatmulAddReluRaw::source_template()
            .register(&quot;workgroup_size_x&quot;, self.workgroup_size_x.to_string())
            .register(&quot;workgroup_size_y&quot;, self.workgroup_size_y.to_string())
            .register(&quot;elem&quot;, E::type_name())
    }

    fn id(&amp;self) -&gt; String {
        format!(&quot;{:?}&quot;, self)
    }
}</code></pre>
<p>Subsequently, we'll go into implementing our custom backend trait for the WGPU backend.</p>
<pre><code class="language-rust  ignore">/// Implement our custom backend trait for the existing backend `Wgpu`.
impl&lt;G: GraphicsApi, F: FloatElement, I: IntElement&gt; Backend for Wgpu&lt;G, F, I&gt; {
    fn fused_matmul_add_relu&lt;const D: usize&gt;(
        lhs: FloatTensor&lt;Self, D&gt;,
        rhs: FloatTensor&lt;Self, D&gt;,
        bias: FloatTensor&lt;Self, D&gt;,
    ) -&gt; WgpuTensor&lt;F, D&gt; {
        // Define workgroup size, hardcoded for simplicity.
        let workgroup_size_x = 16;
        let workgroup_size_y = 16;

        lhs.assert_is_on_same_device(&amp;rhs);
        lhs.assert_is_on_same_device(&amp;bias);

        // For simplicity, make sure each tensor is continuous.
        let lhs = into_contiguous(lhs);
        let rhs = into_contiguous(rhs);
        let bias = into_contiguous(bias);

        // Get the matmul relevant shapes.
        let num_rows = lhs.shape.dims[D - 2];
        let num_cols = rhs.shape.dims[D - 1];

        // Compute shape of output, while tracking number of batches.
        let mut num_batches = 1;
        let mut shape_out = [0; D];
        for i in 0..D - 2 {
            shape_out[i] = usize::max(lhs.shape.dims[i], rhs.shape.dims[i]);
            num_batches *= shape_out[i];
        }
        shape_out[D - 2] = num_rows;
        shape_out[D - 1] = num_cols;
        let shape_out = Shape::new(shape_out);

        // Create a buffer for the output tensor.
        let buffer = lhs
            .context
            .create_buffer(shape_out.num_elements() * core::mem::size_of::&lt;F&gt;());

        // Create the output tensor primitive.
        let output = WgpuTensor::new(lhs.context.clone(), shape_out, buffer);

        // Create the kernel.
        let kernel = FusedMatmulAddRelu::&lt;F&gt;::new(workgroup_size_x, workgroup_size_y);

        // Build info buffer with tensor information needed by the kernel, such as shapes and strides.
        let info = build_info(&amp;[&amp;lhs, &amp;rhs, &amp;output]);
        let info_buffer = lhs
            .context
            .create_buffer_with_data(bytemuck::cast_slice(&amp;info));

        // Declare the wgsl workgroup with the number of blocks in x, y and z.
        let blocks_needed_in_x = f32::ceil(num_rows as f32 / workgroup_size_x as f32) as u32;
        let blocks_needed_in_y = f32::ceil(num_cols as f32 / workgroup_size_y as f32) as u32;
        let workgroup = WorkGroup::new(blocks_needed_in_x, blocks_needed_in_y, num_batches as u32);

        // Execute lazily the kernel with the launch information and the given buffers.
        lhs.client.execute(
            Box::new(DynamicKernel::new(kernel, workgroup)),
            &amp;[
                &amp;lhs.handle,
                &amp;rhs.handle,
                &amp;bias.handle,
                &amp;output.handle,
                &amp;info_handle,
            ],
        );

        // Return the output tensor.
        output
    }
}</code></pre>
<p>In the preceding code block, we demonstrated how to launch the kernel that modifies the correct
buffer. It's important to note that Rust's mutability safety doesn't apply here; the context has the
capability to execute any mutable operation on any buffer. While this isn't a problem in the
previous scenario where we only modify the newly created output buffer, it is wise to keep this in
mind.</p>
<h2 id="backward"><a class="header" href="#backward">Backward</a></h2>
<p>Now that the custom backend trait is implemented for the WGPU backend, you can use it to invoke the
<code>matmul_add_relu_custom</code> function. However, calculating gradients is not yet possible at this stage.
If your use case does not extend beyond inference, there is no need to implement any of the
following code.</p>
<p>For the backward pass, we will leverage the backend implementation from <code>burn-autodiff</code>, which is
actually generic over the backend. Instead of crafting our own WGSL kernel for the backward pass, we
will use our fused kernel only for the forward pass, and compute the gradient using basic
operations.</p>
<pre><code class="language-rust  ignore">// Implement our custom backend trait for any backend that also implements our custom backend trait.
//
// Note that we could implement the backend trait only for the Wgpu backend instead of any backend that
// also implements our own API. This would allow us to call any function only implemented for Wgpu
// and potentially call a custom kernel crafted only for this task.
impl&lt;B: Backend&gt; Backend for Autodiff&lt;B&gt; {
    fn fused_matmul_add_relu&lt;const D: usize&gt;(
        lhs: FloatTensor&lt;Self, D&gt;,
        rhs: FloatTensor&lt;Self, D&gt;,
        bias: FloatTensor&lt;Self, D&gt;,
    ) -&gt; FloatTensor&lt;Self, D&gt; {
        // Create our zero-sized type that will implement the Backward trait.
        #[derive(Debug)]
        struct FusedMatmulAddReluBackward&lt;const D: usize&gt;;

        // Implement the backward trait for the given backend B, the node gradient being of rank D
        // with three other gradients to calculate (lhs, rhs, and bias).
        impl&lt;B: Backend, const D: usize&gt; Backward&lt;B, D, 3&gt; for FusedMatmulAddReluBackward&lt;D&gt; {
            // The state that must be built during the forward pass to compute the backward pass.
            //
            // Note that we could improve the performance further by only keeping the state of
            // tensors that are tracked, improving memory management, but for simplicity, we avoid
            // that part.
            type State = (
                FloatTensor&lt;B, D&gt;,
                FloatTensor&lt;B, D&gt;,
                FloatTensor&lt;B, D&gt;,
                Shape&lt;D&gt;,
            );

            fn backward(self, ops: Ops&lt;Self::State, 3&gt;, grads: &amp;mut Gradients) {
                // Get the nodes of each variable.
                let [node_lhs, node_rhs, node_bias] = ops.parents;
                // Fetch the gradient for the current node.
                let grad = grads.consume::&lt;B, D&gt;(&amp;ops.node);

                // Set the state.
                let (lhs, rhs, output, shape_bias) = ops.state;

                // Fetch shapes of the tensors to support broadcasting.
                let shape_lhs = B::shape(&amp;lhs);
                let shape_rhs = B::shape(&amp;rhs);

                // Compute the gradient of the output using the already existing `relu_backward`
                // function in the basic Burn backend trait.
                let grad_output = B::relu_backward(output, grad);

                // Compute the lhs gradient, which is the derivative of matmul with support for
                // broadcasting.
                let grad_lhs = broadcast_shape::&lt;B, D&gt;(
                    B::matmul(grad_output.clone(), B::transpose(rhs)),
                    &amp;shape_lhs,
                );
                // Compute the rhs gradient, which is the derivative of matmul with support for
                // broadcasting.
                let grad_rhs = broadcast_shape::&lt;B, D&gt;(
                    B::matmul(B::transpose(lhs), grad_output.clone()),
                    &amp;shape_rhs,
                );
                // The add derivative is only 1, so we just need to support broadcasting to
                // compute the bias gradient.
                let grad_bias = broadcast_shape::&lt;B, D&gt;(grad_output, &amp;shape_bias);

                // Register the gradient for each variable based on whether they are marked as
                // `tracked`.
                if let Some(node) = node_bias {
                    grads.register::&lt;B, D&gt;(node, grad_bias);
                }
                if let Some(node) = node_lhs {
                    grads.register::&lt;B, D&gt;(node, grad_lhs);
                }
                if let Some(node) = node_rhs {
                    grads.register::&lt;B, D&gt;(node, grad_rhs);
                }
            }
        }

        // Prepare a stateful operation with each variable node and corresponding graph.
        //
        // Each node can be fetched with `ops.parents` in the same order as defined here.
        match FusedMatmulAddReluBackward
            .prepare(
                [lhs.node, rhs.node, bias.node],
                [lhs.graph, rhs.graph, bias.graph],
            )
            .stateful()
        {
            OpsKind::Tracked(prep) =&gt; {
                // When at least one node is tracked, we should register our backward step.
                // We compute the output and the state before finishing the preparation.
                let bias_shape = B::shape(&amp;bias.primitive);
                let output = B::fused_matmul_add_relu(
                    lhs.primitive.clone(),
                    rhs.primitive.clone(),
                    bias.primitive,
                );

                let state = (lhs.primitive, rhs.primitive, output.clone(), bias_shape);
                prep.finish(state, output)
            }
            OpsKind::UnTracked(prep) =&gt; {
                // When no node is tracked, we can just compute the original operation without
                // keeping any state.
                let output = B::fused_matmul_add_relu(lhs.primitive, rhs.primitive, bias.primitive);
                prep.finish(output)
            }
        }
    }
}</code></pre>
<p>The previous code is self-documented to make it clearer, but here is what it does in summary.</p>
<p>We define <code>fused_matmul_add_relu</code> within <code>Autodiff&lt;B&gt;</code>, allowing any autodiff-decorated backend to
benefit from our implementation. In an autodiff-decorated backend, the forward pass must still be
implemented. This is achieved using a comprehensive match statement block where computation is
delegated to the inner backend, while keeping track of a state. The state comprises any information
relevant to the backward pass, such as input and output tensors, along with the bias shape. When an
operation isn't tracked (meaning there won't be a backward pass for this specific operation in the
graph), storing a state becomes unnecessary, and we simply perform the forward computation.</p>
<p>The backward pass uses the gradient obtained from the preceding node in the computation graph. It
calculates the derivatives for <code>relu</code> (<code>relu_backward</code>), add (no operation is required here, as the
derivative is one), and <code>matmul</code> (another <code>matmul</code> with transposed inputs). This results in
gradients for both input tensors and the bias, which are registered for consumption by subsequent
operation nodes.</p>
<p>The only remaining part is to implement our autodiff-decorated backend trait for our WGPU Backend.</p>
<pre><code class="language-rust  ignore">impl&lt;G: GraphicsApi, F: FloatElement, I: IntElement&gt; AutodiffBackend for Autodiff&lt;Wgpu&lt;G, F, I&gt;&gt;
{
}</code></pre>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>In this guide, we've implemented a fused kernel using the WGPU backend, enabling execution on any
GPU. By delving into the inner workings of both the WGPU backend and the autodiff backend, we've
gained a deeper understanding of these systems.</p>
<p>While extending a backend may be harder than working with straightforward tensors, the benefits can
be worth it. This approach enables the crafting of custom models with greater control over
execution, which can potentially greatly enhance the performance of your models.</p>
<p>It is worth noting that while the manual fusion of operations can be valuable, our future plans
include the development of a backend extension that will automate this process. As we conclude this
guide, we hope that you have gained insights into Burn's world of backend extensions, and that it
will help you to unleash the full potential of your projects.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
